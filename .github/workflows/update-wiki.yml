name: Update Wiki from DeepWiki

on:
  push:
    branches:
      - main

# Environment variables for cleaner workflow
env:
  DEEPWIKI_URL: "https://deepwiki.com/VforVitorio/F1_Strat_Manager"
  DEEPWIKI_MCP_PORT: "3000"
  IMAGE_DIR: "docs-md/images"
  WIKI_DIR: "wiki"
  DOCS_DIR: "docs-md"

jobs:
  update-wiki:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Setup Chrome for Selenium
        uses: browser-actions/setup-chrome@latest

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Clone DeepWiki MCP
        run: |
          git clone https://github.com/regenrek/deepwiki-mcp.git deepwiki-mcp
          cd deepwiki-mcp
          npm install

      - name: Build DeepWiki MCP
        working-directory: deepwiki-mcp
        run: |
          # Check if package.json exists and available scripts
          echo "Package.json scripts:"
          npm run || true
          # Try to build the project
          if npm run build 2>/dev/null; then
            echo "Build completed with npm run build"
          elif npm run compile 2>/dev/null; then
            echo "Build completed with npm run compile"
          elif npm run dist 2>/dev/null; then
            echo "Build completed with npm run dist"
          else
            echo "Trying manual build with typescript..."
            npx tsc || echo "Could not compile with tsc"
          fi
          # Verify that dist file exists
          echo "Checking compiled files:"
          ls -la
          if [ -d "dist" ]; then
            ls -la dist/
          else
            echo "dist directory not found"
            echo "Looking for .mjs files in project:"
            find . -name "*.mjs" -type f
          fi

      - name: Start DeepWiki MCP (HTTP mode)
        working-directory: deepwiki-mcp
        run: |
          # Verify that necessary files exist
          ls -la
          ls -la bin/
          # Verify that dist/index.mjs file exists
          if [ ! -f "dist/index.mjs" ]; then
            echo "Error: dist/index.mjs not found"
            echo "Available files:"
            find . -name "*.mjs" -type f
            exit 1
          fi
          # Start service in background
          node ./bin/cli.mjs --http --port ${{ env.DEEPWIKI_MCP_PORT }} > ../deepwiki.log 2>&1 &
          DEEPWIKI_PID=$!
          echo "DeepWiki PID: $DEEPWIKI_PID"
          # Wait and verify service is available
          for i in {1..30}; do
            if curl -s http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp > /dev/null 2>&1; then
              echo "DeepWiki MCP is available"
              break
            fi
            echo "Waiting for DeepWiki MCP to be available... attempt $i/30"
            sleep 2
          done
          # Verify once more - check /mcp endpoint
          if ! curl -s http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp > /dev/null 2>&1; then
            echo "Error: DeepWiki MCP not available after 60 seconds"
            echo "Service logs:"
            cat ../deepwiki.log
            echo "Process status:"
            ps aux | grep node | grep -v grep || echo "Process not found"
            exit 1
          fi
          # Verify process is running
          ps aux | grep node | grep -v grep

      - name: Create necessary directories
        run: |
          # Create directories for documentation and images
          mkdir -p ${{ env.DOCS_DIR }}
          mkdir -p ${{ env.IMAGE_DIR }}
          echo "Created directories:"
          ls -la

      - name: Export ALL Markdown from DeepWiki
        run: |
          # Verify service is available before making the call
          if ! curl -s http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp > /dev/null 2>&1; then
            echo "Error: Service not available before export"
            echo "Service logs:"
            cat deepwiki.log || echo "No logs found"
            exit 1
          fi
          # Test connectivity first
          echo "Testing service connectivity..."
          curl -X POST http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp -H "Content-Type: application/json" -d '{"jsonrpc":"2.0","method":"tools/list","id":1}' -v || echo "Tools list check failed"
          # Validate DeepWiki URL accessibility
          echo "Validating DeepWiki URL accessibility..."
          # Check if the URL is accessible
          if curl -s --head "${{ env.DEEPWIKI_URL }}" | head -n 1 | grep -q "200 OK"; then
            echo "‚úÖ DeepWiki URL is accessible: ${{ env.DEEPWIKI_URL }}"
          else
            echo "‚ö†Ô∏è Warning: DeepWiki URL might not be accessible: ${{ env.DEEPWIKI_URL }}"
          fi
          # Create JSON payload to get ALL pages (maxDepth: 1, mode: "pages")
          JSON_PAYLOAD='{"jsonrpc":"2.0","method":"tools/call","params":{"name":"deepwiki_fetch","arguments":{"url":"'${{ env.DEEPWIKI_URL }}'","maxDepth":1,"mode":"pages"}},"id":1}'
          echo "JSON-RPC 2.0 payload to send (ALL PAGES):"
          echo "$JSON_PAYLOAD" | jq . || echo "$JSON_PAYLOAD"
          # Validate JSON syntax
          if echo "$JSON_PAYLOAD" | jq . > /dev/null 2>&1; then
            echo "‚úÖ JSON payload is valid"
          else
            echo "‚ùå JSON payload is invalid"
            exit 1
          fi
          # Make the POST request to get ALL pages
          echo "Making POST request to get ALL DeepWiki pages..."
          HTTP_CODE=$(curl -X POST http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp \
            -H "Content-Type: application/json" \
            -d "$JSON_PAYLOAD" \
            -o ${{ env.DOCS_DIR }}/all-pages-raw.json \
            -w "%{http_code}" \
            -s)
          echo "HTTP status code: $HTTP_CODE"
          # Handle different HTTP response codes
          if [ "$HTTP_CODE" = "200" ]; then
            echo "‚úÖ Success: All pages request completed successfully"
          else
            echo "‚ùå Error HTTP $HTTP_CODE"
            echo "Response content:"
            cat ${{ env.DOCS_DIR }}/all-pages-raw.json
            
            # Try fallback with aggregate mode
            echo "Trying fallback with aggregate mode..."
            FALLBACK_PAYLOAD='{"jsonrpc":"2.0","method":"tools/call","params":{"name":"deepwiki_fetch","arguments":{"url":"'${{ env.DEEPWIKI_URL }}'","maxDepth":1,"mode":"aggregate"}},"id":1}'
            
            HTTP_CODE=$(curl -X POST http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp \
              -H "Content-Type: application/json" \
              -d "$FALLBACK_PAYLOAD" \
              -o ${{ env.DOCS_DIR }}/all-pages-raw.json \
              -w "%{http_code}" \
              -s)
            
            if [ "$HTTP_CODE" != "200" ]; then
              echo "Fallback method also failed with HTTP $HTTP_CODE"
              exit 1
            fi
            echo "‚úÖ Fallback method succeeded"
          fi
          # Verify file was created and is not empty
          if [ ! -f ${{ env.DOCS_DIR }}/all-pages-raw.json ]; then
            echo "Error: Response file was not created"
            exit 1
          fi
          if [ ! -s ${{ env.DOCS_DIR }}/all-pages-raw.json ]; then
            echo "Error: Response file is empty"
            echo "File content:"
            cat ${{ env.DOCS_DIR }}/all-pages-raw.json
            exit 1
          fi
          echo "‚úÖ All pages response received successfully:"
          ls -la ${{ env.DOCS_DIR }}/
          echo "Response structure:"
          jq -r 'keys' ${{ env.DOCS_DIR }}/all-pages-raw.json || echo "Not valid JSON, showing first 500 chars:"
          head -c 500 ${{ env.DOCS_DIR }}/all-pages-raw.json

      - name: Install Required Dependencies
        run: |
          echo "Installing required tools for processing (including BeautifulSoup and Selenium for web scraping)..."
          sudo apt-get update && sudo apt-get install -y jq curl wget python3-pip
          pip3 install beautifulsoup4 requests lxml selenium webdriver-manager
          echo "‚úÖ Dependencies installed including BeautifulSoup, Selenium, and WebDriver Manager"

      - name: Create Image Processing Helper Script
        run: |
          cat > image_processor.py << 'EOF'
          import subprocess
          import os
          import urllib.parse
          from pathlib import Path
          from bs4 import BeautifulSoup
          import requests

          def extract_and_download_images_with_soup(html_content, base_url, image_dir, downloaded_images):
              """Extract image URLs from HTML using BeautifulSoup and download them"""
              print(f"üîç Processing HTML content for images (length: {len(html_content)} chars)...")
              
              # Parse HTML with BeautifulSoup - much more robust than regex
              try:
                  soup = BeautifulSoup(html_content, 'html.parser')
              except Exception as e:
                  print(f"‚ùå Error parsing HTML with BeautifulSoup: {e}")
                  return {}
              
              # Find all img tags
              img_tags = soup.find_all('img')
              print(f"üñºÔ∏è Found {len(img_tags)} <img> tags in HTML")
              
              if len(img_tags) > 0:
                  print("üìù Sample img tags found:")
                  for i, img in enumerate(img_tags[:3]):  # Show first 3
                      print(f"  {i+1}. {str(img)[:100]}...")
              
              downloaded_paths = {}
              
              for i, img_tag in enumerate(img_tags):
                  src = img_tag.get('src')
                  alt = img_tag.get('alt', '')
                  
                  if not src:
                      print(f"‚ö†Ô∏è Skipping img tag {i+1}: no src attribute")
                      continue
                  
                  # Skip if already downloaded
                  if src in downloaded_images:
                      downloaded_paths[src] = downloaded_images[src]
                      print(f"‚ôªÔ∏è Already downloaded: {src}")
                      continue
                  
                  try:
                      # Build absolute URL
                      if src.startswith('http://') or src.startswith('https://'):
                          full_url = src
                      elif src.startswith('//'):
                          full_url = 'https:' + src
                      elif src.startswith('/'):
                          # Extract base domain from base_url
                          parsed = urllib.parse.urlparse(base_url)
                          full_url = f"{parsed.scheme}://{parsed.netloc}{src}"
                      else:
                          # Relative URL
                          full_url = urllib.parse.urljoin(base_url, src)
                      
                      print(f"üåê Processing image {i+1}/{len(img_tags)}: {full_url}")
                      
                      # Generate descriptive filename
                      url_path = urllib.parse.urlparse(full_url).path
                      if url_path and url_path != '/':
                          filename = os.path.basename(url_path)
                          if not filename or '.' not in filename:
                              # No extension, try to get from URL or use default
                              filename = f"image_{i+1}.png"
                      else:
                          # Generate descriptive name from alt text or use default
                          if alt:
                              safe_alt = "".join(c for c in alt if c.isalnum() or c in (' ', '-', '_')).strip()
                              safe_alt = safe_alt.replace(' ', '_')[:30]  # Limit length
                              filename = f"{safe_alt}_{i+1}.png"
                          else:
                              filename = f"image_{i+1}.png"
                      
                      # Ensure unique filename
                      base_name, ext = os.path.splitext(filename)
                      if not ext:
                          ext = '.png'  # Default extension
                      counter = 1
                      while os.path.exists(os.path.join(image_dir, filename)):
                          filename = f"{base_name}_{counter}{ext}"
                          counter += 1
                      
                      local_path = os.path.join(image_dir, filename)
                      
                      # Download using requests with proper headers
                      success = download_image_with_requests(full_url, local_path)
                      
                      if success:
                          # Store relative path for markdown
                          relative_path = f"images/{filename}"
                          downloaded_images[src] = relative_path
                          downloaded_paths[src] = (relative_path, alt)
                          print(f"‚úÖ Downloaded: {filename} -> {relative_path}")
                      else:
                          print(f"‚ùå Failed to download: {full_url}")
                          downloaded_paths[src] = (None, alt)
                      
                  except Exception as e:
                      print(f"‚ùå Error processing image {src}: {e}")
                      downloaded_paths[src] = (None, alt)
              
              print(f"üìä Image processing summary: {len(downloaded_paths)} total, {len([p for p in downloaded_paths.values() if p[0]])} successful downloads")
              return downloaded_paths

          def download_image_with_requests(url, dest_path):
              """Download image using requests with proper error handling"""
              try:
                  # Create directory if it doesn't exist
                  os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                  
                  # Download with requests (more reliable than curl for this use case)
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (compatible; GitHubActions; +https://github.com)',
                      'Accept': 'image/*,*/*;q=0.8',
                      'Accept-Language': 'en-US,en;q=0.5',
                      'Accept-Encoding': 'gzip, deflate',
                      'DNT': '1',
                      'Connection': 'keep-alive',
                      'Upgrade-Insecure-Requests': '1',
                  }
                  
                  response = requests.get(url, headers=headers, timeout=30, stream=True)
                  response.raise_for_status()
                  
                  # Check if we actually got image content
                  content_type = response.headers.get('content-type', '')
                  if not content_type.startswith('image/'):
                      print(f"‚ö†Ô∏è Warning: Content-Type is '{content_type}', may not be an image")
                  
                  # Write the image data
                  with open(dest_path, 'wb') as f:
                      for chunk in response.iter_content(chunk_size=8192):
                          f.write(chunk)
                  
                  # Verify file was created and has content
                  if os.path.exists(dest_path) and os.path.getsize(dest_path) > 0:
                      file_size = os.path.getsize(dest_path)
                      print(f"  üìÅ File saved: {os.path.basename(dest_path)} ({file_size} bytes)")
                      return True
                  else:
                      print(f"  ‚ùå File not created or empty: {dest_path}")
                      return False
                      
              except requests.exceptions.RequestException as e:
                  print(f"  ‚ùå Request error: {e}")
                  return False
              except Exception as e:
                  print(f"  ‚ùå Unexpected error: {e}")
                  return False

          def convert_html_to_markdown_with_images(html_content, image_paths):
              """Convert HTML to Markdown while replacing <img> tags with Markdown syntax"""
              print(f"üîÑ Converting HTML to Markdown with {len(image_paths)} image replacements")
              
              # Parse HTML again for replacement
              try:
                  soup = BeautifulSoup(html_content, 'html.parser')
              except Exception as e:
                  print(f"‚ùå Error parsing HTML for conversion: {e}")
                  return html_content
              
              # Replace each img tag with Markdown syntax
              replacements_made = 0
              for img_tag in soup.find_all('img'):
                  src = img_tag.get('src')
                  if src in image_paths:
                      local_path, alt_text = image_paths[src]
                      if local_path:
                          # Create Markdown image syntax
                          markdown_img = f"![{alt_text or 'Image'}]({local_path})"
                          print(f"  üîÑ Replacing <img> with: {markdown_img}")
                      else:
                          # Image couldn't be downloaded
                          filename = os.path.basename(src) if src else 'unknown'
                          markdown_img = f"[IMAGE NOT AVAILABLE: {filename}]"
                          print(f"  ‚ö†Ô∏è Replacing with placeholder: {markdown_img}")
                      
                      # Replace the img tag in the soup
                      img_tag.replace_with(markdown_img)
                      replacements_made += 1
              
              print(f"‚úÖ Made {replacements_made} image replacements")
              return str(soup)

          # Legacy function names for backward compatibility
          def extract_and_download_images(html_content, base_url, image_dir, downloaded_images):
              """Legacy wrapper - now uses BeautifulSoup instead of regex"""
              return extract_and_download_images_with_soup(html_content, base_url, image_dir, downloaded_images)
          EOF
          echo "‚úÖ Image processing helper created (now using BeautifulSoup instead of unreliable regex)"

      - name: Create Content Processing Helper Script
        run: |
          cat > content_processor.py << 'EOF'
          import re
          from image_processor import extract_and_download_images_with_soup, convert_html_to_markdown_with_images

          def clean_deepwiki_content(content, preserve_images=True, image_dir="", base_url="", downloaded_images={}):
              """Clean and format DeepWiki content for GitHub Wiki"""
              
              # Extract and download images FIRST using robust BeautifulSoup parsing
              image_paths = {}
              if preserve_images:
                  print("üñºÔ∏è Starting image extraction and download process...")
                  image_paths = extract_and_download_images_with_soup(content, base_url, image_dir, downloaded_images)
                  print(f"üìä Image extraction completed: {len(image_paths)} images processed")
              
              # Convert HTML to Markdown with images
              if image_paths:
                  print("üîÑ Converting HTML to Markdown with image replacements...")
                  content = convert_html_to_markdown_with_images(content, image_paths)
                  print("‚úÖ HTML to Markdown conversion completed")
              else:
                  print("‚ÑπÔ∏è No images found, proceeding with text-only conversion")
              
              # Continue with existing cleaning logic...
              # Eliminar bloque de navegaci√≥n/√≠ndice al principio (men√∫ DeepWiki)
              content = re.sub(r'^(?:[\s\S]{0,300}?)(?:github-actions\[bot\].*?revision\n)?(?:[\s\S]{0,300}?)(?:System Architecture|Overview|Streamlit Dashboard|Machine Learning Models|NLP Pipeline|Expert System|Developer Guide|Other Sections)[\s\S]+?(?=\n# |\n## |\n### |\n\Z)', '', content, flags=re.MULTILINE)
              # Eliminar secci√≥n '/' y sus repeticiones
              content = re.sub(r'^#? ?/?\n(?:Documentation\n)+', '', content, flags=re.MULTILINE)
              content = re.sub(r'^/?\n+', '', content, flags=re.MULTILINE)
              # Remove DeepWiki UI elements
              content = re.sub(r'.*?DeepWiki.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Powered by Devin.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Share.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Last indexed:.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Try DeepWiki.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Auto-refresh not enabled yet.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Which repo would you like to understand.*?\n', '', content, flags=re.IGNORECASE)
              
              # Remove navigation elements
              content = re.sub(r'- Overview\n- System Architecture.*?- Getting Started\n', '', content, flags=re.DOTALL)
              content = re.sub(r'Menu\n', '', content)
              content = re.sub(r'### On this page.*?- Getting Started\n', '', content, flags=re.DOTALL)
              
              # Remove source file references (they clutter the wiki)
              content = re.sub(r'Relevant source files.*?\n\n', '', content, flags=re.DOTALL)
              content = re.sub(r'Sources:.*?\n', '', content, flags=re.MULTILINE)
              
              # Clean up multiple consecutive newlines
              content = re.sub(r'\n{3,}', '\n\n', content)
              
              # Remove empty sections
              content = re.sub(r'\n## \n', '', content)
              content = re.sub(r'\n### \n', '', content)
              
              # Fix malformed headers
              content = re.sub(r'^([#]+)\s*$', '', content, flags=re.MULTILINE)
              
              # Remove VforVitorio/F1_Strat_Manager title duplicates
              content = re.sub(r'^# /VforVitorio/F1_Strat_Manager.*?\n', '', content, flags=re.MULTILINE)
              content = re.sub(r'VforVitorio/F1_Strat_Manager \| DeepWiki.*?\n', '', content)
              
              # Clean up beginning of content
              content = content.strip()
              
              return content
              
          def get_page_title_from_content(content):
              """Extract a clean title from content"""
              lines = content.split('\n')
              for line in lines:
                  if line.strip().startswith('# ') and len(line.strip()) > 2:
                      title = line.strip()[2:].strip()
                      # Clean the title
                      title = re.sub(r'^/.*?/', '', title)  # Remove leading path
                      if title:
                          return title
              return "Documentation"

          def safe_filename(title, fallback_idx=None):
              """Genera un nombre de archivo seguro a partir del t√≠tulo. Si el t√≠tulo es vac√≠o, usa un fallback √∫nico."""
              if not title or len(title.strip()) == 0:
                  if fallback_idx is not None:
                      return f"unknown-section-{fallback_idx}"
                  return "unknown-section"
              
              filename = title.lower().strip()
              # Elimina caracteres especiales y reemplaza por guiones
              filename = re.sub(r'[^\w\s-]', '', filename)
              filename = re.sub(r'[-\s]+', '-', filename)
              filename = filename.strip('-')  # Quita guiones al inicio/fin
              if not filename:
                  if fallback_idx is not None:
                      return f"unknown-section-{fallback_idx}"
                  return "unknown-section"
              return filename
          EOF
          echo "‚úÖ Content processing helper created (now with improved BeautifulSoup-based image handling)"

      - name: Create Section Categorization Helper Script
        run: |
          cat > section_categorizer.py << 'EOF'
          from content_processor import safe_filename

          def categorize_section(title, fallback_idx=None):
              """Categoriza secciones seg√∫n la jerarqu√≠a especificada"""
              title_lower = title.lower() if title else ''
              
              # Main sections
              if any(keyword in title_lower for keyword in ['overview', 'introducci√≥n', 'introduction']):
                  return ('main', 'Overview', '01-overview.md')
              elif any(keyword in title_lower for keyword in ['streamlit', 'dashboard', 'interfaz']):
                  return ('main', 'Streamlit Dashboard', '02-streamlit-dashboard.md')
              elif any(keyword in title_lower for keyword in ['machine learning', 'ml', 'modelo']):
                  return ('main', 'Machine Learning Models', '03-machine-learning-models.md')
              elif any(keyword in title_lower for keyword in ['nlp', 'natural language', 'radio', 'processing']):
                  return ('main', 'NLP Pipeline', '04-nlp-pipeline.md')
              elif any(keyword in title_lower for keyword in ['expert', 'rules', 'engine', 'reglas']):
                  return ('main', 'Expert System', '05-expert-system.md')
              elif any(keyword in title_lower for keyword in ['developer', 'api', 'integration', 'guide']):
                  return ('main', 'Developer Guide', '06-developer-guide.md')
              
              # Streamlit Dashboard subsections
              elif any(keyword in title_lower for keyword in ['strategy recommendations', 'recomendaciones']):
                  return ('sub', 'Streamlit Dashboard', '02-01-strategy-recommendations-view.md')
              elif any(keyword in title_lower for keyword in ['gap analysis view', 'an√°lisis de gaps']):
                  return ('sub', 'Streamlit Dashboard', '02-02-gap-analysis-view.md')
              elif any(keyword in title_lower for keyword in ['radio analysis view', 'an√°lisis de radio']):
                  return ('sub', 'Streamlit Dashboard', '02-03-radio-analysis-view.md')
              elif any(keyword in title_lower for keyword in ['time predictions', 'predicciones']):
                  return ('sub', 'Streamlit Dashboard', '02-04-time-predictions-view.md')
              elif any(keyword in title_lower for keyword in ['chat interface', 'interfaz de chat']):
                  return ('sub', 'Streamlit Dashboard', '02-05-strategy-chat-interface.md')
              
              # Machine Learning subsections
              elif any(keyword in title_lower for keyword in ['lap time prediction', 'predicci√≥n de tiempos']):
                  return ('sub', 'Machine Learning Models', '03-01-lap-time-prediction.md')
              elif any(keyword in title_lower for keyword in ['tire degradation', 'degradaci√≥n', 'neum√°ticos']):
                  return ('sub', 'Machine Learning Models', '03-02-tire-degradation-modeling.md')
              elif any(keyword in title_lower for keyword in ['vision', 'gap calculation', 'c√°lculo']):
                  return ('sub', 'Machine Learning Models', '03-03-vision-based-gap-calculation.md')
              
              # NLP Pipeline subsections
              elif any(keyword in title_lower for keyword in ['transcription', 'transcripci√≥n']):
                  return ('sub', 'NLP Pipeline', '04-01-radio-transcription.md')
              elif any(keyword in title_lower for keyword in ['sentiment', 'intent', 'an√°lisis']):
                  return ('sub', 'NLP Pipeline', '04-02-sentiment-intent-analysis.md')
              elif any(keyword in title_lower for keyword in ['named entity', 'ner', 'entidades']):
                  return ('sub', 'NLP Pipeline', '04-03-named-entity-recognition.md')
              
              # Expert System subsections
              elif any(keyword in title_lower for keyword in ['degradation rules', 'reglas de degradaci√≥n']):
                  return ('sub', 'Expert System', '05-01-degradation-rules.md')
              elif any(keyword in title_lower for keyword in ['gap analysis rules', 'reglas de gaps']):
                  return ('sub', 'Expert System', '05-02-gap-analysis-rules.md')
              elif any(keyword in title_lower for keyword in ['radio message rules', 'reglas de radio']):
                  return ('sub', 'Expert System', '05-03-radio-message-rules.md')
              elif any(keyword in title_lower for keyword in ['integrated rule engine', 'motor de reglas']):
                  return ('sub', 'Expert System', '05-04-integrated-rule-engine.md')
              
              # Developer Guide subsections
              elif any(keyword in title_lower for keyword in ['api reference', 'referencia api']):
                  return ('sub', 'Developer Guide', '06-01-api-reference.md')
              elif any(keyword in title_lower for keyword in ['integration guide', 'gu√≠a de integraci√≥n']):
                  return ('sub', 'Developer Guide', '06-02-integration-guide.md')
              elif any(keyword in title_lower for keyword in ['system architecture', 'arquitectura']):
                  return ('sub', 'Overview', '01-01-system-architecture.md')
              elif any(keyword in title_lower for keyword in ['installation', 'setup', 'instalaci√≥n']):
                  return ('sub', 'Overview', '01-02-installation-setup.md')
              # Fallback seguro
              return ('misc', 'Other', f'99-{safe_filename(title, fallback_idx)}.md')
          EOF
          echo "‚úÖ Section categorization helper created"

      - name: Process DeepWiki Response and Extract Content
        run: |
          echo "Processing multiple pages from DeepWiki response..."

          python3 << 'EOF'
          import json
          import sys
          import os
          import re
          from pathlib import Path
          from bs4 import BeautifulSoup
          from content_processor import clean_deepwiki_content, get_page_title_from_content
          from section_categorizer import categorize_section

          # Environment variables
          DOCS_DIR = os.environ.get('DOCS_DIR', 'docs-md')
          IMAGE_DIR = os.environ.get('IMAGE_DIR', 'docs-md/images')
          DEEPWIKI_URL = os.environ.get('DEEPWIKI_URL', 'https://deepwiki.com/VforVitorio/F1_Strat_Manager')

          # Create image directory
          Path(IMAGE_DIR).mkdir(parents=True, exist_ok=True)

          # Track all downloaded images
          downloaded_images = {}

          # Read the JSON response
          try:
              with open(f'{DOCS_DIR}/all-pages-raw.json', 'r', encoding='utf-8') as f:
                  data = json.load(f)
              
              print(f"Response structure: {list(data.keys())}")
              
              # Handle different response structures
              pages_content = []
              
              if 'result' in data and 'content' in data['result']:
                  if isinstance(data['result']['content'], list):
                      # Multiple content items
                      for item in data['result']['content']:
                          if 'text' in item:
                              pages_content.append(item['text'])
                  else:
                      # Single content item
                      if 'text' in data['result']['content']:
                          pages_content.append(data['result']['content']['text'])
              elif 'result' in data and isinstance(data['result'], str):
                  # Direct string result
                  pages_content.append(data['result'])
              else:
                  print("‚ùå Unexpected response structure")
                  print(f"Data keys: {list(data.keys())}")
                  sys.exit(1)
              
              print(f"Found {len(pages_content)} content sections")
              
              # üîç INVESTIGATE: Analyze content structure for image patterns
              print('\nüîç CONTENT ANALYSIS:')
              print('=' * 50)
              
              total_content = '\n'.join(str(content) for content in pages_content)
              print(f'üìÑ Total content length: {len(total_content)} characters')
              
              # Look for different image patterns across all content
              img_tags = total_content.count('<img')
              svg_tags = total_content.count('<svg')
              background_imgs = total_content.count('background-image')
              data_src = total_content.count('data-src')
              src_attrs = total_content.count('src=')
              base64_imgs = total_content.count('data:image')
              
              print(f'üìä Image pattern analysis across all sections:')
              print(f'   ‚Ä¢ <img tags: {img_tags}')
              print(f'   ‚Ä¢ <svg tags: {svg_tags}')
              print(f'   ‚Ä¢ background-image: {background_imgs}')
              print(f'   ‚Ä¢ data-src attributes: {data_src}')
              print(f'   ‚Ä¢ src attributes: {src_attrs}')
              print(f'   ‚Ä¢ base64 images: {base64_imgs}')
              
              # Look for React/Vue components or other image patterns
              react_img = total_content.count('Image')
              react_figure = total_content.count('Figure')
              media_patterns = total_content.count('media')
              asset_patterns = total_content.count('asset')
              cdn_patterns = total_content.count('cdn')
              
              print(f'\\nüîß Framework and content patterns:')
              print(f'   ‚Ä¢ Image components: {react_img}')
              print(f'   ‚Ä¢ Figure components: {react_figure}')
              print(f'   ‚Ä¢ Media references: {media_patterns}')
              print(f'   ‚Ä¢ Asset references: {asset_patterns}')
              print(f'   ‚Ä¢ CDN references: {cdn_patterns}')
              
              # Sample content for investigation
              if len(total_content) > 2000:
                  sample = total_content[:2000] + '...'
                  print(f'\\nüìù Content sample (first 2000 chars):')
                  print('-' * 30)
                  print(repr(sample))
                  print('-' * 30)
              
              # Look for alternative patterns like markdown images, CSS references, etc.
              markdown_imgs = total_content.count('![')
              css_urls = total_content.count('url(')
              http_refs = total_content.count('http')
              
              print(f'\\nüåê Alternative image patterns:')
              print(f'   ‚Ä¢ Markdown images (![): {markdown_imgs}')
              print(f'   ‚Ä¢ CSS url() references: {css_urls}')
              print(f'   ‚Ä¢ HTTP/HTTPS links: {http_refs}')
              
              print('=' * 50)
              print('üîç End of content analysis\\n')
              
              # üåê WEB SCRAPING: Extract SVG diagrams directly from DeepWiki web interface
              print('\nüåê EXTRACTING SVG DIAGRAMS FROM WEB INTERFACE:')
              print('=' * 60)
              
              import requests
              from selenium import webdriver
              from selenium.webdriver.chrome.options import Options
              from selenium.webdriver.common.by import By
              from selenium.webdriver.support.ui import WebDriverWait
              from selenium.webdriver.support import expected_conditions as EC
              from webdriver_manager.chrome import ChromeDriverManager
              from selenium.webdriver.chrome.service import Service
              import time
              import base64
              
              def extract_svg_diagrams(url):
                  """Extract SVG diagrams from DeepWiki web interface"""
                  print(f"üîç Scraping SVG diagrams from: {url}")
                  
                  # Setup headless Chrome
                  chrome_options = Options()
                  chrome_options.add_argument("--headless")
                  chrome_options.add_argument("--no-sandbox")
                  chrome_options.add_argument("--disable-dev-shm-usage")
                  chrome_options.add_argument("--disable-gpu")
                  chrome_options.add_argument("--window-size=1920,1080")
                  
                  try:
                      # Setup Chrome driver with webdriver-manager
                      service = Service(ChromeDriverManager().install())
                      driver = webdriver.Chrome(service=service, options=chrome_options)
                      driver.get(url)
                      
                      # Wait for page to load completely
                      print("‚è≥ Waiting for page to load...")
                      time.sleep(5)
                      
                      # Wait for any dynamic content
                      try:
                          WebDriverWait(driver, 10).until(
                              EC.presence_of_element_located((By.TAG_NAME, "svg"))
                          )
                          print("‚úÖ SVG elements detected!")
                      except:
                          print("‚ö†Ô∏è No SVG elements found with WebDriverWait, continuing...")
                      
                      # Extract all SVG elements
                      svg_elements = driver.find_elements(By.TAG_NAME, "svg")
                      print(f"üìä Found {len(svg_elements)} SVG elements")
                      
                      extracted_svgs = []
                      
                      for i, svg in enumerate(svg_elements):
                          try:
                              # Get SVG content
                              svg_html = svg.get_attribute('outerHTML')
                              
                              if svg_html and len(svg_html) > 100:  # Skip very small SVGs
                                  # Extract dimensions and content info
                                  width = svg.get_attribute('width') or 'auto'
                                  height = svg.get_attribute('height') or 'auto'
                                  viewbox = svg.get_attribute('viewBox') or 'not-set'
                                  
                                  print(f"  üìã SVG {i+1}: width={width}, height={height}, viewBox={viewbox}")
                                  print(f"      Content length: {len(svg_html)} characters")
                                  
                                  # Determine diagram type based on content
                                  diagram_type = "unknown"
                                  if "flowchart" in svg_html.lower():
                                      diagram_type = "flowchart"
                                  elif "mermaid" in svg_html.lower():
                                      diagram_type = "mermaid"
                                  elif "system" in svg_html.lower() or "architecture" in svg_html.lower():
                                      diagram_type = "system-architecture"
                                  
                                  # Save SVG file
                                  svg_filename = f"diagram-{diagram_type}-{i+1}.svg"
                                  svg_path = f"{IMAGE_DIR}/{svg_filename}"
                                  
                                  with open(svg_path, 'w', encoding='utf-8') as f:
                                      # Add XML declaration and clean SVG
                                      clean_svg = '<?xml version="1.0" encoding="UTF-8"?>\\n' + svg_html
                                      f.write(clean_svg)
                                  
                                  extracted_svgs.append({
                                      'filename': svg_filename,
                                      'path': svg_path,
                                      'type': diagram_type,
                                      'width': width,
                                      'height': height,
                                      'size': len(svg_html)
                                  })
                                  
                                  print(f"      ‚úÖ Saved as: {svg_filename}")
                          
                          except Exception as e:
                              print(f"      ‚ùå Error processing SVG {i+1}: {e}")
                      
                      # Also look for any canvas elements or other image containers
                      canvas_elements = driver.find_elements(By.TAG_NAME, "canvas")
                      if canvas_elements:
                          print(f"\\nüé® Found {len(canvas_elements)} canvas elements")
                          for i, canvas in enumerate(canvas_elements):
                              try:
                                  # Try to extract canvas as image
                                  canvas_data = driver.execute_script(
                                      "return arguments[0].toDataURL('image/png');", canvas
                                  )
                                  if canvas_data:
                                      # Save canvas as PNG
                                      img_data = canvas_data.split(',')[1]
                                      img_binary = base64.b64decode(img_data)
                                      
                                      canvas_filename = f"canvas-diagram-{i+1}.png"
                                      canvas_path = f"{IMAGE_DIR}/{canvas_filename}"
                                      
                                      with open(canvas_path, 'wb') as f:
                                          f.write(img_binary)
                                      
                                      extracted_svgs.append({
                                          'filename': canvas_filename,
                                          'path': canvas_path,
                                          'type': 'canvas-diagram',
                                          'format': 'png'
                                      })
                                      
                                      print(f"      ‚úÖ Canvas saved as: {canvas_filename}")
                              except Exception as e:
                                  print(f"      ‚ùå Error processing canvas {i+1}: {e}")
                      
                      return extracted_svgs
                      
                  except Exception as e:
                      print(f"‚ùå Error during web scraping: {e}")
                      return []
                  finally:
                      try:
                          driver.quit()
                      except:
                          pass
              
              # Extract diagrams from the main DeepWiki page
              extracted_diagrams = extract_svg_diagrams(DEEPWIKI_URL)
              
              print(f"\\nüìä EXTRACTION SUMMARY:")
              print(f"   ‚Ä¢ Total diagrams extracted: {len(extracted_diagrams)}")
              for diag in extracted_diagrams:
                  print(f"   ‚Ä¢ {diag['filename']} ({diag['type']}) - {diag.get('size', 'unknown')} chars")
              
              print('=' * 60)
              print('üåê End of SVG extraction\\n')
              
              # Structure to organize content by hierarchy
              organized_content = {
                  'main_sections': {},
                  'subsections': {},
                  'misc': []
              }
              
              # Process each page/section
              for i, content in enumerate(pages_content):
                  print(f"\nüîÑ Processing content section {i+1}/{len(pages_content)}...")
                  print(f"Content length: {len(content)} characters")
                  
                  # üîç SECTION ANALYSIS: Check this specific section for image patterns
                  section_img_tags = str(content).count('<img')
                  section_src_attrs = str(content).count('src=')
                  section_markdown_imgs = str(content).count('![')
                  section_http_refs = str(content).count('http')
                  
                  print(f"üìä Section {i+1} image analysis:")
                  print(f"   ‚Ä¢ <img tags: {section_img_tags}")
                  print(f"   ‚Ä¢ src attributes: {section_src_attrs}")
                  print(f"   ‚Ä¢ Markdown images: {section_markdown_imgs}")
                  print(f"   ‚Ä¢ HTTP refs: {section_http_refs}")
                  
                  # Show a smaller sample of this section's content if it has potential images
                  if section_img_tags > 0 or section_src_attrs > 0 or section_markdown_imgs > 0:
                      sample_content = str(content)[:800] + '...' if len(str(content)) > 800 else str(content)
                      print(f"üìù Section {i+1} content sample (potential images found):")
                      print('-' * 20)
                      print(repr(sample_content))
                      print('-' * 20)
                  
                  # Validate content is not None or empty
                  if not content or len(str(content).strip()) == 0:
                      print(f"Skipping empty content section {i+1}")
                      continue
                  
                  # Ensure content is string
                  if not isinstance(content, str):
                      print(f"Converting content to string for section {i+1}")
                      content = str(content)
                  
                  # Debug: Check for images in raw content before cleaning using BeautifulSoup
                  from bs4 import BeautifulSoup
                  try:
                      soup = BeautifulSoup(content, 'html.parser')
                      img_tags = soup.find_all('img')
                      total_imgs_before = len(img_tags)
                      print(f"üñºÔ∏è Images found in raw content (BeautifulSoup): {total_imgs_before}")
                      
                      if total_imgs_before > 0:
                          print("üìù Sample image tags found:")
                          for j, img in enumerate(img_tags[:3]):  # Show first 3
                              src = img.get('src', 'No src')
                              alt = img.get('alt', 'No alt')
                              print(f"  {j+1}. src='{src[:60]}...' alt='{alt[:30]}...'")
                  except Exception as e:
                      print(f"‚ö†Ô∏è Error parsing HTML for image preview: {e}")
                      # Fallback to regex for basic counting
                      img_check_patterns = [r'<img[^>]*>', r'!\[.*?\]\([^)]*\)']
                      total_imgs_before = sum(len(re.findall(p, content, re.IGNORECASE)) for p in img_check_patterns)
                      print(f"Images found in raw content (regex fallback): {total_imgs_before}")
                  
                  # Clean the content (with image support)
                  cleaned_content = clean_deepwiki_content(
                      content, 
                      preserve_images=True, 
                      image_dir=IMAGE_DIR, 
                      base_url=DEEPWIKI_URL, 
                      downloaded_images=downloaded_images
                  )
                  
                  if len(cleaned_content.strip()) < 50:  # Skip very short content
                      print(f"Skipping short content section {i+1}")
                      continue
                  
                  # Get title and categorize
                  title = get_page_title_from_content(cleaned_content)
                  # Validaci√≥n de t√≠tulo vac√≠o
                  if not title or len(title.strip()) == 0:
                      print(f"Warning: Empty title found in section {i+1}, using fallback")
                      title = f"Section_{i+1}"
                  section_type, parent_section, filename = categorize_section(title, fallback_idx=i+1)
                  # Validaci√≥n adicional para filename
                  if not filename or filename == '.md' or filename.startswith('99-.md'):
                      print(f"Warning: Invalid filename generated for '{title}', using fallback")
                      filename = f'99-section-{i+1}.md'
                  
                  # Ensure proper title format
                  if not cleaned_content.strip().startswith('# '):
                      cleaned_content = f'# {title}\n\n' + cleaned_content.strip()
                  
                  # Write individual file
                  filepath = f'{DOCS_DIR}/{filename}'
                  with open(filepath, 'w', encoding='utf-8') as f:
                      f.write(cleaned_content)
                  
                  # Organize content
                  if section_type == 'main':
                      organized_content['main_sections'][parent_section] = {
                          'filename': filename,
                          'title': title,
                          'content': cleaned_content
                      }
                  elif section_type == 'sub':
                      if parent_section not in organized_content['subsections']:
                          organized_content['subsections'][parent_section] = []
                      organized_content['subsections'][parent_section].append({
                          'filename': filename,
                          'title': title,
                          'content': cleaned_content
                      })
                  else:
                      organized_content['misc'].append({
                          'filename': filename,
                          'title': title,
                          'content': cleaned_content
                      })
                  
                  print(f"‚úÖ Created: {filename} (Category: {section_type}, Parent: {parent_section})")
              
              # Save organized content for next step
              import pickle
              with open('organized_content.pkl', 'wb') as f:
                  pickle.dump(organized_content, f)
              
              # Save downloaded images info
              with open('downloaded_images.pkl', 'wb') as f:
                  pickle.dump(downloaded_images, f)
              
              print(f"‚úÖ Content processing completed")
              
          except json.JSONDecodeError as e:
              print(f"‚ùå Error parsing JSON: {e}")
              print("Response might not be valid JSON, trying as plain text...")
              
              # Try to handle as plain text
              with open(f'{DOCS_DIR}/all-pages-raw.json', 'r', encoding='utf-8') as f:
                  content = f.read()
              
              print(f"Processing plain text content (length: {len(content)} chars)")
              print("First 500 chars of content:")
              print(content[:500])
              
              cleaned_content = clean_deepwiki_content(
                  content, 
                  preserve_images=True, 
                  image_dir=IMAGE_DIR, 
                  base_url=DEEPWIKI_URL, 
                  downloaded_images=downloaded_images
              )
              
              if not cleaned_content.strip().startswith('# '):
                  cleaned_content = '# F1 Strategy Manager\n\n' + cleaned_content.strip()
              
              with open(f'{DOCS_DIR}/f1-strat-manager-complete.md', 'w', encoding='utf-8') as f:
                  f.write(cleaned_content)
              
              print("‚úÖ Processed as plain text")
              
          except Exception as e:
              print(f"‚ùå Error processing content: {e}")
              sys.exit(1)
          EOF

      - name: Generate Comprehensive Documentation
        run: |
          echo "Creating structured comprehensive documentation..."

          python3 << 'EOF'
          import pickle
          import re
          import os
          from pathlib import Path

          # Environment variables
          DOCS_DIR = os.environ.get('DOCS_DIR', 'docs-md')
          IMAGE_DIR = os.environ.get('IMAGE_DIR', 'docs-md/images')

          try:
              # Load organized content
              with open('organized_content.pkl', 'rb') as f:
                  organized_content = pickle.load(f)
              
              # Load downloaded images info
              with open('downloaded_images.pkl', 'rb') as f:
                  downloaded_images = pickle.load(f)
              
              # Define the correct order of main sections
              section_order = [
                  'Overview',
                  'Streamlit Dashboard', 
                  'Machine Learning Models',
                  'NLP Pipeline',
                  'Expert System',
                  'Developer Guide'
              ]
              
              main_content = "# F1 Strategy Manager - Complete Documentation\n\n"
              main_content += "This document contains the complete documentation for the F1 Strategy Manager project, organized in a hierarchical structure.\n\n"
              main_content += "## Table of Contents\n\n"
              
              # Build table of contents
              for section in section_order:
                  if section in organized_content['main_sections']:
                      section_link = section.lower().replace(' ', '-').replace('.', '').replace('(', '').replace(')', '')
                      main_content += f"### {section}\n"
                      main_content += f"- **[{section}](#{section_link})**\n"
                      
                      # Add subsections
                      if section in organized_content['subsections']:
                          for subsection in organized_content['subsections'][section]:
                              sub_link = subsection['title'].lower().replace(' ', '-').replace('.', '').replace('(', '').replace(')', '')
                              main_content += f"  - [{subsection['title']}](#{sub_link})\n"
                      main_content += "\n"
              
              # Add miscellaneous sections
              if organized_content['misc']:
                  main_content += "### Other Sections\n"
                  for misc_item in organized_content['misc']:
                      misc_link = misc_item['title'].lower().replace(' ', '-').replace('.', '').replace('(', '').replace(')', '')
                      main_content += f"- [{misc_item['title']}](#{misc_link})\n"
                  main_content += "\n"
              
              main_content += "\n---\n\n"
              
              # Add all content in the specified order
              for section in section_order:
                  if section in organized_content['main_sections']:
                      main_section = organized_content['main_sections'][section]
                      main_content += main_section['content'] + "\n\n"
                      
                      # Add subsections immediately after their parent
                      if section in organized_content['subsections']:
                          for subsection in organized_content['subsections'][section]:
                              # Adjust header levels for subsections
                              subsection_content = subsection['content']
                              # Convert main headers to subheaders
                              subsection_content = re.sub(r'^# ', '## ', subsection_content, flags=re.MULTILINE)
                              subsection_content = re.sub(r'^## ', '### ', subsection_content, flags=re.MULTILINE)
                              subsection_content = re.sub(r'^### ', '#### ', subsection_content, flags=re.MULTILINE)
                              main_content += subsection_content + "\n\n"
                      
                      main_content += "---\n\n"
              
              # Add miscellaneous content at the end
              for misc_item in organized_content['misc']:
                  main_content += misc_item['content'] + "\n\n---\n\n"
              
              # Write main comprehensive file
              with open(f'{DOCS_DIR}/f1-strat-manager-complete.md', 'w', encoding='utf-8') as f:
                  f.write(main_content)
              
              # Count total files
              total_main = len(organized_content['main_sections'])
              total_sub = sum(len(subs) for subs in organized_content['subsections'].values())
              total_misc = len(organized_content['misc'])
              total_files = total_main + total_sub + total_misc + 1  # +1 for complete doc
              
              print(f"‚úÖ Created structured documentation:")
              print(f"  - Main sections: {total_main}")
              print(f"  - Subsections: {total_sub}")
              print(f"  - Miscellaneous: {total_misc}")
              print(f"  - Total files: {total_files}")
              
              # Report on downloaded images
              print(f"\n‚úÖ Downloaded images:")
              print(f"  - Total images processed: {len(downloaded_images)}")
              image_list = list(Path(IMAGE_DIR).glob('*'))
              print(f"  - Images successfully downloaded: {len(image_list)}")
              if image_list:
                  print("  - Image files:")
                  for img in image_list[:10]:  # Show first 10
                      print(f"    - {img.name}")
                  if len(image_list) > 10:
                      print(f"    ... and {len(image_list) - 10} more")
              
          except Exception as e:
              print(f"‚ùå Error generating documentation: {e}")
              import sys
              sys.exit(1)
          EOF

          echo "Final documentation files created:"
          ls -la ${{ env.DOCS_DIR }}/
          echo "Images downloaded:"
          ls -la ${{ env.IMAGE_DIR }}/ || echo "No images directory found"
          echo "Preview of main documentation:"
          head -30 ${{ env.DOCS_DIR }}/f1-strat-manager-complete.md

      - name: Checkout Wiki
        uses: actions/checkout@v3
        with:
          repository: ${{ github.repository }}.wiki
          token: ${{ secrets.WIKI_PAT }}
          path: ${{ env.WIKI_DIR }}

      - name: Copy All Documentation and Images to Wiki
        run: |
          # Verify we have files to copy
          if [ ! -d ${{ env.DOCS_DIR }} ] || [ -z "$(ls -A ${{ env.DOCS_DIR }}/*.md 2>/dev/null)" ]; then
            echo "‚ùå Error: No markdown files to copy"
            exit 1
          fi
          echo "üìÅ Files to copy:"
          ls -la ${{ env.DOCS_DIR }}/

          # Ensure wiki directory exists
          if [ ! -d ${{ env.WIKI_DIR }} ]; then
            echo "‚ùå Error: Wiki directory not found"
            exit 1
          fi
          echo "üìÇ Current wiki contents:"
          ls -la ${{ env.WIKI_DIR }}/

          # Copy ALL markdown files to wiki
          echo "üìÑ Copying Markdown files..."
          cp ${{ env.DOCS_DIR }}/*.md ${{ env.WIKI_DIR }}/
          MARKDOWN_COUNT=$(ls -1 ${{ env.DOCS_DIR }}/*.md | wc -l)
          echo "‚úÖ Copied $MARKDOWN_COUNT Markdown files"

          # Copy images directory with enhanced logging and error handling
          if [ -d "${{ env.IMAGE_DIR }}" ]; then
            IMAGE_COUNT=$(find "${{ env.IMAGE_DIR }}" -type f -name "*.png" -o -name "*.jpg" -o -name "*.jpeg" -o -name "*.gif" -o -name "*.svg" -o -name "*.webp" | wc -l)
            
            if [ $IMAGE_COUNT -gt 0 ]; then
              echo "üñºÔ∏è Found $IMAGE_COUNT image files to copy..."
              # Create images directory in wiki
              mkdir -p ${{ env.WIKI_DIR }}/images
              
              # Copy all images preserving directory structure
              echo "üìã Copying images to wiki/images/..."
              cp -r ${{ env.IMAGE_DIR }}/* ${{ env.WIKI_DIR }}/images/ 2>/dev/null || {
                echo "‚ö†Ô∏è Warning: Some image files could not be copied"
                # Try copying individual files instead
                find "${{ env.IMAGE_DIR }}" -type f \( -name "*.png" -o -name "*.jpg" -o -name "*.jpeg" -o -name "*.gif" -o -name "*.svg" -o -name "*.webp" \) -exec cp {} ${{ env.WIKI_DIR }}/images/ \;
              }
              
              # Verify images were copied
              COPIED_COUNT=$(find "${{ env.WIKI_DIR }}/images/" -type f 2>/dev/null | wc -l)
              echo "‚úÖ Successfully copied $COPIED_COUNT image files"
              
              if [ $COPIED_COUNT -gt 0 ]; then
                echo "üñºÔ∏è Images copied to wiki:"
                ls -la ${{ env.WIKI_DIR }}/images/ | head -10
                if [ $COPIED_COUNT -gt 10 ]; then
                  echo "   ... and $(($COPIED_COUNT - 10)) more images"
                fi
              fi
            else
              echo "‚ÑπÔ∏è No image files found in ${{ env.IMAGE_DIR }}"
            fi
          else
            echo "‚ÑπÔ∏è No images directory found (${{ env.IMAGE_DIR }})"
          fi

          # Create or update index page with hierarchical structure and dynamic SVG diagram listing
          echo "üìù Creating Home.md with SVG diagram detection..."

          # First create the basic Home.md structure
          cat > ${{ env.WIKI_DIR }}/Home.md << 'EOF'
          # Welcome to F1 Strategy Manager Wiki

          This wiki contains comprehensive documentation for the F1 Strategy Manager project, organized in a hierarchical structure for easy navigation. 
          It is generated through DeepWiki. To see the full documentation with all figures and interactive elements, please visit [DeepWiki](https://deepwiki.com/VforVitorio/F1_Strat_Manager).

          ## üìã Complete Documentation
          - **[üìñ Complete Documentation](f1-strat-manager-complete)** - Full system documentation with all sections organized hierarchically

          ## üñºÔ∏è System Diagrams
          The following SVG diagrams have been extracted directly from the DeepWiki interface:

          EOF

          # Add SVG diagram references dynamically
          if [ -d "${{ env.WIKI_DIR }}/images" ]; then
            SVG_COUNT=$(find "${{ env.WIKI_DIR }}/images" -name "diagram-*.svg" 2>/dev/null | wc -l)
            if [ $SVG_COUNT -gt 0 ]; then
              echo "Found $SVG_COUNT SVG diagrams to reference"
              for svg_file in ${{ env.WIKI_DIR }}/images/diagram-*.svg; do
                if [ -f "$svg_file" ]; then
                  filename=$(basename "$svg_file")
                  echo "- **[üèóÔ∏è $filename](images/$filename)** - System diagram extracted from DeepWiki" >> ${{ env.WIKI_DIR }}/Home.md
                fi
              done
            else
              echo "- *SVG diagrams will be extracted during the next workflow run*" >> ${{ env.WIKI_DIR }}/Home.md
            fi
          else
            echo "- *SVG diagrams will be extracted during the next workflow run*" >> ${{ env.WIKI_DIR }}/Home.md
          fi

          # Continue with the rest of the Home.md content
          cat >> ${{ env.WIKI_DIR }}/Home.md << 'EOF'

          ## üóÇÔ∏è Documentation Structure

          ### üîç 1. Overview
          - **[Overview](01-overview)** - Project introduction and general information
            - [System Architecture](01-01-system-architecture) - Overall system design and components
            - [Installation and Setup](01-02-installation-setup) - Getting started guide

          ### üìä 2. Streamlit Dashboard
          - **[Streamlit Dashboard](02-streamlit-dashboard)** - Interactive web interface
            - [Strategy Recommendations View](02-01-strategy-recommendations-view) - Strategic decision interface
            - [Gap Analysis View](02-02-gap-analysis-view) - Real-time gap tracking
            - [Radio Analysis View](02-03-radio-analysis-view) - Team radio insights
            - [Time Predictions View](02-04-time-predictions-view) - Lap time forecasting
            - [Strategy Chat Interface](02-05-strategy-chat-interface) - AI-powered strategy chat

          ### ü§ñ 3. Machine Learning Models
          - **[Machine Learning Models](03-machine-learning-models)** - AI/ML components
            - [Lap Time Prediction](03-01-lap-time-prediction) - Predictive models for lap times
            - [Tire Degradation Modeling](03-02-tire-degradation-modeling) - Tire performance analysis
            - [Vision-based Gap Calculation](03-03-vision-based-gap-calculation) - Computer vision for gap detection

          ### üé§ 4. NLP Pipeline
          - **[NLP Pipeline](04-nlp-pipeline)** - Natural Language Processing components
            - [Radio Transcription](04-01-radio-transcription) - Speech-to-text processing
            - [Sentiment and Intent Analysis](04-02-sentiment-intent-analysis) - Emotional and intent recognition
            - [Named Entity Recognition](04-03-named-entity-recognition) - Entity extraction from radio communications

          ### ‚öôÔ∏è 5. Expert System
          - **[Expert System](05-expert-system)** - Rule-based decision engine
            - [Degradation Rules](05-01-degradation-rules) - Tire degradation logic
            - [Gap Analysis Rules](05-02-gap-analysis-rules) - Gap calculation rules
            - [Radio Message Rules](05-03-radio-message-rules) - Communication analysis rules
            - [Integrated Rule Engine](05-04-integrated-rule-engine) - Unified rule processing

          ### üë®‚Äçüíª 6. Developer Guide
          - **[Developer Guide](06-developer-guide)** - Technical documentation for developers
            - [API Reference](06-01-api-reference) - Complete API documentation
            - [Integration Guide](06-02-integration-guide) - How to integrate with external systems

          ## üèéÔ∏è Project Overview

          The F1 Strategy Manager is an integrated AI-powered system for Formula 1 race strategy analysis and decision support, combining:

          - **ü§ñ Machine Learning Models** - Predictive analytics for lap times and tire performance
          - **üëÅÔ∏è Computer Vision** - Automated gap calculation from video feeds
          - **üé§ Natural Language Processing** - Radio communication analysis and insights
          - **‚öôÔ∏è Rule-based Expert Systems** - Strategic recommendations based on F1 expertise
          - **üìä Interactive Streamlit Dashboard** - User-friendly web interface for real-time analysis

          ---

          *üìù This documentation is automatically generated and updated from the project's DeepWiki documentation.*
          *üñºÔ∏è Images and diagrams are included where available. SVG diagrams are extracted directly from the DeepWiki web interface.*
          *If a diagram is missing, please check the [DeepWiki source](https://deepwiki.com/VforVitorio/F1_Strat_Manager).*
          EOF

          echo "üìÇ Final wiki contents:"
          ls -la ${{ env.WIKI_DIR }}/

      - name: Commit & Push changes
        working-directory: ${{ env.WIKI_DIR }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add .

          # Check if there are changes before committing
          if git diff --quiet --staged; then
            echo "‚ÑπÔ∏è No changes to commit"
          else
            # Count different types of files
            IMAGE_COUNT=$(find . -name "*.png" -o -name "*.jpg" -o -name "*.jpeg" -o -name "*.gif" -o -name "*.svg" -o -name "*.webp" | wc -l)
            MD_COUNT=$(find . -name "*.md" | wc -l)
            
            # Build comprehensive commit message
            COMMIT_MSG="üîÑ Update Complete Wiki from DeepWiki with SVG Extraction - $(date '+%Y-%m-%d %H:%M')"
            COMMIT_MSG="$COMMIT_MSG"$'\n'"üìÑ Updated $MD_COUNT Markdown files with hierarchical documentation"
            
            if [ $IMAGE_COUNT -gt 0 ]; then
              SVG_COUNT=$(find . -name "*.svg" | wc -l)
              PNG_COUNT=$(find . -name "*.png" | wc -l)
              COMMIT_MSG="$COMMIT_MSG"$'\n'"üñºÔ∏è Includes $IMAGE_COUNT images: $SVG_COUNT SVG diagrams + $PNG_COUNT other images"
            else
              COMMIT_MSG="$COMMIT_MSG"$'\n'"‚ÑπÔ∏è No images found in this update"
            fi
            
            COMMIT_MSG="$COMMIT_MSG"$'\n'$'\n'"üåê Features: Web scraping for SVG extraction, BeautifulSoup HTML parsing, Selenium automation"
            
            echo "üìù Commit message:"
            echo "$COMMIT_MSG"
            echo ""
            
            git commit -m "$COMMIT_MSG"
            git push
            echo "‚úÖ Changes pushed successfully to wiki"
            
            # Final summary
            echo ""
            echo "üéâ Wiki update completed successfully!"
            echo "üìä Summary:"
            echo "  - Markdown files: $MD_COUNT"
            echo "  - Images: $IMAGE_COUNT"
            echo "  - Using: BeautifulSoup HTML parsing (replaces unreliable regex)"
          fi
