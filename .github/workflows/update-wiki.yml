name: Update Wiki from DeepWiki

on:
  push:
    branches:
      - main

# Environment variables for cleaner workflow
env:
  DEEPWIKI_URL: "https://deepwiki.com/VforVitorio/F1_Strat_Manager"
  DEEPWIKI_MCP_PORT: "3000"
  IMAGE_DIR: "docs-md/images"
  WIKI_DIR: "wiki"
  DOCS_DIR: "docs-md"

jobs:
  update-wiki:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Clone DeepWiki MCP
        run: |
          git clone https://github.com/regenrek/deepwiki-mcp.git deepwiki-mcp
          cd deepwiki-mcp
          npm install

      - name: Build DeepWiki MCP
        working-directory: deepwiki-mcp
        run: |
          # Check if package.json exists and available scripts
          echo "Package.json scripts:"
          npm run || true
          # Try to build the project
          if npm run build 2>/dev/null; then
            echo "Build completed with npm run build"
          elif npm run compile 2>/dev/null; then
            echo "Build completed with npm run compile"
          elif npm run dist 2>/dev/null; then
            echo "Build completed with npm run dist"
          else
            echo "Trying manual build with typescript..."
            npx tsc || echo "Could not compile with tsc"
          fi
          # Verify that dist file exists
          echo "Checking compiled files:"
          ls -la
          if [ -d "dist" ]; then
            ls -la dist/
          else
            echo "dist directory not found"
            echo "Looking for .mjs files in project:"
            find . -name "*.mjs" -type f
          fi

      - name: Start DeepWiki MCP (HTTP mode)
        working-directory: deepwiki-mcp
        run: |
          # Verify that necessary files exist
          ls -la
          ls -la bin/
          # Verify that dist/index.mjs file exists
          if [ ! -f "dist/index.mjs" ]; then
            echo "Error: dist/index.mjs not found"
            echo "Available files:"
            find . -name "*.mjs" -type f
            exit 1
          fi
          # Start service in background
          node ./bin/cli.mjs --http --port ${{ env.DEEPWIKI_MCP_PORT }} > ../deepwiki.log 2>&1 &
          DEEPWIKI_PID=$!
          echo "DeepWiki PID: $DEEPWIKI_PID"
          # Wait and verify service is available
          for i in {1..30}; do
            if curl -s http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp > /dev/null 2>&1; then
              echo "DeepWiki MCP is available"
              break
            fi
            echo "Waiting for DeepWiki MCP to be available... attempt $i/30"
            sleep 2
          done
          # Verify once more - check /mcp endpoint
          if ! curl -s http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp > /dev/null 2>&1; then
            echo "Error: DeepWiki MCP not available after 60 seconds"
            echo "Service logs:"
            cat ../deepwiki.log
            echo "Process status:"
            ps aux | grep node | grep -v grep || echo "Process not found"
            exit 1
          fi
          # Verify process is running
          ps aux | grep node | grep -v grep

      - name: Create necessary directories
        run: |
          # Create directories for documentation and images
          mkdir -p ${{ env.DOCS_DIR }}
          mkdir -p ${{ env.IMAGE_DIR }}
          echo "Created directories:"
          ls -la

      - name: Export ALL Markdown from DeepWiki
        run: |
          # Verify service is available before making the call
          if ! curl -s http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp > /dev/null 2>&1; then
            echo "Error: Service not available before export"
            echo "Service logs:"
            cat deepwiki.log || echo "No logs found"
            exit 1
          fi
          # Test connectivity first
          echo "Testing service connectivity..."
          curl -X POST http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp -H "Content-Type: application/json" -d '{"jsonrpc":"2.0","method":"tools/list","id":1}' -v || echo "Tools list check failed"
          # Validate DeepWiki URL accessibility
          echo "Validating DeepWiki URL accessibility..."
          # Check if the URL is accessible
          if curl -s --head "${{ env.DEEPWIKI_URL }}" | head -n 1 | grep -q "200 OK"; then
            echo "‚úÖ DeepWiki URL is accessible: ${{ env.DEEPWIKI_URL }}"
          else
            echo "‚ö†Ô∏è Warning: DeepWiki URL might not be accessible: ${{ env.DEEPWIKI_URL }}"
          fi
          # Create JSON payload to get ALL pages (maxDepth: 1, mode: "pages")
          JSON_PAYLOAD='{"jsonrpc":"2.0","method":"tools/call","params":{"name":"deepwiki_fetch","arguments":{"url":"'${{ env.DEEPWIKI_URL }}'","maxDepth":1,"mode":"pages"}},"id":1}'
          echo "JSON-RPC 2.0 payload to send (ALL PAGES):"
          echo "$JSON_PAYLOAD" | jq . || echo "$JSON_PAYLOAD"
          # Validate JSON syntax
          if echo "$JSON_PAYLOAD" | jq . > /dev/null 2>&1; then
            echo "‚úÖ JSON payload is valid"
          else
            echo "‚ùå JSON payload is invalid"
            exit 1
          fi
          # Make the POST request to get ALL pages
          echo "Making POST request to get ALL DeepWiki pages..."
          HTTP_CODE=$(curl -X POST http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp \
            -H "Content-Type: application/json" \
            -d "$JSON_PAYLOAD" \
            -o ${{ env.DOCS_DIR }}/all-pages-raw.json \
            -w "%{http_code}" \
            -s)
          echo "HTTP status code: $HTTP_CODE"
          # Handle different HTTP response codes
          if [ "$HTTP_CODE" = "200" ]; then
            echo "‚úÖ Success: All pages request completed successfully"
          else
            echo "‚ùå Error HTTP $HTTP_CODE"
            echo "Response content:"
            cat ${{ env.DOCS_DIR }}/all-pages-raw.json
            
            # Try fallback with aggregate mode
            echo "Trying fallback with aggregate mode..."
            FALLBACK_PAYLOAD='{"jsonrpc":"2.0","method":"tools/call","params":{"name":"deepwiki_fetch","arguments":{"url":"'${{ env.DEEPWIKI_URL }}'","maxDepth":1,"mode":"aggregate"}},"id":1}'
            
            HTTP_CODE=$(curl -X POST http://localhost:${{ env.DEEPWIKI_MCP_PORT }}/mcp \
              -H "Content-Type: application/json" \
              -d "$FALLBACK_PAYLOAD" \
              -o ${{ env.DOCS_DIR }}/all-pages-raw.json \
              -w "%{http_code}" \
              -s)
            
            if [ "$HTTP_CODE" != "200" ]; then
              echo "Fallback method also failed with HTTP $HTTP_CODE"
              exit 1
            fi
            echo "‚úÖ Fallback method succeeded"
          fi
          # Verify file was created and is not empty
          if [ ! -f ${{ env.DOCS_DIR }}/all-pages-raw.json ]; then
            echo "Error: Response file was not created"
            exit 1
          fi
          if [ ! -s ${{ env.DOCS_DIR }}/all-pages-raw.json ]; then
            echo "Error: Response file is empty"
            echo "File content:"
            cat ${{ env.DOCS_DIR }}/all-pages-raw.json
            exit 1
          fi
          echo "‚úÖ All pages response received successfully:"
          ls -la ${{ env.DOCS_DIR }}/
          echo "Response structure:"
          jq -r 'keys' ${{ env.DOCS_DIR }}/all-pages-raw.json || echo "Not valid JSON, showing first 500 chars:"
          head -c 500 ${{ env.DOCS_DIR }}/all-pages-raw.json

      - name: Install Required Dependencies
        run: |
          echo "Installing required tools for processing..."
          sudo apt-get update && sudo apt-get install -y jq curl wget
          echo "‚úÖ Dependencies installed"

      - name: Create Image Processing Helper Script
        run: |
          cat > image_processor.py << 'EOF'
          import subprocess
          import re
          import os
          import urllib.parse
          from pathlib import Path

          def extract_and_download_images(html_content, base_url, image_dir, downloaded_images):
              """Extract image URLs from HTML and download them"""
              # Find all img tags with various attribute patterns
              img_patterns = [
                  r'<img\s+[^>]*src=["\'](.*?)["\'][^>]*>',
                  r'<img\s+[^>]*src=([^\s>]+)[^>]*>',
                  r'!\[([^\]]*)\]\(([^)]+)\)'  # Markdown images
              ]
              
              images_found = []
              
              for pattern in img_patterns:
                  matches = re.findall(pattern, html_content, re.IGNORECASE)
                  if pattern.startswith('!'):  # Markdown pattern
                      images_found.extend([(m[1], m[0]) for m in matches])
                  else:
                      # Extract alt text if available
                      for match in matches:
                          alt_match = re.search(r'alt=["\'](.*?)["\']', html_content[html_content.find(match)-100:html_content.find(match)+100])
                          alt_text = alt_match.group(1) if alt_match else ''
                          images_found.append((match, alt_text))
              
              downloaded_paths = {}
              
              for img_url, alt_text in images_found:
                  # Skip if already downloaded
                  if img_url in downloaded_images:
                      downloaded_paths[img_url] = downloaded_images[img_url]
                      continue
                  
                  try:
                      # Handle relative and absolute URLs
                      if img_url.startswith('http://') or img_url.startswith('https://'):
                          full_url = img_url
                      elif img_url.startswith('//'):
                          full_url = 'https:' + img_url
                      elif img_url.startswith('/'):
                          # Extract base domain from base_url
                          parsed = urllib.parse.urlparse(base_url)
                          full_url = f"{parsed.scheme}://{parsed.netloc}{img_url}"
                      else:
                          # Relative URL
                          full_url = urllib.parse.urljoin(base_url, img_url)
                      
                      # Generate local filename
                      url_path = urllib.parse.urlparse(full_url).path
                      if url_path:
                          filename = os.path.basename(url_path)
                          if not filename or filename == '/':
                              filename = f"image_{len(downloaded_images)}.png"
                      else:
                          filename = f"image_{len(downloaded_images)}.png"
                      
                      # Ensure unique filename
                      base_name, ext = os.path.splitext(filename)
                      counter = 1
                      while os.path.exists(os.path.join(image_dir, filename)):
                          filename = f"{base_name}_{counter}{ext}"
                          counter += 1
                      
                      local_path = os.path.join(image_dir, filename)
                      
                      # Download image using curl
                      print(f"Downloading image: {full_url} -> {local_path}")
                      result = subprocess.run(
                          ['curl', '-L', '-s', '-o', local_path, '--create-dirs', '--max-time', '30', full_url],
                          capture_output=True
                      )
                      
                      if result.returncode == 0 and os.path.exists(local_path) and os.path.getsize(local_path) > 0:
                          # Store relative path for markdown
                          relative_path = os.path.relpath(local_path, os.path.dirname(image_dir))
                          downloaded_images[img_url] = relative_path
                          downloaded_paths[img_url] = (relative_path, alt_text)
                          print(f"‚úÖ Downloaded: {filename}")
                      else:
                          print(f"‚ùå Failed to download: {full_url}")
                          downloaded_paths[img_url] = (None, alt_text)
                          # Clean up empty file if created
                          if os.path.exists(local_path) and os.path.getsize(local_path) == 0:
                              os.remove(local_path)
                      
                  except Exception as e:
                      print(f"‚ùå Error downloading {img_url}: {e}")
                      downloaded_paths[img_url] = (None, alt_text)
              
              return downloaded_paths

          def convert_html_to_markdown_with_images(html_content, image_paths):
              """Convert HTML to Markdown while preserving image positions"""
              content = html_content
              
              # First, handle img tags
              for img_url, (local_path, alt_text) in image_paths.items():
                  if local_path:
                      # Replace HTML img with Markdown image
                      markdown_img = f"![{alt_text or 'Image'}]({local_path})"
                  else:
                      # Image couldn't be downloaded
                      markdown_img = f"[IMAGE NOT AVAILABLE: {os.path.basename(img_url) or 'unknown'}]"
                  
                  # Replace various img tag patterns
                  patterns = [
                      rf'<img\s+[^>]*src=["\']?{re.escape(img_url)}["\']?[^>]*>',
                      rf'!\[[^\]]*\]\({re.escape(img_url)}\)'
                  ]
                  
                  for pattern in patterns:
                      content = re.sub(pattern, markdown_img, content, flags=re.IGNORECASE)
              
              return content
          EOF
          echo "‚úÖ Image processing helper created"

      - name: Create Content Processing Helper Script
        run: |
          cat > content_processor.py << 'EOF'
          import re
          from image_processor import extract_and_download_images, convert_html_to_markdown_with_images

          def clean_deepwiki_content(content, preserve_images=True, image_dir="", base_url="", downloaded_images={}):
              """Clean and format DeepWiki content for GitHub Wiki"""
              
              # Extract and download images before cleaning
              image_paths = {}
              if preserve_images:
                  image_paths = extract_and_download_images(content, base_url, image_dir, downloaded_images)
              
              # Convert HTML to Markdown with images
              if image_paths:
                  content = convert_html_to_markdown_with_images(content, image_paths)
              
              # Continue with existing cleaning logic...
              # Eliminar bloque de navegaci√≥n/√≠ndice al principio (men√∫ DeepWiki)
              content = re.sub(r'^(?:[\s\S]{0,300}?)(?:github-actions\[bot\].*?revision\n)?(?:[\s\S]{0,300}?)(?:System Architecture|Overview|Streamlit Dashboard|Machine Learning Models|NLP Pipeline|Expert System|Developer Guide|Other Sections)[\s\S]+?(?=\n# |\n## |\n### |\n\Z)', '', content, flags=re.MULTILINE)
              # Eliminar secci√≥n '/' y sus repeticiones
              content = re.sub(r'^#? ?/?\n(?:Documentation\n)+', '', content, flags=re.MULTILINE)
              content = re.sub(r'^/?\n+', '', content, flags=re.MULTILINE)
              # Remove DeepWiki UI elements
              content = re.sub(r'.*?DeepWiki.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Powered by Devin.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Share.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Last indexed:.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Try DeepWiki.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Auto-refresh not enabled yet.*?\n', '', content, flags=re.IGNORECASE)
              content = re.sub(r'.*?Which repo would you like to understand.*?\n', '', content, flags=re.IGNORECASE)
              
              # Remove navigation elements
              content = re.sub(r'- Overview\n- System Architecture.*?- Getting Started\n', '', content, flags=re.DOTALL)
              content = re.sub(r'Menu\n', '', content)
              content = re.sub(r'### On this page.*?- Getting Started\n', '', content, flags=re.DOTALL)
              
              # Remove source file references (they clutter the wiki)
              content = re.sub(r'Relevant source files.*?\n\n', '', content, flags=re.DOTALL)
              content = re.sub(r'Sources:.*?\n', '', content, flags=re.MULTILINE)
              
              # Clean up multiple consecutive newlines
              content = re.sub(r'\n{3,}', '\n\n', content)
              
              # Remove empty sections
              content = re.sub(r'\n## \n', '', content)
              content = re.sub(r'\n### \n', '', content)
              
              # Fix malformed headers
              content = re.sub(r'^([#]+)\s*$', '', content, flags=re.MULTILINE)
              
              # Remove VforVitorio/F1_Strat_Manager title duplicates
              content = re.sub(r'^# /VforVitorio/F1_Strat_Manager.*?\n', '', content, flags=re.MULTILINE)
              content = re.sub(r'VforVitorio/F1_Strat_Manager \| DeepWiki.*?\n', '', content)
              
              # Clean up beginning of content
              content = content.strip()
              
              return content
              
          def get_page_title_from_content(content):
              """Extract a clean title from content"""
              lines = content.split('\n')
              for line in lines:
                  if line.strip().startswith('# ') and len(line.strip()) > 2:
                      title = line.strip()[2:].strip()
                      # Clean the title
                      title = re.sub(r'^/.*?/', '', title)  # Remove leading path
                      if title:
                          return title
              return "Documentation"

          def safe_filename(title, fallback_idx=None):
              """Genera un nombre de archivo seguro a partir del t√≠tulo. Si el t√≠tulo es vac√≠o, usa un fallback √∫nico."""
              if not title or len(title.strip()) == 0:
                  if fallback_idx is not None:
                      return f"unknown-section-{fallback_idx}"
                  return "unknown-section"
              
              filename = title.lower().strip()
              # Elimina caracteres especiales y reemplaza por guiones
              filename = re.sub(r'[^\w\s-]', '', filename)
              filename = re.sub(r'[-\s]+', '-', filename)
              filename = filename.strip('-')  # Quita guiones al inicio/fin
              if not filename:
                  if fallback_idx is not None:
                      return f"unknown-section-{fallback_idx}"
                  return "unknown-section"
              return filename
          EOF
          echo "‚úÖ Content processing helper created"

      - name: Create Section Categorization Helper Script
        run: |
          cat > section_categorizer.py << 'EOF'
          from content_processor import safe_filename

          def categorize_section(title, fallback_idx=None):
              """Categoriza secciones seg√∫n la jerarqu√≠a especificada"""
              title_lower = title.lower() if title else ''
              
              # Main sections
              if any(keyword in title_lower for keyword in ['overview', 'introducci√≥n', 'introduction']):
                  return ('main', 'Overview', '01-overview.md')
              elif any(keyword in title_lower for keyword in ['streamlit', 'dashboard', 'interfaz']):
                  return ('main', 'Streamlit Dashboard', '02-streamlit-dashboard.md')
              elif any(keyword in title_lower for keyword in ['machine learning', 'ml', 'modelo']):
                  return ('main', 'Machine Learning Models', '03-machine-learning-models.md')
              elif any(keyword in title_lower for keyword in ['nlp', 'natural language', 'radio', 'processing']):
                  return ('main', 'NLP Pipeline', '04-nlp-pipeline.md')
              elif any(keyword in title_lower for keyword in ['expert', 'rules', 'engine', 'reglas']):
                  return ('main', 'Expert System', '05-expert-system.md')
              elif any(keyword in title_lower for keyword in ['developer', 'api', 'integration', 'guide']):
                  return ('main', 'Developer Guide', '06-developer-guide.md')
              
              # Streamlit Dashboard subsections
              elif any(keyword in title_lower for keyword in ['strategy recommendations', 'recomendaciones']):
                  return ('sub', 'Streamlit Dashboard', '02-01-strategy-recommendations-view.md')
              elif any(keyword in title_lower for keyword in ['gap analysis view', 'an√°lisis de gaps']):
                  return ('sub', 'Streamlit Dashboard', '02-02-gap-analysis-view.md')
              elif any(keyword in title_lower for keyword in ['radio analysis view', 'an√°lisis de radio']):
                  return ('sub', 'Streamlit Dashboard', '02-03-radio-analysis-view.md')
              elif any(keyword in title_lower for keyword in ['time predictions', 'predicciones']):
                  return ('sub', 'Streamlit Dashboard', '02-04-time-predictions-view.md')
              elif any(keyword in title_lower for keyword in ['chat interface', 'interfaz de chat']):
                  return ('sub', 'Streamlit Dashboard', '02-05-strategy-chat-interface.md')
              
              # Machine Learning subsections
              elif any(keyword in title_lower for keyword in ['lap time prediction', 'predicci√≥n de tiempos']):
                  return ('sub', 'Machine Learning Models', '03-01-lap-time-prediction.md')
              elif any(keyword in title_lower for keyword in ['tire degradation', 'degradaci√≥n', 'neum√°ticos']):
                  return ('sub', 'Machine Learning Models', '03-02-tire-degradation-modeling.md')
              elif any(keyword in title_lower for keyword in ['vision', 'gap calculation', 'c√°lculo']):
                  return ('sub', 'Machine Learning Models', '03-03-vision-based-gap-calculation.md')
              
              # NLP Pipeline subsections
              elif any(keyword in title_lower for keyword in ['transcription', 'transcripci√≥n']):
                  return ('sub', 'NLP Pipeline', '04-01-radio-transcription.md')
              elif any(keyword in title_lower for keyword in ['sentiment', 'intent', 'an√°lisis']):
                  return ('sub', 'NLP Pipeline', '04-02-sentiment-intent-analysis.md')
              elif any(keyword in title_lower for keyword in ['named entity', 'ner', 'entidades']):
                  return ('sub', 'NLP Pipeline', '04-03-named-entity-recognition.md')
              
              # Expert System subsections
              elif any(keyword in title_lower for keyword in ['degradation rules', 'reglas de degradaci√≥n']):
                  return ('sub', 'Expert System', '05-01-degradation-rules.md')
              elif any(keyword in title_lower for keyword in ['gap analysis rules', 'reglas de gaps']):
                  return ('sub', 'Expert System', '05-02-gap-analysis-rules.md')
              elif any(keyword in title_lower for keyword in ['radio message rules', 'reglas de radio']):
                  return ('sub', 'Expert System', '05-03-radio-message-rules.md')
              elif any(keyword in title_lower for keyword in ['integrated rule engine', 'motor de reglas']):
                  return ('sub', 'Expert System', '05-04-integrated-rule-engine.md')
              
              # Developer Guide subsections
              elif any(keyword in title_lower for keyword in ['api reference', 'referencia api']):
                  return ('sub', 'Developer Guide', '06-01-api-reference.md')
              elif any(keyword in title_lower for keyword in ['integration guide', 'gu√≠a de integraci√≥n']):
                  return ('sub', 'Developer Guide', '06-02-integration-guide.md')
              elif any(keyword in title_lower for keyword in ['system architecture', 'arquitectura']):
                  return ('sub', 'Overview', '01-01-system-architecture.md')
              elif any(keyword in title_lower for keyword in ['installation', 'setup', 'instalaci√≥n']):
                  return ('sub', 'Overview', '01-02-installation-setup.md')
              # Fallback seguro
              return ('misc', 'Other', f'99-{safe_filename(title, fallback_idx)}.md')
          EOF
          echo "‚úÖ Section categorization helper created"

      - name: Process DeepWiki Response and Extract Content
        run: |
          echo "Processing multiple pages from DeepWiki response..."

          python3 << 'EOF'
          import json
          import sys
          import os
          from pathlib import Path
          from content_processor import clean_deepwiki_content, get_page_title_from_content
          from section_categorizer import categorize_section

          # Environment variables
          DOCS_DIR = os.environ.get('DOCS_DIR', 'docs-md')
          IMAGE_DIR = os.environ.get('IMAGE_DIR', 'docs-md/images')
          DEEPWIKI_URL = os.environ.get('DEEPWIKI_URL', 'https://deepwiki.com/VforVitorio/F1_Strat_Manager')

          # Create image directory
          Path(IMAGE_DIR).mkdir(parents=True, exist_ok=True)

          # Track all downloaded images
          downloaded_images = {}

          # Read the JSON response
          try:
              with open(f'{DOCS_DIR}/all-pages-raw.json', 'r', encoding='utf-8') as f:
                  data = json.load(f)
              
              print(f"Response structure: {list(data.keys())}")
              
              # Handle different response structures
              pages_content = []
              
              if 'result' in data and 'content' in data['result']:
                  if isinstance(data['result']['content'], list):
                      # Multiple content items
                      for item in data['result']['content']:
                          if 'text' in item:
                              pages_content.append(item['text'])
                  else:
                      # Single content item
                      if 'text' in data['result']['content']:
                          pages_content.append(data['result']['content']['text'])
              elif 'result' in data and isinstance(data['result'], str):
                  # Direct string result
                  pages_content.append(data['result'])
              else:
                  print("‚ùå Unexpected response structure")
                  print(f"Data keys: {list(data.keys())}")
                  sys.exit(1)
              
              print(f"Found {len(pages_content)} content sections")
              
              # Structure to organize content by hierarchy
              organized_content = {
                  'main_sections': {},
                  'subsections': {},
                  'misc': []
              }
              
              # Process each page/section
              for i, content in enumerate(pages_content):
                  print(f"\nProcessing content section {i+1}...")
                  print(f"Content length: {len(content)} characters")
                  
                  # Validate content is not None or empty
                  if not content or len(str(content).strip()) == 0:
                      print(f"Skipping empty content section {i+1}")
                      continue
                  
                  # Ensure content is string
                  if not isinstance(content, str):
                      print(f"Converting content to string for section {i+1}")
                      content = str(content)
                  
                  # Clean the content (with image support)
                  cleaned_content = clean_deepwiki_content(
                      content, 
                      preserve_images=True, 
                      image_dir=IMAGE_DIR, 
                      base_url=DEEPWIKI_URL, 
                      downloaded_images=downloaded_images
                  )
                  
                  if len(cleaned_content.strip()) < 50:  # Skip very short content
                      print(f"Skipping short content section {i+1}")
                      continue
                  
                  # Get title and categorize
                  title = get_page_title_from_content(cleaned_content)
                  # Validaci√≥n de t√≠tulo vac√≠o
                  if not title or len(title.strip()) == 0:
                      print(f"Warning: Empty title found in section {i+1}, using fallback")
                      title = f"Section_{i+1}"
                  section_type, parent_section, filename = categorize_section(title, fallback_idx=i+1)
                  # Validaci√≥n adicional para filename
                  if not filename or filename == '.md' or filename.startswith('99-.md'):
                      print(f"Warning: Invalid filename generated for '{title}', using fallback")
                      filename = f'99-section-{i+1}.md'
                  
                  # Ensure proper title format
                  if not cleaned_content.strip().startswith('# '):
                      cleaned_content = f'# {title}\n\n' + cleaned_content.strip()
                  
                  # Write individual file
                  filepath = f'{DOCS_DIR}/{filename}'
                  with open(filepath, 'w', encoding='utf-8') as f:
                      f.write(cleaned_content)
                  
                  # Organize content
                  if section_type == 'main':
                      organized_content['main_sections'][parent_section] = {
                          'filename': filename,
                          'title': title,
                          'content': cleaned_content
                      }
                  elif section_type == 'sub':
                      if parent_section not in organized_content['subsections']:
                          organized_content['subsections'][parent_section] = []
                      organized_content['subsections'][parent_section].append({
                          'filename': filename,
                          'title': title,
                          'content': cleaned_content
                      })
                  else:
                      organized_content['misc'].append({
                          'filename': filename,
                          'title': title,
                          'content': cleaned_content
                      })
                  
                  print(f"‚úÖ Created: {filename} (Category: {section_type}, Parent: {parent_section})")
              
              # Save organized content for next step
              import pickle
              with open('organized_content.pkl', 'wb') as f:
                  pickle.dump(organized_content, f)
              
              # Save downloaded images info
              with open('downloaded_images.pkl', 'wb') as f:
                  pickle.dump(downloaded_images, f)
              
              print(f"‚úÖ Content processing completed")
              
          except json.JSONDecodeError as e:
              print(f"‚ùå Error parsing JSON: {e}")
              print("Response might not be valid JSON, trying as plain text...")
              
              # Try to handle as plain text
              with open(f'{DOCS_DIR}/all-pages-raw.json', 'r', encoding='utf-8') as f:
                  content = f.read()
              
              cleaned_content = clean_deepwiki_content(
                  content, 
                  preserve_images=True, 
                  image_dir=IMAGE_DIR, 
                  base_url=DEEPWIKI_URL, 
                  downloaded_images=downloaded_images
              )
              
              if not cleaned_content.strip().startswith('# '):
                  cleaned_content = '# F1 Strategy Manager\n\n' + cleaned_content.strip()
              
              with open(f'{DOCS_DIR}/f1-strat-manager-complete.md', 'w', encoding='utf-8') as f:
                  f.write(cleaned_content)
              
              print("‚úÖ Processed as plain text")
              
          except Exception as e:
              print(f"‚ùå Error processing content: {e}")
              sys.exit(1)
          EOF

      - name: Generate Comprehensive Documentation
        run: |
          echo "Creating structured comprehensive documentation..."

          python3 << 'EOF'
          import pickle
          import re
          import os
          from pathlib import Path

          # Environment variables
          DOCS_DIR = os.environ.get('DOCS_DIR', 'docs-md')
          IMAGE_DIR = os.environ.get('IMAGE_DIR', 'docs-md/images')

          try:
              # Load organized content
              with open('organized_content.pkl', 'rb') as f:
                  organized_content = pickle.load(f)
              
              # Load downloaded images info
              with open('downloaded_images.pkl', 'rb') as f:
                  downloaded_images = pickle.load(f)
              
              # Define the correct order of main sections
              section_order = [
                  'Overview',
                  'Streamlit Dashboard', 
                  'Machine Learning Models',
                  'NLP Pipeline',
                  'Expert System',
                  'Developer Guide'
              ]
              
              main_content = "# F1 Strategy Manager - Complete Documentation\n\n"
              main_content += "This document contains the complete documentation for the F1 Strategy Manager project, organized in a hierarchical structure.\n\n"
              main_content += "## Table of Contents\n\n"
              
              # Build table of contents
              for section in section_order:
                  if section in organized_content['main_sections']:
                      section_link = section.lower().replace(' ', '-').replace('.', '').replace('(', '').replace(')', '')
                      main_content += f"### {section}\n"
                      main_content += f"- **[{section}](#{section_link})**\n"
                      
                      # Add subsections
                      if section in organized_content['subsections']:
                          for subsection in organized_content['subsections'][section]:
                              sub_link = subsection['title'].lower().replace(' ', '-').replace('.', '').replace('(', '').replace(')', '')
                              main_content += f"  - [{subsection['title']}](#{sub_link})\n"
                      main_content += "\n"
              
              # Add miscellaneous sections
              if organized_content['misc']:
                  main_content += "### Other Sections\n"
                  for misc_item in organized_content['misc']:
                      misc_link = misc_item['title'].lower().replace(' ', '-').replace('.', '').replace('(', '').replace(')', '')
                      main_content += f"- [{misc_item['title']}](#{misc_link})\n"
                  main_content += "\n"
              
              main_content += "\n---\n\n"
              
              # Add all content in the specified order
              for section in section_order:
                  if section in organized_content['main_sections']:
                      main_section = organized_content['main_sections'][section]
                      main_content += main_section['content'] + "\n\n"
                      
                      # Add subsections immediately after their parent
                      if section in organized_content['subsections']:
                          for subsection in organized_content['subsections'][section]:
                              # Adjust header levels for subsections
                              subsection_content = subsection['content']
                              # Convert main headers to subheaders
                              subsection_content = re.sub(r'^# ', '## ', subsection_content, flags=re.MULTILINE)
                              subsection_content = re.sub(r'^## ', '### ', subsection_content, flags=re.MULTILINE)
                              subsection_content = re.sub(r'^### ', '#### ', subsection_content, flags=re.MULTILINE)
                              main_content += subsection_content + "\n\n"
                      
                      main_content += "---\n\n"
              
              # Add miscellaneous content at the end
              for misc_item in organized_content['misc']:
                  main_content += misc_item['content'] + "\n\n---\n\n"
              
              # Write main comprehensive file
              with open(f'{DOCS_DIR}/f1-strat-manager-complete.md', 'w', encoding='utf-8') as f:
                  f.write(main_content)
              
              # Count total files
              total_main = len(organized_content['main_sections'])
              total_sub = sum(len(subs) for subs in organized_content['subsections'].values())
              total_misc = len(organized_content['misc'])
              total_files = total_main + total_sub + total_misc + 1  # +1 for complete doc
              
              print(f"‚úÖ Created structured documentation:")
              print(f"  - Main sections: {total_main}")
              print(f"  - Subsections: {total_sub}")
              print(f"  - Miscellaneous: {total_misc}")
              print(f"  - Total files: {total_files}")
              
              # Report on downloaded images
              print(f"\n‚úÖ Downloaded images:")
              print(f"  - Total images processed: {len(downloaded_images)}")
              image_list = list(Path(IMAGE_DIR).glob('*'))
              print(f"  - Images successfully downloaded: {len(image_list)}")
              if image_list:
                  print("  - Image files:")
                  for img in image_list[:10]:  # Show first 10
                      print(f"    - {img.name}")
                  if len(image_list) > 10:
                      print(f"    ... and {len(image_list) - 10} more")
              
          except Exception as e:
              print(f"‚ùå Error generating documentation: {e}")
              import sys
              sys.exit(1)
          EOF

          echo "Final documentation files created:"
          ls -la ${{ env.DOCS_DIR }}/
          echo "Images downloaded:"
          ls -la ${{ env.IMAGE_DIR }}/ || echo "No images directory found"
          echo "Preview of main documentation:"
          head -30 ${{ env.DOCS_DIR }}/f1-strat-manager-complete.md

      - name: Checkout Wiki
        uses: actions/checkout@v3
        with:
          repository: ${{ github.repository }}.wiki
          token: ${{ secrets.WIKI_PAT }}
          path: ${{ env.WIKI_DIR }}

      - name: Copy All Documentation and Images to Wiki
        run: |
          # Verify we have files to copy
          if [ ! -d ${{ env.DOCS_DIR }} ] || [ -z "$(ls -A ${{ env.DOCS_DIR }}/*.md 2>/dev/null)" ]; then
            echo "Error: No markdown files to copy"
            exit 1
          fi
          echo "Files to copy:"
          ls -la ${{ env.DOCS_DIR }}/

          # Ensure wiki directory exists
          if [ ! -d ${{ env.WIKI_DIR }} ]; then
            echo "Error: Wiki directory not found"
            exit 1
          fi
          echo "Current wiki contents:"
          ls -la ${{ env.WIKI_DIR }}/

          # Copy ALL markdown files to wiki
          cp ${{ env.DOCS_DIR }}/*.md ${{ env.WIKI_DIR }}/

          # Copy images directory if it exists and has content
          if [ -d "${{ env.IMAGE_DIR }}" ] && [ "$(ls -A ${{ env.IMAGE_DIR }})" ]; then
            echo "Copying images to wiki..."
            # Create images directory in wiki
            mkdir -p ${{ env.WIKI_DIR }}/images
            # Copy all images preserving directory structure
            cp -r ${{ env.IMAGE_DIR }}/* ${{ env.WIKI_DIR }}/images/
            echo "Images copied:"
            ls -la ${{ env.WIKI_DIR }}/images/
          else
            echo "No images to copy"
          fi

          # Create or update index page with hierarchical structure
          cat > ${{ env.WIKI_DIR }}/Home.md << 'EOF'
          # Welcome to F1 Strategy Manager Wiki

          This wiki contains comprehensive documentation for the F1 Strategy Manager project, organized in a hierarchical structure for easy navigation. 
          It is generated through DeepWiki. To see the full documentation with all figures and interactive elements, please visit [DeepWiki](https://deepwiki.com/VforVitorio/F1_Strat_Manager).

          ## üìã Complete Documentation
          - **[üìñ Complete Documentation](f1-strat-manager-complete)** - Full system documentation with all sections organized hierarchically

          ## üóÇÔ∏è Documentation Structure

          ### üîç 1. Overview
          - **[Overview](01-overview)** - Project introduction and general information
            - [System Architecture](01-01-system-architecture) - Overall system design and components
            - [Installation and Setup](01-02-installation-setup) - Getting started guide

          ### üìä 2. Streamlit Dashboard
          - **[Streamlit Dashboard](02-streamlit-dashboard)** - Interactive web interface
            - [Strategy Recommendations View](02-01-strategy-recommendations-view) - Strategic decision interface
            - [Gap Analysis View](02-02-gap-analysis-view) - Real-time gap tracking
            - [Radio Analysis View](02-03-radio-analysis-view) - Team radio insights
            - [Time Predictions View](02-04-time-predictions-view) - Lap time forecasting
            - [Strategy Chat Interface](02-05-strategy-chat-interface) - AI-powered strategy chat

          ### ü§ñ 3. Machine Learning Models
          - **[Machine Learning Models](03-machine-learning-models)** - AI/ML components
            - [Lap Time Prediction](03-01-lap-time-prediction) - Predictive models for lap times
            - [Tire Degradation Modeling](03-02-tire-degradation-modeling) - Tire performance analysis
            - [Vision-based Gap Calculation](03-03-vision-based-gap-calculation) - Computer vision for gap detection

          ### üé§ 4. NLP Pipeline
          - **[NLP Pipeline](04-nlp-pipeline)** - Natural Language Processing components
            - [Radio Transcription](04-01-radio-transcription) - Speech-to-text processing
            - [Sentiment and Intent Analysis](04-02-sentiment-intent-analysis) - Emotional and intent recognition
            - [Named Entity Recognition](04-03-named-entity-recognition) - Entity extraction from radio communications

          ### ‚öôÔ∏è 5. Expert System
          - **[Expert System](05-expert-system)** - Rule-based decision engine
            - [Degradation Rules](05-01-degradation-rules) - Tire degradation logic
            - [Gap Analysis Rules](05-02-gap-analysis-rules) - Gap calculation rules
            - [Radio Message Rules](05-03-radio-message-rules) - Communication analysis rules
            - [Integrated Rule Engine](05-04-integrated-rule-engine) - Unified rule processing

          ### üë®‚Äçüíª 6. Developer Guide
          - **[Developer Guide](06-developer-guide)** - Technical documentation for developers
            - [API Reference](06-01-api-reference) - Complete API documentation
            - [Integration Guide](06-02-integration-guide) - How to integrate with external systems

          ## üèéÔ∏è Project Overview

          The F1 Strategy Manager is an integrated AI-powered system for Formula 1 race strategy analysis and decision support, combining:

          - **ü§ñ Machine Learning Models** - Predictive analytics for lap times and tire performance
          - **üëÅÔ∏è Computer Vision** - Automated gap calculation from video feeds
          - **üé§ Natural Language Processing** - Radio communication analysis and insights
          - **‚öôÔ∏è Rule-based Expert Systems** - Strategic recommendations based on F1 expertise
          - **üìä Interactive Streamlit Dashboard** - User-friendly web interface for real-time analysis

          ---

          *üìù This documentation is automatically generated and updated from the project's DeepWiki documentation.*
          *üñºÔ∏è Images and diagrams are included where available. If an image is missing, please check the [DeepWiki source](https://deepwiki.com/VforVitorio/F1_Strat_Manager).*
          EOF

          echo "Files copied to wiki:"
          ls -la ${{ env.WIKI_DIR }}/

      - name: Commit & Push changes
        working-directory: ${{ env.WIKI_DIR }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add .

          # Check if there are changes before committing
          if git diff --quiet --staged; then
            echo "No changes to commit"
          else
            # Count images if any
            IMAGE_COUNT=$(find . -name "*.png" -o -name "*.jpg" -o -name "*.jpeg" -o -name "*.gif" -o -name "*.svg" | wc -l)
            COMMIT_MSG="üîÑ Update Complete Wiki from DeepWiki - $(date '+%Y-%m-%d %H:%M')"
            if [ $IMAGE_COUNT -gt 0 ]; then
              COMMIT_MSG="$COMMIT_MSG (includes $IMAGE_COUNT images)"
            fi
            git commit -m "$COMMIT_MSG"
            git push
            echo "Changes pushed successfully"
          fi
