name: Update Wiki from DeepWiki

on:
  push:
    branches:
      - main

jobs:
  update-wiki:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Clone DeepWiki MCP
        run: |
          git clone https://github.com/regenrek/deepwiki-mcp.git deepwiki-mcp
          cd deepwiki-mcp
          npm install

      - name: Build DeepWiki MCP
        working-directory: deepwiki-mcp
        run: |
          # Check if package.json exists and available scripts
          echo "Package.json scripts:"
          npm run || true

          # Try to build the project
          if npm run build 2>/dev/null; then
            echo "Build completed with npm run build"
          elif npm run compile 2>/dev/null; then
            echo "Build completed with npm run compile"
          elif npm run dist 2>/dev/null; then
            echo "Build completed with npm run dist"
          else
            echo "Trying manual build with typescript..."
            npx tsc || echo "Could not compile with tsc"
          fi

          # Verify that dist file exists
          echo "Checking compiled files:"
          ls -la
          if [ -d "dist" ]; then
            ls -la dist/
          else
            echo "dist directory not found"
            echo "Looking for .mjs files in project:"
            find . -name "*.mjs" -type f
          fi

      - name: Start DeepWiki MCP (HTTP mode)
        working-directory: deepwiki-mcp
        run: |
          # Verify that necessary files exist
          ls -la
          ls -la bin/

          # Verify that dist/index.mjs file exists
          if [ ! -f "dist/index.mjs" ]; then
            echo "Error: dist/index.mjs not found"
            echo "Available files:"
            find . -name "*.mjs" -type f
            exit 1
          fi

          # Start service in background
          node ./bin/cli.mjs --http --port 3000 > ../deepwiki.log 2>&1 &
          DEEPWIKI_PID=$!
          echo "DeepWiki PID: $DEEPWIKI_PID"

          # Wait and verify service is available
          for i in {1..30}; do
            if curl -s http://localhost:3000/mcp > /dev/null 2>&1; then
              echo "DeepWiki MCP is available"
              break
            fi
            echo "Waiting for DeepWiki MCP to be available... attempt $i/30"
            sleep 2
          done

          # Verify once more - check /mcp endpoint
          if ! curl -s http://localhost:3000/mcp > /dev/null 2>&1; then
            echo "Error: DeepWiki MCP not available after 60 seconds"
            echo "Service logs:"
            cat ../deepwiki.log
            echo "Process status:"
            ps aux | grep node | grep -v grep || echo "Process not found"
            exit 1
          fi

          # Verify process is running
          ps aux | grep node | grep -v grep

      - name: Export ALL Markdown from DeepWiki
        run: |
          mkdir -p docs-md

          # Verify service is available before making the call
          if ! curl -s http://localhost:3000/mcp > /dev/null 2>&1; then
            echo "Error: Service not available before export"
            echo "Service logs:"
            cat deepwiki.log || echo "No logs found"
            exit 1
          fi

          # Test connectivity first
          echo "Testing service connectivity..."
          curl -X POST http://localhost:3000/mcp -H "Content-Type: application/json" -d '{"jsonrpc":"2.0","method":"tools/list","id":1}' -v || echo "Tools list check failed"

          # Validate DeepWiki URL accessibility
          echo "Validating DeepWiki URL accessibility..."
          DEEPWIKI_URL="https://deepwiki.com/VforVitorio/F1_Strat_Manager"

          # Check if the URL is accessible
          if curl -s --head "$DEEPWIKI_URL" | head -n 1 | grep -q "200 OK"; then
            echo "‚úÖ DeepWiki URL is accessible: $DEEPWIKI_URL"
          else
            echo "‚ö†Ô∏è Warning: DeepWiki URL might not be accessible: $DEEPWIKI_URL"
          fi

          # Create JSON payload to get ALL pages (maxDepth: 1, mode: "pages")
          JSON_PAYLOAD='{"jsonrpc":"2.0","method":"tools/call","params":{"name":"deepwiki_fetch","arguments":{"url":"'$DEEPWIKI_URL'","maxDepth":1,"mode":"pages"}},"id":1}'

          echo "JSON-RPC 2.0 payload to send (ALL PAGES):"
          echo "$JSON_PAYLOAD" | jq . || echo "$JSON_PAYLOAD"

          # Validate JSON syntax
          if echo "$JSON_PAYLOAD" | jq . > /dev/null 2>&1; then
            echo "‚úÖ JSON payload is valid"
          else
            echo "‚ùå JSON payload is invalid"
            exit 1
          fi

          # Make the POST request to get ALL pages
          echo "Making POST request to get ALL DeepWiki pages..."
          HTTP_CODE=$(curl -X POST http://localhost:3000/mcp \
            -H "Content-Type: application/json" \
            -d "$JSON_PAYLOAD" \
            -o docs-md/all-pages-raw.json \
            -w "%{http_code}" \
            -s)

          echo "HTTP status code: $HTTP_CODE"

          # Handle different HTTP response codes
          if [ "$HTTP_CODE" = "200" ]; then
            echo "‚úÖ Success: All pages request completed successfully"
          else
            echo "‚ùå Error HTTP $HTTP_CODE"
            echo "Response content:"
            cat docs-md/all-pages-raw.json
            
            # Try fallback with aggregate mode
            echo "Trying fallback with aggregate mode..."
            FALLBACK_PAYLOAD='{"jsonrpc":"2.0","method":"tools/call","params":{"name":"deepwiki_fetch","arguments":{"url":"'$DEEPWIKI_URL'","maxDepth":1,"mode":"aggregate"}},"id":1}'
            
            HTTP_CODE=$(curl -X POST http://localhost:3000/mcp \
              -H "Content-Type: application/json" \
              -d "$FALLBACK_PAYLOAD" \
              -o docs-md/all-pages-raw.json \
              -w "%{http_code}" \
              -s)
            
            if [ "$HTTP_CODE" != "200" ]; then
              echo "Fallback method also failed with HTTP $HTTP_CODE"
              exit 1
            fi
            echo "‚úÖ Fallback method succeeded"
          fi

          # Verify file was created and is not empty
          if [ ! -f docs-md/all-pages-raw.json ]; then
            echo "Error: Response file was not created"
            exit 1
          fi

          if [ ! -s docs-md/all-pages-raw.json ]; then
            echo "Error: Response file is empty"
            echo "File content:"
            cat docs-md/all-pages-raw.json
            exit 1
          fi

          echo "‚úÖ All pages response received successfully:"
          ls -la docs-md/
          echo "Response structure:"
          jq -r 'keys' docs-md/all-pages-raw.json || echo "Not valid JSON, showing first 500 chars:"
          head -c 500 docs-md/all-pages-raw.json

      - name: Process and Split Multiple Pages
        run: |
          echo "Processing multiple pages from DeepWiki response..."

          # Install jq if not available
          sudo apt-get update && sudo apt-get install -y jq

          # Process the response with Python to handle multiple pages
          python3 << 'EOF'
          import json
          import re
          import sys
          import os

          # Define the diagrams with unique markers
          def get_diagram(diagram_type):
              diagrams = {
                  "SYSTEM_ARCHITECTURE_MAIN": [
                      "```mermaid",
                      "flowchart TB",
                      '    subgraph "Data Sources"',
                      "        A1[FastF1 Telemetry]",
                      "        A2[OpenF1 Radio]",
                      "        A3[Video Footage]",
                      "    end",
                      "    ",
                      '    subgraph "Processing Layer"',
                      "        B1[Data Processing]",
                      "        B2[Feature Engineering]",
                      "        B3[Gap Calculation]",
                      "    end",
                      "    ",
                      '    subgraph "Analysis Layer"',
                      "        C1[ML Models]",
                      "        C2[Computer Vision]",
                      "        C3[NLP Pipeline]",
                      "    end",
                      "    ",
                      '    subgraph "Expert System"',
                      "        D1[Rule Engine]",
                      "        D2[Strategy Recommendations]",
                      "    end",
                      "    ",
                      '    subgraph "User Interface"',
                      "        E1[Streamlit Dashboard]",
                      "        E2[Interactive Views]",
                      "    end",
                      "    ",
                      "    A1 --> B1",
                      "    A2 --> B2",
                      "    A3 --> B3",
                      "    B1 --> C1",
                      "    B2 --> C3",
                      "    B3 --> C2",
                      "    C1 --> D1",
                      "    C2 --> D1",
                      "    C3 --> D1",
                      "    D1 --> D2",
                      "    D2 --> E1",
                      "    E1 --> E2",
                      "```"
                  ],
                  "DATA_FLOW_SEQUENCE": [
                      "```mermaid",
                      "sequenceDiagram",
                      "    participant FS as FastF1 API",
                      "    participant OF as OpenF1 Radio",
                      "    participant VF as Video Feed",
                      "    participant DP as Data Processing",
                      "    participant ML as ML Models",
                      "    participant CV as Computer Vision",
                      "    participant NLP as NLP Pipeline",
                      "    participant ES as Expert System",
                      "    participant UI as Streamlit UI",
                      "    participant U as User",
                      "    ",
                      "    FS->>DP: Telemetry Data",
                      "    OF->>DP: Radio Transcripts",
                      "    VF->>DP: Video Frames",
                      "    ",
                      "    DP->>ML: Processed Telemetry",
                      "    DP->>CV: Video Frames",
                      "    DP->>NLP: Radio Text",
                      "    ",
                      "    ML->>ES: Lap Time Predictions",
                      "    CV->>ES: Gap Calculations",
                      "    NLP->>ES: Radio Insights",
                      "    ",
                      "    ES->>UI: Strategy Recommendations",
                      "    UI->>U: Interactive Dashboard",
                      "```"
                  ],
                  "EXPERT_SYSTEM_RULES": [
                      "```mermaid",
                      "flowchart TB",
                      "    A[Race Data Input] --> B{Data Analysis}",
                      "    ",
                      "    B --> C[Degradation Rules]",
                      "    B --> D[Gap Analysis Rules]",
                      "    B --> E[Radio Message Rules]",
                      "    ",
                      "    C --> F[Tire Strategy]",
                      "    D --> G[Position Strategy]",
                      "    E --> H[Communication Insights]",
                      "    ",
                      "    F --> I[Strategy Engine]",
                      "    G --> I",
                      "    H --> I",
                      "    ",
                      "    I --> J{Conflict Resolution}",
                      "    J --> K[Final Recommendations]",
                      "    ",
                      "    K --> L[Pit Window]",
                      "    K --> M[Tire Choice]",
                      "    K --> N[Race Strategy]",
                      "```"
                  ],
                  "ML_PIPELINE": [
                      "```mermaid",
                      "graph LR",
                      "    A[Raw Telemetry] --> B[Data Preprocessing]",
                      "    B --> C[Feature Engineering]",
                      "    ",
                      "    C --> D[XGBoost Model]",
                      "    C --> E[TCN Model]",
                      "    C --> F[Vision Model]",
                      "    ",
                      "    D --> G[Lap Time Predictions]",
                      "    E --> H[Degradation Predictions]",
                      "    F --> I[Gap Calculations]",
                      "    ",
                      "    G --> J[Expert System]",
                      "    H --> J",
                      "    I --> J",
                      "    ",
                      "    J --> K[Strategy Recommendations]",
                      "```"
                  ]
              }
              return '\n'.join(diagrams.get(diagram_type, []))

          def clean_deepwiki_content(content):
              """Clean and format DeepWiki content for GitHub Wiki"""
              
              # First, mark image/diagram locations before cleaning
              # Look for patterns that indicate where diagrams should be
              diagram_markers = []
              
              # Find empty code blocks or image placeholders
              lines = content.split('\n')
              for i, line in enumerate(lines):
                  if line.strip() == '```' and i + 1 < len(lines) and lines[i+1].strip() == '```':
                      diagram_markers.append((i, i+1, 'EMPTY_BLOCK'))
              
              # Find image references 
              import re
              for match in re.finditer(r'!\[([^\]]*)\]\(([^\)]*)\)', content):
                  diagram_markers.append((match.start(), match.end(), 'IMAGE', match.group(1), match.group(2)))
              
              # Find diagram placeholders or references
              for match in re.finditer(r'(?:diagram|figure|chart|graph)[\s:]*(?:\n\n+|$)', content, re.IGNORECASE):
                  diagram_markers.append((match.start(), match.end(), 'DIAGRAM_REF'))
              
              # Sort markers by position
              diagram_markers.sort(key=lambda x: x[0])
              
              # Now clean the content while preserving markers
              # Remove DeepWiki UI elements
              patterns_to_remove = [
                  r'.*?DeepWiki.*?\n',
                  r'.*?Powered by Devin.*?\n',
                  r'.*?Share.*?\n',
                  r'.*?Last indexed:.*?\n',
                  r'.*?Try DeepWiki.*?\n',
                  r'.*?Auto-refresh not enabled yet.*?\n',
                  r'.*?Which repo would you like to understand.*?\n',
                  r'Menu\n',
                  r'Sources:.*?\n',
                  r'^# /VforVitorio/F1_Strat_Manager.*?\n',
                  r'VforVitorio/F1_Strat_Manager \| DeepWiki.*?\n'
              ]
              
              for pattern in patterns_to_remove:
                  content = re.sub(pattern, '', content, flags=re.IGNORECASE | re.MULTILINE)
              
              # Remove navigation elements
              content = re.sub(r'- Overview\n- System Architecture.*?- Getting Started\n', '', content, flags=re.DOTALL)
              content = re.sub(r'### On this page.*?- Getting Started\n', '', content, flags=re.DOTALL)
              
              # Remove source file references 
              content = re.sub(r'Relevant source files.*?\n\n', '', content, flags=re.DOTALL)
              
              # Clean up multiple consecutive newlines but preserve structure
              content = re.sub(r'\n{4,}', '\n\n\n', content)
              
              # Remove empty sections
              content = re.sub(r'\n## \n', '', content)
              content = re.sub(r'\n### \n', '', content)
              
              # Fix malformed headers
              content = re.sub(r'^([#]+)\s*

          def insert_diagrams_precisely(content, markers):
              """Insert diagrams at their exact locations based on context"""
              
              # Create a mapping of content sections to diagrams
              section_diagram_map = [
                  (r'system\s+architecture|architecture\s+overview', 'SYSTEM_ARCHITECTURE_MAIN'),
                  (r'data\s+flow|data\s+pipeline', 'DATA_FLOW_SEQUENCE'),
                  (r'expert\s+system|rule\s+engine|strategy\s+engine', 'EXPERT_SYSTEM_RULES'),
                  (r'machine\s+learning|ml\s+pipeline|ml\s+models', 'ML_PIPELINE')
              ]
              
              # Process content line by line to find exact insertion points
              lines = content.split('\n')
              processed_lines = []
              i = 0
              
              while i < len(lines):
                  line = lines[i]
                  processed_lines.append(line)
                  
                  # Check if this is a section header
                  if line.strip().startswith('#'):
                      header_text = line.strip().lower()
                      
                      # Check if we should insert a diagram after this header
                      for pattern, diagram_key in section_diagram_map:
                          if re.search(pattern, header_text, re.IGNORECASE):
                              # Look ahead to see if there's already a diagram or if there's an empty space
                              next_lines = []
                              j = i + 1
                              while j < len(lines) and j < i + 5:
                                  next_lines.append(lines[j])
                                  j += 1
                              
                              # Check if there's an empty space or placeholder for a diagram
                              combined_next = '\n'.join(next_lines)
                              
                              should_insert = False
                              insertion_index = i + 1
                              
                              # Check for empty lines
                              if len(next_lines) > 1 and not next_lines[0].strip() and not next_lines[1].strip():
                                  should_insert = True
                                  insertion_index = i + 1
                              
                              # Check for empty code blocks
                              for k, next_line in enumerate(next_lines):
                                  if next_line.strip() == '```' and k + 1 < len(next_lines) and next_lines[k+1].strip() == '```':
                                      should_insert = True
                                      insertion_index = i + 1 + k
                                      break
                              
                              # Check if text mentions diagram
                              if not should_insert:
                                  for k, next_line in enumerate(next_lines):
                                      if next_line.strip() and re.search(r'following|below|above', next_line, re.IGNORECASE) and re.search(r'diagram|illustrat|show', next_line, re.IGNORECASE):
                                          should_insert = True
                                          insertion_index = i + 1 + k + 1
                                          break
                              
                              if should_insert:
                                  # Skip to the insertion point
                                  while i < insertion_index - 1:
                                      i += 1
                                      if i < len(lines):
                                          processed_lines.append(lines[i])
                                  
                                  # Insert the diagram
                                  processed_lines.append('')
                                  processed_lines.append(get_diagram(diagram_key))
                                  processed_lines.append('')
                                  
                                  # Skip any empty lines or placeholders at the insertion point
                                  while i + 1 < len(lines):
                                      next_line = lines[i + 1]
                                      if not next_line.strip() or (next_line.strip() == '```' and i + 2 < len(lines) and lines[i + 2].strip() == '```'):
                                          i += 1
                                      else:
                                          break
                              
                              break
                  
                  i += 1
              
              return '\n'.join(processed_lines)

          def preserve_original_structure(content):
              """Preserve the exact structure and spacing from the original content"""
              
              # Mark all potential diagram locations with placeholders
              placeholder_count = 0
              placeholders = {}
              
              # Find empty code blocks
              lines = content.split('\n')
              new_lines = []
              i = 0
              while i < len(lines):
                  if i + 1 < len(lines) and lines[i].strip() == '```' and lines[i+1].strip() == '```':
                      placeholder = f"<<<DIAGRAM_PLACEHOLDER_{placeholder_count}>>>"
                      placeholders[placeholder] = ('EMPTY_BLOCK', i)
                      placeholder_count += 1
                      new_lines.append(placeholder)
                      i += 2
                  else:
                      new_lines.append(lines[i])
                      i += 1
              content = '\n'.join(new_lines)
              
              # Find and mark image references
              import re
              for match in re.finditer(r'!\[([^\]]*)\]\(([^\)]*)\)', content):
                  placeholder = f"<<<DIAGRAM_PLACEHOLDER_{placeholder_count}>>>"
                  placeholders[placeholder] = ('IMAGE', match.start(), match.group(1), match.group(2))
                  placeholder_count += 1
                  content = content[:match.start()] + placeholder + content[match.end():]
              
              # Find and mark sections that reference diagrams
              lines = content.split('\n')
              for i, line in enumerate(lines):
                  if re.search(r'(?:following|below|above).*?(?:diagram|illustrat|show|figure)', line, re.IGNORECASE):
                      # Mark the next empty area as a diagram location
                      for j in range(i + 1, min(i + 5, len(lines))):
                          if not lines[j].strip() and j + 1 < len(lines) and not lines[j + 1].strip():
                              placeholder = f"<<<DIAGRAM_PLACEHOLDER_{placeholder_count}>>>"
                              lines[j] = placeholder
                              placeholders[placeholder] = ('DIAGRAM_AREA', j)
                              placeholder_count += 1
                              break
              
              content = '\n'.join(lines)
              
              return content, placeholders

          def get_page_title_from_content(content):
              """Extract a clean title from content"""
              lines = content.split('\n')
              for line in lines:
                  if line.strip().startswith('# ') and len(line.strip()) > 2:
                      title = line.strip()[2:].strip()
                      # Clean the title
                      title = re.sub(r'^/.*?/', '', title)  # Remove leading path
                      if title:
                          return title
              return "Documentation"

          def generate_filename(title, url=""):
              """Generate a safe filename from title"""
              if url and "F1_Strat_Manager/" in url:
                  # Extract section from URL
                  section = url.split("F1_Strat_Manager/")[-1]
                  if section and section != "":
                      return section.replace("-", "_").replace(".", "_") + ".md"
              
              # Fallback to title-based filename
              filename = title.lower()
              filename = re.sub(r'[^\w\s-]', '', filename)
              filename = re.sub(r'[-\s]+', '-', filename)
              return filename + ".md"

          # Read the JSON response
          try:
              with open('docs-md/all-pages-raw.json', 'r', encoding='utf-8') as f:
                  data = json.load(f)
              
              print(f"Response structure: {list(data.keys())}")
              
              # Handle different response structures
              pages_content = []
              
              if 'result' in data and 'content' in data['result']:
                  if isinstance(data['result']['content'], list):
                      # Multiple content items
                      for item in data['result']['content']:
                          if 'text' in item:
                              pages_content.append(item['text'])
                  else:
                      # Single content item
                      if 'text' in data['result']['content']:
                          pages_content.append(data['result']['content']['text'])
              elif 'result' in data and isinstance(data['result'], str):
                  # Direct string result
                  pages_content.append(data['result'])
              else:
                  print("‚ùå Unexpected response structure")
                  print(f"Data keys: {list(data.keys())}")
                  sys.exit(1)
              
              print(f"Found {len(pages_content)} content sections")
              
              # Process each page/section
              all_files = []
              for i, content in enumerate(pages_content):
                  print(f"\nProcessing content section {i+1}...")
                  print(f"Content length: {len(content)} characters")
                  
                  # First preserve the structure
                  structured_content, placeholders = preserve_original_structure(content)
                  
                  # Clean the content while preserving placeholders
                  cleaned_content, original_markers = clean_deepwiki_content(structured_content)
                  
                  # Insert diagrams at the exact preserved locations
                  final_content = insert_diagrams_precisely(cleaned_content, original_markers)
                  
                  if len(final_content.strip()) < 50:  # Skip very short content
                      print(f"Skipping short content section {i+1}")
                      continue
                  
                  # Get title and filename
                  title = get_page_title_from_content(final_content)
                  filename = generate_filename(title)
                  
                  # Ensure proper title format
                  if not final_content.strip().startswith('# '):
                      final_content = f'# {title}\n\n' + final_content.strip()
                  
                  # Write individual file
                  filepath = f'docs-md/{filename}'
                  with open(filepath, 'w', encoding='utf-8') as f:
                      f.write(final_content)
                  
                  all_files.append((filename, title))
                  print(f"‚úÖ Created: {filename} ({len(final_content)} chars)")
              
              # Create a comprehensive main file that combines everything
              print(f"\nCreating comprehensive documentation...")
              
              main_content = "# F1 Strategy Manager - Complete Documentation\n\n"
              main_content += "This document contains the complete documentation for the F1 Strategy Manager project.\n\n"
              main_content += "> **Note**: All diagrams are positioned exactly as they appear in the original DeepWiki documentation.\n\n"
              main_content += "## Table of Contents\n\n"
              
              # Add table of contents
              for filename, title in all_files:
                  section_link = title.lower().replace(' ', '-').replace('.', '').replace('(', '').replace(')', '')
                  main_content += f"- [{title}](#{section_link})\n"
              
              main_content += "\n---\n\n"
              
              # Add all content
              for filename, title in all_files:
                  with open(f'docs-md/{filename}', 'r', encoding='utf-8') as f:
                      content = f.read()
                  main_content += content + "\n\n---\n\n"
              
              # Write main comprehensive file
              with open('docs-md/f1-strat-manager-complete.md', 'w', encoding='utf-8') as f:
                  f.write(main_content)
              
              print(f"‚úÖ Created comprehensive documentation: f1-strat-manager-complete.md")
              print(f"Total files created: {len(all_files) + 1}")
              print(f"Files: {[f[0] for f in all_files] + ['f1-strat-manager-complete.md']}")
              
          except json.JSONDecodeError as e:
              print(f"‚ùå Error parsing JSON: {e}")
              print("Response might not be valid JSON, trying as plain text...")
              
              # Try to handle as plain text
              with open('docs-md/all-pages-raw.json', 'r', encoding='utf-8') as f:
                  content = f.read()
              
              # Process with structure preservation
              structured_content, placeholders = preserve_original_structure(content)
              cleaned_content, original_markers = clean_deepwiki_content(structured_content)
              final_content = insert_diagrams_precisely(cleaned_content, original_markers)
              
              if not final_content.strip().startswith('# '):
                  final_content = '# F1 Strategy Manager\n\n' + final_content.strip()
              
              with open('docs-md/f1-strat-manager-complete.md', 'w', encoding='utf-8') as f:
                  f.write(final_content)
              
              print("‚úÖ Processed as plain text")
              
          except Exception as e:
              print(f"‚ùå Error processing content: {e}")
              sys.exit(1)
          EOF

          echo "Final documentation files created:"
          ls -la docs-md/
          echo "Preview of main documentation:"
          head -30 docs-md/f1-strat-manager-complete.md

      - name: Checkout Wiki
        uses: actions/checkout@v3
        with:
          repository: ${{ github.repository }}.wiki
          token: ${{ secrets.WIKI_PAT }}
          path: wiki

      - name: Copy All Documentation to Wiki
        run: |
          # Verify we have files to copy
          if [ ! -d docs-md ] || [ -z "$(ls -A docs-md/*.md 2>/dev/null)" ]; then
            echo "Error: No markdown files to copy"
            exit 1
          fi

          echo "Files to copy:"
          ls -la docs-md/

          # Ensure wiki directory exists
          if [ ! -d wiki ]; then
            echo "Error: Wiki directory not found"
            exit 1
          fi

          echo "Current wiki contents:"
          ls -la wiki/

          # Copy ALL markdown files to wiki
          cp docs-md/*.md wiki/

          # Create or update index page with links to all documentation
          cat > wiki/Home.md << 'EOF'
          # Welcome to F1 Strategy Manager Wiki

          This wiki contains comprehensive documentation for the F1 Strategy Manager project.

          ## Complete Documentation

          - **[Complete Documentation](f1-strat-manager-complete)** - Full system documentation with all sections

          ## Individual Sections

          The complete documentation is also available in separate sections:
          EOF

          # Add links to individual files
          echo "" >> wiki/Home.md
          for file in docs-md/*.md; do
            if [ -f "$file" ] && [ "$(basename "$file")" != "f1-strat-manager-complete.md" ]; then
              filename=$(basename "$file" .md)
              title=$(head -1 "$file" | sed 's/^# //' | sed 's/\r$//')
              if [ ! -z "$title" ]; then
                echo "- **[$title]($filename)** - Individual section documentation" >> wiki/Home.md
              fi
            fi
          done

          cat >> wiki/Home.md << 'EOF'

          ## Project Overview

          The F1 Strategy Manager is an integrated AI-powered system for Formula 1 race strategy analysis and decision support, combining:

          - Machine learning models for predictive analytics
          - Computer vision for gap calculation
          - Natural language processing for radio analysis
          - Rule-based expert systems for strategy recommendations
          - Interactive Streamlit dashboard

          ---

          *This documentation is automatically generated from the project's DeepWiki documentation.*
          EOF

          echo "Files copied to wiki:"
          ls -la wiki/

      - name: Commit & Push changes
        working-directory: wiki
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add .

          # Check if there are changes before committing
          if git diff --quiet --staged; then
            echo "No changes to commit"
          else
            git commit -m "üîÑ Update Complete Wiki from DeepWiki - $(date '+%Y-%m-%d %H:%M')"
            git push
            echo "Changes pushed successfully"
          fi, '', content, flags=re.MULTILINE)
              
              # Clean up beginning of content
              content = content.strip()
              
              return content, diagram_markers

          def insert_diagrams_precisely(content, markers):
              """Insert diagrams at their exact locations based on context"""
              
              # Create a mapping of content sections to diagrams
              section_diagram_map = {
                  r'(?:system\s+architecture|architecture\s+overview)': 'SYSTEM_ARCHITECTURE_MAIN',
                  r'(?:data\s+flow|data\s+pipeline)': 'DATA_FLOW_SEQUENCE',
                  r'(?:expert\s+system|rule\s+engine|strategy\s+engine)': 'EXPERT_SYSTEM_RULES',
                  r'(?:machine\s+learning|ml\s+pipeline|ml\s+models)': 'ML_PIPELINE'
              }
              
              # Process content line by line to find exact insertion points
              lines = content.split('\n')
              processed_lines = []
              i = 0
              
              while i < len(lines):
                  line = lines[i]
                  processed_lines.append(line)
                  
                  # Check if this is a section header
                  if line.strip().startswith('#'):
                      header_text = line.strip().lower()
                      
                      # Check if we should insert a diagram after this header
                      for pattern, diagram_key in section_diagram_map.items():
                          if re.search(pattern, header_text, re.IGNORECASE):
                              # Look ahead to see if there's already a diagram or if there's an empty space
                              next_lines = []
                              j = i + 1
                              while j < len(lines) and j < i + 5:
                                  next_lines.append(lines[j])
                                  j += 1
                              
                              # Check if there's an empty space or placeholder for a diagram
                              combined_next = '\n'.join(next_lines)
                              
                              # Patterns that indicate a diagram should go here
                              insertion_patterns = [
                                  r'^\s*$\n\s*$',  # Multiple empty lines
                                  r'```\s*```',     # Empty code block
                                  r'!\[[^\]]*\]\([^\)]*\)',  # Image placeholder
                                  r'(?:diagram|figure|chart).*?:?\s*$',  # Diagram reference
                              ]
                              
                              should_insert = False
                              insertion_index = i + 1
                              
                              for pattern in insertion_patterns:
                                  match = re.search(pattern, combined_next, re.MULTILINE | re.IGNORECASE)
                                  if match:
                                      should_insert = True
                                      # Find the exact line where we should insert
                                      for k, next_line in enumerate(next_lines):
                                          if re.search(pattern, next_line, re.IGNORECASE):
                                              insertion_index = i + 1 + k
                                              break
                                      break
                              
                              # Also check if the next non-empty line is a paragraph that mentions the diagram
                              if not should_insert:
                                  for k, next_line in enumerate(next_lines):
                                      if next_line.strip():
                                          if re.search(r'(?:following|below|above).*?(?:diagram|illustrat|show)', next_line, re.IGNORECASE):
                                              should_insert = True
                                              insertion_index = i + 1 + k + 1
                                          break
                              
                              if should_insert:
                                  # Skip to the insertion point
                                  while i < insertion_index - 1:
                                      i += 1
                                      if i < len(lines):
                                          processed_lines.append(lines[i])
                                  
                                  # Insert the diagram
                                  processed_lines.append('')
                                  processed_lines.append(get_diagram(diagram_key))
                                  processed_lines.append('')
                                  
                                  # Skip any empty lines or placeholders at the insertion point
                                  while i + 1 < len(lines) and (not lines[i + 1].strip() or 
                                                                 re.match(r'```\s*```', lines[i + 1]) or
                                                                 re.match(r'!\[[^\]]*\]\([^\)]*\)', lines[i + 1])):
                                      i += 1
                              
                              break
                  
                  i += 1
              
              return '\n'.join(processed_lines)

          def preserve_original_structure(content):
              """Preserve the exact structure and spacing from the original content"""
              
              # Mark all potential diagram locations with placeholders
              placeholder_count = 0
              placeholders = {}
              
              # Find empty code blocks
              lines = content.split('\n')
              new_lines = []
              i = 0
              while i < len(lines):
                  if i + 1 < len(lines) and lines[i].strip() == '```' and lines[i+1].strip() == '```':
                      placeholder = f"<<<DIAGRAM_PLACEHOLDER_{placeholder_count}>>>"
                      placeholders[placeholder] = ('EMPTY_BLOCK', i)
                      placeholder_count += 1
                      new_lines.append(placeholder)
                      i += 2
                  else:
                      new_lines.append(lines[i])
                      i += 1
              content = '\n'.join(new_lines)
              
              # Find and mark image references
              import re
              for match in re.finditer(r'!\[([^\]]*)\]\(([^\)]*)\)', content):
                  placeholder = f"<<<DIAGRAM_PLACEHOLDER_{placeholder_count}>>>"
                  placeholders[placeholder] = ('IMAGE', match.start(), match.group(1), match.group(2))
                  placeholder_count += 1
                  content = content[:match.start()] + placeholder + content[match.end():]
              
              # Find and mark sections that reference diagrams
              lines = content.split('\n')
              for i, line in enumerate(lines):
                  if re.search(r'(?:following|below|above).*?(?:diagram|illustrat|show|figure)', line, re.IGNORECASE):
                      # Mark the next empty area as a diagram location
                      for j in range(i + 1, min(i + 5, len(lines))):
                          if not lines[j].strip() and j + 1 < len(lines) and not lines[j + 1].strip():
                              placeholder = f"<<<DIAGRAM_PLACEHOLDER_{placeholder_count}>>>"
                              lines[j] = placeholder
                              placeholders[placeholder] = ('DIAGRAM_AREA', j)
                              placeholder_count += 1
                              break
              
              content = '\n'.join(lines)
              
              return content, placeholders

          def get_page_title_from_content(content):
              """Extract a clean title from content"""
              lines = content.split('\n')
              for line in lines:
                  if line.strip().startswith('# ') and len(line.strip()) > 2:
                      title = line.strip()[2:].strip()
                      # Clean the title
                      title = re.sub(r'^/.*?/', '', title)  # Remove leading path
                      if title:
                          return title
              return "Documentation"

          def generate_filename(title, url=""):
              """Generate a safe filename from title"""
              if url and "F1_Strat_Manager/" in url:
                  # Extract section from URL
                  section = url.split("F1_Strat_Manager/")[-1]
                  if section and section != "":
                      return section.replace("-", "_").replace(".", "_") + ".md"
              
              # Fallback to title-based filename
              filename = title.lower()
              filename = re.sub(r'[^\w\s-]', '', filename)
              filename = re.sub(r'[-\s]+', '-', filename)
              return filename + ".md"

          # Read the JSON response
          try:
              with open('docs-md/all-pages-raw.json', 'r', encoding='utf-8') as f:
                  data = json.load(f)
              
              print(f"Response structure: {list(data.keys())}")
              
              # Handle different response structures
              pages_content = []
              
              if 'result' in data and 'content' in data['result']:
                  if isinstance(data['result']['content'], list):
                      # Multiple content items
                      for item in data['result']['content']:
                          if 'text' in item:
                              pages_content.append(item['text'])
                  else:
                      # Single content item
                      if 'text' in data['result']['content']:
                          pages_content.append(data['result']['content']['text'])
              elif 'result' in data and isinstance(data['result'], str):
                  # Direct string result
                  pages_content.append(data['result'])
              else:
                  print("‚ùå Unexpected response structure")
                  print(f"Data keys: {list(data.keys())}")
                  sys.exit(1)
              
              print(f"Found {len(pages_content)} content sections")
              
              # Process each page/section
              all_files = []
              for i, content in enumerate(pages_content):
                  print(f"\nProcessing content section {i+1}...")
                  print(f"Content length: {len(content)} characters")
                  
                  # First preserve the structure
                  structured_content, placeholders = preserve_original_structure(content)
                  
                  # Clean the content while preserving placeholders
                  cleaned_content, original_markers = clean_deepwiki_content(structured_content)
                  
                  # Insert diagrams at the exact preserved locations
                  final_content = insert_diagrams_precisely(cleaned_content, original_markers)
                  
                  if len(final_content.strip()) < 50:  # Skip very short content
                      print(f"Skipping short content section {i+1}")
                      continue
                  
                  # Get title and filename
                  title = get_page_title_from_content(final_content)
                  filename = generate_filename(title)
                  
                  # Ensure proper title format
                  if not final_content.strip().startswith('# '):
                      final_content = f'# {title}\n\n' + final_content.strip()
                  
                  # Write individual file
                  filepath = f'docs-md/{filename}'
                  with open(filepath, 'w', encoding='utf-8') as f:
                      f.write(final_content)
                  
                  all_files.append((filename, title))
                  print(f"‚úÖ Created: {filename} ({len(final_content)} chars)")
              
              # Create a comprehensive main file that combines everything
              print(f"\nCreating comprehensive documentation...")
              
              main_content = "# F1 Strategy Manager - Complete Documentation\n\n"
              main_content += "This document contains the complete documentation for the F1 Strategy Manager project.\n\n"
              main_content += "> **Note**: All diagrams are positioned exactly as they appear in the original DeepWiki documentation.\n\n"
              main_content += "## Table of Contents\n\n"
              
              # Add table of contents
              for filename, title in all_files:
                  section_link = title.lower().replace(' ', '-').replace('.', '').replace('(', '').replace(')', '')
                  main_content += f"- [{title}](#{section_link})\n"
              
              main_content += "\n---\n\n"
              
              # Add all content
              for filename, title in all_files:
                  with open(f'docs-md/{filename}', 'r', encoding='utf-8') as f:
                      content = f.read()
                  main_content += content + "\n\n---\n\n"
              
              # Write main comprehensive file
              with open('docs-md/f1-strat-manager-complete.md', 'w', encoding='utf-8') as f:
                  f.write(main_content)
              
              print(f"‚úÖ Created comprehensive documentation: f1-strat-manager-complete.md")
              print(f"Total files created: {len(all_files) + 1}")
              print(f"Files: {[f[0] for f in all_files] + ['f1-strat-manager-complete.md']}")
              
          except json.JSONDecodeError as e:
              print(f"‚ùå Error parsing JSON: {e}")
              print("Response might not be valid JSON, trying as plain text...")
              
              # Try to handle as plain text
              with open('docs-md/all-pages-raw.json', 'r', encoding='utf-8') as f:
                  content = f.read()
              
              # Process with structure preservation
              structured_content, placeholders = preserve_original_structure(content)
              cleaned_content, original_markers = clean_deepwiki_content(structured_content)
              final_content = insert_diagrams_precisely(cleaned_content, original_markers)
              
              if not final_content.strip().startswith('# '):
                  final_content = '# F1 Strategy Manager\n\n' + final_content.strip()
              
              with open('docs-md/f1-strat-manager-complete.md', 'w', encoding='utf-8') as f:
                  f.write(final_content)
              
              print("‚úÖ Processed as plain text")
              
          except Exception as e:
              print(f"‚ùå Error processing content: {e}")
              sys.exit(1)
          EOF

          echo "Final documentation files created:"
          ls -la docs-md/
          echo "Preview of main documentation:"
          head -30 docs-md/f1-strat-manager-complete.md

      - name: Checkout Wiki
        uses: actions/checkout@v3
        with:
          repository: ${{ github.repository }}.wiki
          token: ${{ secrets.WIKI_PAT }}
          path: wiki

      - name: Copy All Documentation to Wiki
        run: |
          # Verify we have files to copy
          if [ ! -d docs-md ] || [ -z "$(ls -A docs-md/*.md 2>/dev/null)" ]; then
            echo "Error: No markdown files to copy"
            exit 1
          fi

          echo "Files to copy:"
          ls -la docs-md/

          # Ensure wiki directory exists
          if [ ! -d wiki ]; then
            echo "Error: Wiki directory not found"
            exit 1
          fi

          echo "Current wiki contents:"
          ls -la wiki/

          # Copy ALL markdown files to wiki
          cp docs-md/*.md wiki/

          # Create or update index page with links to all documentation
          cat > wiki/Home.md << 'EOF'
          # Welcome to F1 Strategy Manager Wiki

          This wiki contains comprehensive documentation for the F1 Strategy Manager project.

          ## Complete Documentation

          - **[Complete Documentation](f1-strat-manager-complete)** - Full system documentation with all sections

          ## Individual Sections

          The complete documentation is also available in separate sections:
          EOF

          # Add links to individual files
          echo "" >> wiki/Home.md
          for file in docs-md/*.md; do
            if [ -f "$file" ] && [ "$(basename "$file")" != "f1-strat-manager-complete.md" ]; then
              filename=$(basename "$file" .md)
              title=$(head -1 "$file" | sed 's/^# //' | sed 's/\r$//')
              if [ ! -z "$title" ]; then
                echo "- **[$title]($filename)** - Individual section documentation" >> wiki/Home.md
              fi
            fi
          done

          cat >> wiki/Home.md << 'EOF'

          ## Project Overview

          The F1 Strategy Manager is an integrated AI-powered system for Formula 1 race strategy analysis and decision support, combining:

          - Machine learning models for predictive analytics
          - Computer vision for gap calculation
          - Natural language processing for radio analysis
          - Rule-based expert systems for strategy recommendations
          - Interactive Streamlit dashboard

          ---

          *This documentation is automatically generated from the project's DeepWiki documentation.*
          EOF

          echo "Files copied to wiki:"
          ls -la wiki/

      - name: Commit & Push changes
        working-directory: wiki
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add .

          # Check if there are changes before committing
          if git diff --quiet --staged; then
            echo "No changes to commit"
          else
            git commit -m "üîÑ Update Complete Wiki from DeepWiki - $(date '+%Y-%m-%d %H:%M')"
            git push
            echo "Changes pushed successfully"
          fi
