{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tire Degradation Prediction Model - Week 5\n",
    "\n",
    "## Overview\n",
    "This notebook implements advanced models to predict tire degradation in Formula 1 races. Since tire degradation is not strictly linear and depends on multiple factors (compound type, track temperature, driving style, etc.), we'll use sequence models like LSTM to capture these complex patterns.\n",
    "\n",
    "## Approach\n",
    "1. **Data Exploration**\n",
    "   - Analyze the relationship between lap times and tire age\n",
    "   - Visualize performance degradation patterns by compound\n",
    "   - Determine how to quantify \"degradation\" (lap time delta or derived metric)\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Create a derived tire degradation metric\n",
    "   - Organize data into sequential format for LSTM\n",
    "   - Normalize features appropriately\n",
    "   - Create sliding windows of previous laps to predict future performance\n",
    "\n",
    "3. **Model Development**\n",
    "   - **Primary Model**: LSTM network to predict degradation trajectory\n",
    "   - **Alternative Model**: XGBoost with quantile regression for uncertainty estimation\n",
    "\n",
    "4. **Evaluation & Visualization**\n",
    "   - Compare predicted vs. actual degradation curves\n",
    "   - Analyze performance across different compounds and race conditions\n",
    "   - Create interactive Plotly visualizations of degradation patterns\n",
    "\n",
    "5. **Implementation Details**\n",
    "   - Sequence length: 5 laps (input) â†’ predict next 3-5 laps\n",
    "   - Features: Tire age, compound, lap time trends, fuel load\n",
    "   - Target: Derived degradation metric or direct lap time prediction\n",
    "\n",
    "## Expected Outcomes\n",
    "- Trained model to predict tire performance over extended stints\n",
    "- Uncertainty bounds for degradation predictions (10th, 50th, 90th percentiles)\n",
    "- Interactive visualization of degradation curves by compound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Necessary Libraries and Creatind New Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions from our module\n",
    "from ML_utils.N00_model_lap_prediction import compound_colors, compound_names\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs('../../outputs/week5/models', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../outputs/week3/lap_prediction_data.csv\")\n",
    "print(\"\\nRegular data sample:\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = pd.read_csv(\"../../outputs/week3/sequential_lap_prediction_data.csv\")\n",
    "print(\"\\nSequential data sample:\")\n",
    "display(seq_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"Basic dataset information:\")\n",
    "print(f\"Regular data shape: {data.shape}\")\n",
    "print(f\"Sequential data shape: {seq_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Locating Tire Related Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for tire-related columns\n",
    "tire_columns = ['CompoundID', 'TyreAge']\n",
    "print(f\"\\nTire-related columns: {tire_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for tire-related columns\n",
    "print(\"\\nTire-related statistics:\")\n",
    "display(data[tire_columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compound Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print compound mappings for reference\n",
    "print(\"\\nCompound mappings:\")\n",
    "print(f\"Compound names: {compound_names}\")\n",
    "print(f\"Compound colors: {compound_colors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Relationship between Tire Age and Lap Time by compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulate the tire-degradation-by-compound plot into a function\n",
    "def plot_tire_degradation_by_compound(\n",
    "    data,\n",
    "    compound_colors,\n",
    "    compound_names,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot lap time vs tire age for each compound, with error bands,\n",
    "    and optionally save the figure.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): must contain 'CompoundID', 'TyreAge', 'LapTime'\n",
    "        compound_colors (dict): mapping CompoundID -> color string\n",
    "        compound_names (dict): mapping CompoundID -> display name\n",
    "        save_path (str or Path, optional): filepath to save the figure\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: the generated figure\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Loop over each compound\n",
    "    for compound_id in data['CompoundID'].unique():\n",
    "        subset = data[data['CompoundID'] == compound_id]\n",
    "        agg = (\n",
    "            subset\n",
    "            .groupby('TyreAge')['LapTime']\n",
    "            .agg(['mean', 'std', 'count'])\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Only plot if there's more than one point\n",
    "        if len(agg) > 1:\n",
    "            color = compound_colors.get(compound_id, 'black')\n",
    "            name = compound_names.get(compound_id, f'Unknown ({compound_id})')\n",
    "\n",
    "            ax.plot(\n",
    "                agg['TyreAge'],\n",
    "                agg['mean'],\n",
    "                'o-',\n",
    "                color=color,\n",
    "                label=f'{name} Tire'\n",
    "            )\n",
    "            # Error band\n",
    "            ax.fill_between(\n",
    "                agg['TyreAge'],\n",
    "                agg['mean'] - agg['std'],\n",
    "                agg['mean'] + agg['std'],\n",
    "                color=color,\n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel('Tire Age (laps)')\n",
    "    ax.set_ylabel('Lap Time (s)')\n",
    "    ax.set_title('Tire Degradation: Effect on Lap Time')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        fig.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "fig_tire_curve = plot_tire_degradation_by_compound(\n",
    "    data,\n",
    "    compound_colors,\n",
    "    compound_names,\n",
    "    save_path='../../outputs/week5/tire_deg_curve.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploring Lap Times Deltas and Tire Ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulate lap time delta exploration into a function\n",
    "def plot_lap_time_deltas_by_tyre_age(\n",
    "    seq_data,\n",
    "    compound_colors,\n",
    "    compound_names,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot average LapTime_Delta vs TyreAge for each compound.\n",
    "    Positive delta means lap times are getting slower.\n",
    "\n",
    "    Args:\n",
    "        seq_data (pd.DataFrame): must contain 'CompoundID', 'TyreAge', 'LapTime_Delta'\n",
    "        compound_colors (dict): mapping CompoundID -> color\n",
    "        compound_names (dict): mapping CompoundID -> display name\n",
    "        save_path (str or Path, optional): path to save the figure\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: the generated figure\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if 'LapTime_Delta' not in seq_data.columns:\n",
    "        print(\"LapTime_Delta column not available in the data.\")\n",
    "        return None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Loop through compounds\n",
    "    for compound_id in seq_data['CompoundID'].unique():\n",
    "        subset = seq_data[seq_data['CompoundID'] == compound_id]\n",
    "        agg = subset.groupby('TyreAge')['LapTime_Delta'].mean().reset_index()\n",
    "\n",
    "        if len(agg) > 1:\n",
    "            color = compound_colors.get(compound_id, 'black')\n",
    "            name = compound_names.get(compound_id, f'Unknown ({compound_id})')\n",
    "            ax.plot(\n",
    "                agg['TyreAge'],\n",
    "                agg['LapTime_Delta'],\n",
    "                'o-',\n",
    "                color=color,\n",
    "                label=f'{name} Tire'\n",
    "            )\n",
    "\n",
    "    ax.axhline(y=0, linestyle='--', color='black', alpha=0.5)\n",
    "    ax.set_xlabel('Tire Age (laps)')\n",
    "    ax.set_ylabel('Lap Time Delta (s) - Positive means getting slower')\n",
    "    ax.set_title('Lap Time Degradation Rate by Tire Age')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Save figure if a path is provided\n",
    "    if save_path:\n",
    "        fig.savefig(save_path)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Call the function on your DataFrame\n",
    "fig_lap_delta = plot_lap_time_deltas_by_tyre_age(\n",
    "    seq_data,\n",
    "    compound_colors,\n",
    "    compound_names,\n",
    "    save_path='../../outputs/week5/tire_deg_rate.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploring if Tire Age affects Speed in different Sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulate the tire-age speed effect plot into a function\n",
    "def plot_tire_age_speed_effect(\n",
    "    data,\n",
    "    compound_id=2,\n",
    "    compound_names=None,\n",
    "    speed_columns=None,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot how tire age affects speed in specified sectors for a given compound.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): must contain 'CompoundID', 'TyreAge' and the speed columns\n",
    "        compound_id (int): the compound to focus on\n",
    "        compound_names (dict): mapping CompoundID -> display name\n",
    "        speed_columns (list of str): list of speed column names to plot\n",
    "        save_path (str or Path, optional): filepath to save the figure\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: the generated figure\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if speed_columns is None:\n",
    "        speed_columns = ['SpeedI1', 'SpeedI2', 'SpeedFL']\n",
    "    if compound_names is None:\n",
    "        compound_display = str(compound_id)\n",
    "    else:\n",
    "        compound_display = compound_names.get(compound_id, f'Compound {compound_id}')\n",
    "\n",
    "    # Filter to the chosen compound\n",
    "    subset = data[data['CompoundID'] == compound_id]\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot each speed column\n",
    "    for speed_col in speed_columns:\n",
    "        if speed_col not in subset.columns:\n",
    "            continue\n",
    "        agg = subset.groupby('TyreAge')[speed_col].mean().reset_index()\n",
    "        if len(agg) > 1:\n",
    "            ax.plot(\n",
    "                agg['TyreAge'],\n",
    "                agg[speed_col],\n",
    "                'o-',\n",
    "                label=speed_col\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel('Tire Age (laps)')\n",
    "    ax.set_ylabel('Speed (kph)')\n",
    "    ax.set_title(f'Effect of Tire Age on Speed - {compound_display} Tires')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        fig.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Call the function on your DataFrame\n",
    "fig_speed_effect = plot_tire_age_speed_effect(\n",
    "    data,\n",
    "    compound_id=2,\n",
    "    compound_names=compound_names,\n",
    "    speed_columns=['SpeedI1', 'SpeedI2', 'SpeedFL'],\n",
    "    save_path='../../outputs/week5/tire_age_speed_effect.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating Tire Degradation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For being able to predict tyre degradation more effectively, a good option can be generate more variables with the current data that we have. Therefore, IÂ´ll add this new variables:\n",
    "\n",
    "## Disclaimer: importance of fuel load.\n",
    "\n",
    "After making the cells and looking at the data, some tires shows positive degradation. That means that lap times are descending instead of going up. This is caused due to the less amount of fuel during the race. Therefore, I need to **create an adjusted lap time** that takes into account this fuel factor before creating our prediction models. \n",
    "\n",
    "Then, I will create this variable and then adjust the plots and variable calculation for fitting this feature.\n",
    "\n",
    "*NOTE*: fuel burn calculation and impact will be calculated according to this articles: \n",
    "\n",
    "- [BBC Sport Weight Reduction](https://www.bbc.com/sport/articles/cv2g715dkk1o#:~:text=A%201.5kg%20reduction%20in,so%20over%20a%20race%20distance.)\n",
    "\n",
    "- [Fuel Correction Analysis, Medium](https://medium.com/@umakschually/fuel-correction-29ccd98ae62b#:~:text=Rule%20of%20thumb%20is%20that,tyre%20age%20or%20anything%20else.)\n",
    "\n",
    "#### 1. Absolute Tire Degradation (TireDegAbsolute)\n",
    "\n",
    "Its objective is to **measure how much seconds is the actual lap time slower, compared with the baseline** (new tires or with the less degradation possible, for instance, only 2 laps).\n",
    "\n",
    "**Positive values** implie degradation (car is getting slower). As I said, it would be measured in **seconds**.\n",
    "\n",
    "- *Utility*:\n",
    "    - Allows knowing the direct impact on lap time.\n",
    "    - Helps to determine the *cross point* when a pit stop becomes an advantage.\n",
    "    - Fundamental for strategic calculus, as teams work with absolute times.\n",
    "    - Helps us answering the following: **How many seconds are we losing per lap with degradation?**\n",
    "\n",
    "#### 2. Tire Degradation Percentage \n",
    "\n",
    "It expresses degradation as an **augmenting percentage** to base time. For instance, 2% means the car is 2% slower than with new tires.\n",
    "\n",
    "- *Utility*:\n",
    "    - Allows more intuitive comparisons between different conditions.\n",
    "    - Normalizes the data for more clear comparisons between tires.\n",
    "    - Helps us aswering the following:**Which compound maintains better its relative performance?**\n",
    "\n",
    "#### 3. Degradation Rate\n",
    "\n",
    "Means how much time increases per lap with each aditional lap. Represents the first derivative of degradation curve. \n",
    "\n",
    "- *Utility:*\n",
    "    - Allows knowing if degradation is lineal, progressive or if it stabilizes.\n",
    "    - Crucial for estimating optimum pit stop window during races.\n",
    "    - Allows anticipating future compoundÂ´s behaviour.\n",
    "    - Helps us answering the following: **Degradation is getting worse or it is stabilizing?**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Lap Time Improvement Per Lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear mÃ©tricas de degradaciÃ³n ajustadas usando directamente la mejora de tiempo por vuelta\n",
    "# Tiempo que mejora cada vuelta debido a la reducciÃ³n de combustible\n",
    "LAP_TIME_IMPROVEMENT_PER_LAP = 0.055  # segundos por vuelta (punto medio de 0.05-0.06s)\n",
    "\n",
    "# Create a DataFrame to store all results with fuel adjustment\n",
    "tire_deg_data = pd.DataFrame()\n",
    "\n",
    "# Process each compound separately\n",
    "for compound_id in data['CompoundID'].unique():\n",
    "    compound_name = compound_names.get(compound_id, f\"Unknown ({compound_id})\")\n",
    "    print(f\"Processing {compound_name} tires (ID: {compound_id})...\")\n",
    "    \n",
    "    # Filter for this compound\n",
    "    compound_data = data[data['CompoundID'] == compound_id].copy()\n",
    "    \n",
    "    # Sort by TyreAge to see the degradation trend\n",
    "    compound_data = compound_data.sort_values('TyreAge')\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(compound_data) < 5:\n",
    "        print(f\"  Not enough data for {compound_name} tires, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Find baseline information\n",
    "    if 1 in compound_data['TyreAge'].values:\n",
    "        # Get baseline data (TyreAge=1)\n",
    "        baseline_data = compound_data[compound_data['TyreAge'] == 1]\n",
    "        baseline_lap_time = baseline_data['LapTime'].mean()\n",
    "        baseline_tire_age = 1\n",
    "    else:\n",
    "        # If no 'new tire' laps, use the minimum TyreAge available\n",
    "        min_age = compound_data['TyreAge'].min()\n",
    "        baseline_data = compound_data[compound_data['TyreAge'] == min_age]\n",
    "        baseline_lap_time = baseline_data['LapTime'].mean()\n",
    "        baseline_tire_age = min_age\n",
    "        print(f\"  No laps with new tires for {compound_name}, using TyreAge={min_age} as baseline\")\n",
    "    \n",
    "    # Calculate fuel adjustment directly based on laps from baseline\n",
    "    compound_data['LapsFromBaseline'] = compound_data['TyreAge'] - baseline_tire_age\n",
    "    compound_data['FuelEffect'] = compound_data['LapsFromBaseline'] * LAP_TIME_IMPROVEMENT_PER_LAP\n",
    "    \n",
    "    # Calculate fuel-adjusted lap time\n",
    "    compound_data['FuelAdjustedLapTime'] = compound_data['LapTime'] + compound_data['FuelEffect']\n",
    "    \n",
    "    # Calculate traditional degradation metrics\n",
    "    compound_data['TireDegAbsolute'] = compound_data['LapTime'] - baseline_lap_time\n",
    "    compound_data['TireDegPercent'] = (compound_data['LapTime'] / baseline_lap_time - 1) * 100\n",
    "    \n",
    "    # Calculate fuel-adjusted degradation metrics\n",
    "    baseline_adjusted_lap_time = baseline_lap_time  # For new tires, no adjustment needed\n",
    "    compound_data['FuelAdjustedDegAbsolute'] = compound_data['FuelAdjustedLapTime'] - baseline_adjusted_lap_time\n",
    "    compound_data['FuelAdjustedDegPercent'] = (compound_data['FuelAdjustedLapTime'] / baseline_adjusted_lap_time - 1) * 100\n",
    "    \n",
    "    # Add compound info for later aggregation\n",
    "    compound_data['CompoundName'] = compound_name\n",
    "    \n",
    "    # Add to the combined DataFrame\n",
    "    tire_deg_data = pd.concat([tire_deg_data, compound_data])\n",
    "    \n",
    "    # Calculate maximum laps and total fuel effect\n",
    "    max_laps = compound_data['TyreAge'].max() - baseline_tire_age\n",
    "    total_fuel_effect = max_laps * LAP_TIME_IMPROVEMENT_PER_LAP\n",
    "    \n",
    "    print(f\"  Baseline lap time for {compound_name}: {baseline_lap_time:.3f}s\")\n",
    "    print(f\"  Maximum laps from baseline: {max_laps:.0f}\")\n",
    "    print(f\"  Estimated total fuel benefit: ~{total_fuel_effect:.2f}s\")\n",
    "    print(f\"  Processed {len(compound_data)} laps with {compound_name} tires\")\n",
    "\n",
    "# Display comparison between regular and fuel-adjusted metrics\n",
    "print(\"\\nComparison of regular vs. fuel-adjusted metrics (sample):\")\n",
    "sample_comparison = tire_deg_data.groupby(['CompoundName', 'TyreAge'])[\n",
    "    ['TireDegAbsolute', 'FuelAdjustedDegAbsolute', 'FuelEffect']\n",
    "].mean().reset_index()\n",
    "display(sample_comparison.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Diferrence between regular and Fuel Adjusted Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regular_vs_adjusted_degradation(tire_deg_data, compound_names, compound_colors, LAP_TIME_IMPROVEMENT_PER_LAP):\n",
    "    # Create a comparison of regular vs fuel-adjusted degradation\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    compound_ids = tire_deg_data['CompoundID'].unique()\n",
    "    \n",
    "    # Loop through the compounds to create comparison plots\n",
    "    for i, compound_id in enumerate(compound_ids):\n",
    "        compound_subset = tire_deg_data[tire_deg_data['CompoundID'] == compound_id]\n",
    "        color = compound_colors.get(compound_id, 'black')\n",
    "        compound_name = compound_names.get(compound_id, f'Unknown ({compound_id})')\n",
    "        \n",
    "        # Calculate means for regular and adjusted degradation\n",
    "        reg_agg = compound_subset.groupby('TyreAge')['TireDegAbsolute'].mean()\n",
    "        adj_agg = compound_subset.groupby('TyreAge')['FuelAdjustedDegAbsolute'].mean()\n",
    "        \n",
    "        # Create subplot\n",
    "        plt.subplot(len(compound_ids), 1, i+1)\n",
    "        \n",
    "        # Plot regular degradation\n",
    "        plt.plot(reg_agg.index, reg_agg.values, 'o--', \n",
    "                 color=color, alpha=0.5, label=f'{compound_name} (Regular)')\n",
    "        \n",
    "        # Plot fuel-adjusted degradation\n",
    "        plt.plot(adj_agg.index, adj_agg.values, 'o-', \n",
    "                 color=color, linewidth=2, label=f'{compound_name} (Fuel Adjusted)')\n",
    "        \n",
    "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "        plt.ylabel('Degradation (s)')\n",
    "        plt.title(f'{compound_name} Tire Degradation: Regular vs. Fuel-Adjusted')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        \n",
    "        min_lap = reg_agg.index.min()\n",
    "        max_lap = reg_agg.index.max()\n",
    "        total_laps = max_lap - min_lap\n",
    "        total_fuel_effect = total_laps * LAP_TIME_IMPROVEMENT_PER_LAP\n",
    "        plt.annotate(f\"Est. total fuel effect: ~{total_fuel_effect:.2f}s\", \n",
    "                     xy=(0.02, 0.05), xycoords='axes fraction',\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
    "        \n",
    "        if i == len(compound_ids)-1:  \n",
    "            plt.xlabel('Tire Age (laps)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../../outputs/week5/regular_vs_adjusted_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_regular_vs_adjusted_degradation(tire_deg_data, compound_names, compound_colors, LAP_TIME_IMPROVEMENT_PER_LAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Absolute Tire Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fuel_adjusted_degradation(tire_deg_data, compound_names, compound_colors):\n",
    "    # Visualize the fuel-adjusted absolute degradation\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    compound_ids = tire_deg_data['CompoundID'].unique()\n",
    "\n",
    "    for compound_id in compound_ids:\n",
    "        compound_subset = tire_deg_data[tire_deg_data['CompoundID'] == compound_id]\n",
    "        color = compound_colors.get(compound_id, 'black')\n",
    "        compound_name = compound_names.get(compound_id, f'Unknown ({compound_id})')\n",
    "        \n",
    "        # Aggregate data for line plot\n",
    "        agg_data = compound_subset.groupby('TyreAge')['FuelAdjustedDegAbsolute'].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        # Plot mean line\n",
    "        plt.plot(agg_data['TyreAge'], agg_data['mean'], 'o-', \n",
    "                 color=color, linewidth=2, label=f'{compound_name}')\n",
    "        \n",
    "        # Add error bands if we have standard deviation\n",
    "        if 'std' in agg_data.columns and not agg_data['std'].isnull().all():\n",
    "            plt.fill_between(agg_data['TyreAge'], \n",
    "                             agg_data['mean'] - agg_data['std'], \n",
    "                             agg_data['mean'] + agg_data['std'],\n",
    "                             color=color, alpha=0.2)\n",
    "\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel('Tire Age (laps)')\n",
    "    plt.ylabel('Fuel-Adjusted Absolute Degradation (s)')\n",
    "    plt.title('Tire Degradation by Compound and Age (Fuel Effect Removed)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('../../outputs/week5/fuel_adjusted_deg_by_compound.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_fuel_adjusted_degradation(tire_deg_data, compound_names, compound_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Tire Degradation Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fuel_adjusted_percentage_degradation(tire_deg_data, compound_names, compound_colors):\n",
    "    # Visualize the fuel-adjusted percentage degradation\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    compound_ids = tire_deg_data['CompoundID'].unique()\n",
    "\n",
    "    for compound_id in compound_ids:\n",
    "        compound_subset = tire_deg_data[tire_deg_data['CompoundID'] == compound_id]\n",
    "        color = compound_colors.get(compound_id, 'black')\n",
    "        compound_name = compound_names.get(compound_id, f'Unknown ({compound_id})')\n",
    "        \n",
    "        # Aggregate data for line plot\n",
    "        agg_data = compound_subset.groupby('TyreAge')['FuelAdjustedDegPercent'].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        # Plot mean line\n",
    "        plt.plot(agg_data['TyreAge'], agg_data['mean'], 'o-', \n",
    "                 color=color, linewidth=2, label=f'{compound_name}')\n",
    "        \n",
    "        # Add error bands if we have standard deviation\n",
    "        if 'std' in agg_data.columns and not agg_data['std'].isnull().all():\n",
    "            plt.fill_between(agg_data['TyreAge'], \n",
    "                             agg_data['mean'] - agg_data['std'], \n",
    "                             agg_data['mean'] + agg_data['std'],\n",
    "                             color=color, alpha=0.2)\n",
    "\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel('Tire Age (laps)')\n",
    "    plt.ylabel('Fuel-Adjusted Percentage Degradation (%)')\n",
    "    plt.title('Percentage Tire Degradation by Compound and Age (Fuel Effect Removed)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('../../outputs/week5/fuel_adjusted_deg_percent_by_compound.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_fuel_adjusted_percentage_degradation(tire_deg_data, compound_names, compound_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Tire Degradation Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_ids = tire_deg_data['CompoundID'].unique()\n",
    "\n",
    "# After plotting, add the variable to the dataframe\n",
    "for compound_id in compound_ids:\n",
    "    # Recalculate using the same method as in the visualization\n",
    "    compound_subset = tire_deg_data[tire_deg_data['CompoundID'] == compound_id]\n",
    "    avg_laptimes = compound_subset.groupby('TyreAge')['FuelAdjustedLapTime'].mean()\n",
    "    deg_rates = avg_laptimes.diff()\n",
    "    \n",
    "    # Assign values to the dataframe\n",
    "    for age, rate in zip(deg_rates.index, deg_rates.values):\n",
    "        mask = (tire_deg_data['CompoundID'] == compound_id) & (tire_deg_data['TyreAge'] == age)\n",
    "        tire_deg_data.loc[mask, 'DegradationRate'] = rate\n",
    "\n",
    "# Verify that it has been added correctly\n",
    "print(\"\\nFirst rows with DegradationRate:\")\n",
    "display(tire_deg_data[['CompoundID', 'TyreAge', 'FuelAdjustedLapTime', 'DegradationRate']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fuel_adjusted_degradation_rate(tire_deg_data, compound_names, compound_colors):\n",
    "    # Plot a line chart showing Tire Degradation Rate by compound with error bands\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    compound_ids = tire_deg_data['CompoundID'].unique()\n",
    "\n",
    "    for compound_id in compound_ids:\n",
    "        # Filter the data for the current compound\n",
    "        compound_subset = tire_deg_data[tire_deg_data['CompoundID'] == compound_id]\n",
    "        \n",
    "        # Calculate the average and standard deviation of degradation rate per tire age\n",
    "        deg_stats = compound_subset.groupby('TyreAge')['DegradationRate'].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        # Get color and compound name for the plot\n",
    "        color = compound_colors.get(compound_id, 'black')\n",
    "        compound_name = compound_names.get(compound_id, f'Unknown ({compound_id})')\n",
    "        \n",
    "        # Plot the line for this compound\n",
    "        plt.plot(deg_stats['TyreAge'], deg_stats['mean'], marker='o', linestyle='-',\n",
    "                 color=color, linewidth=2, label=compound_name)\n",
    "        \n",
    "        # Add error bands (standard deviation)\n",
    "        if 'std' in deg_stats.columns and not deg_stats['std'].isnull().all():\n",
    "            plt.fill_between(deg_stats['TyreAge'], \n",
    "                             deg_stats['mean'] - deg_stats['std'], \n",
    "                             deg_stats['mean'] + deg_stats['std'],\n",
    "                             color=color, alpha=0.2)\n",
    "\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel('Tire Age (laps)')\n",
    "    plt.ylabel('Fuel-Adjusted Degradation Rate (s/lap)')\n",
    "    plt.title('Tire Degradation Rate by Compound (Fuel Effect Removed)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_fuel_adjusted_degradation_rate(tire_deg_data, compound_names, compound_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing NaNs. This are due to diff pandas method. Assume 0 as new tires do not have degradation still\n",
    "\n",
    "num_nans = tire_deg_data['DegradationRate'].isna().sum()\n",
    "print(num_nans)\n",
    "\n",
    "tire_deg_data['DegradationRate'] = tire_deg_data['DegradationRate'].fillna(0)\n",
    "print(f\"Number of NaN after sustitution: {tire_deg_data['DegradationRate'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "#### Medium Tire (Yellow).\n",
    "\n",
    "- The fuel effect was masking significantly degradation. With the adjustment, mroe degradation can be seen.\n",
    "\n",
    "- They offer the best initial advantage (-4 seconds), that stabilizes in -3 seconds until aproximately lap 30.\n",
    "\n",
    "- Total fuel impact is about 1.73 seconds faster at the end of the stint.\n",
    "\n",
    "- They represent the best balance between performance and durability.\n",
    "\n",
    "#### Hard Tire (Gray)\n",
    "\n",
    "- Fuel effect made an stabilization ilusion and even improvement. The adjust reveals a constant and progressive degradation that goes up to +2seconds.\n",
    "\n",
    "- Fuel effect on this tires are the biggest, with 2.64 fastet.\n",
    "\n",
    "- Degradation rate is more stable and predictable, making them ideal for long stints.\n",
    "\n",
    "#### Soft Tire (Red)\n",
    "\n",
    "- There is a bigger volatile effect that it seemed without fuel effect.\n",
    "- Erratic behaviour and big fluctuations after lap 20.\n",
    "- Highly unpredictable and dramatically fluctating, specially after lap 20, with extreme degradation peaks of +2 seconds slower per lap.\n",
    "\n",
    "### Detected Turning Points\n",
    "\n",
    "- *SOFT*: show a cliff degradation over 20 laps.\n",
    "- *MEDIUM*: change in pattern near lap 30.\n",
    "- *HARD*: show a degradation increase after lap 40.\n",
    "\n",
    "\n",
    "### Good Conclussions for predictive model. \n",
    "\n",
    "1. Fuel adjustment was essential for identifying true degradation.\n",
    "2. Each compound shows unique patterns that can be useful for the model:\n",
    "    - Soft tire as high volatile with critic points of sudden degradation.\n",
    "    - Medium tire has a fast fall followed by stabilization.\n",
    "    - Hard tire with slow but continous degradation.\n",
    "\n",
    "3. Identified turning points are going to be crucial parameters for the LSTM or XgBoost, as they show critic moments for pit stop strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Correlation Analysis: Tire-Related Factors with Lap Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert dictionary to apply conversion to compound names to numbers \n",
    "compound_names_inv = {value: key for key, value in compound_names.items()}\n",
    "# Replace the names with its according numbers\n",
    "tire_deg_data[\"CompoundName\"] = tire_deg_data[\"CompoundName\"].replace(compound_names_inv)\n",
    "\n",
    "# We can eliminate this column as it does not provide any information\n",
    "tire_deg_data = tire_deg_data.drop('Unnamed: 0', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making correlation matrix\n",
    "correlation_matrix = tire_deg_data.corr()\n",
    "\n",
    "\n",
    "# Crear un heatmap\n",
    "plt.figure(figsize=(24,12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclussions and Variable Cleaning\n",
    "\n",
    "**Variables to Keep**  \n",
    "- **FuelAdjustedLapTime** (remove `LapTime`, correlation 0.95)  \n",
    "  - Main lap time metric adjusted without the misleading fuel effect  \n",
    "- **FuelAdjustedDegPercent** (remove other degradation metrics)  \n",
    "  - Best metric for comparing compounds  \n",
    "  - The four degradation metrics have very high correlations with each other (0.98â€“1.00). The pther 3 variables were only created to add more explicability to the data analysis.\n",
    "- **DegradationRate**  \n",
    "  - Captures the changing dynamics of degradation  \n",
    "  - Its low correlation with other variables confirms it adds unique information  \n",
    "  - Crucial for detecting inflection points and sudden changes  \n",
    "- **TyreAge**  \n",
    "  - A fundamental variable for the model  \n",
    "  - Tire age is the main predictor of its condition  \n",
    "- **CompoundID** (remove `CompoundName`)  \n",
    "  - Needed to distinguish between different compounds  \n",
    "\n",
    "- **Rest of variables**\n",
    "\n",
    "**Optional Variables (if they improve the model)**  \n",
    "- **SpeedI1**, **SpeedI2**, **SpeedFL**: to capture sector effects  \n",
    "- **FuelLoad**: as a control variable  \n",
    "\n",
    "**Variables to Remove**  \n",
    "- **LapTime** (use only `FuelAdjustedLapTime`)  \n",
    "- **TireDegAbsolute**, **TireDegPercent**, **FuelAdjustedDegAbsolute**  \n",
    "- **CompoundName** (redundant with `CompoundID`)  \n",
    "- **LapsFromBaseline** and **FuelEffect** (perfect correlation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove only the specified redundant variables\n",
    "columns_to_remove = [\n",
    "    'LapTime',                 # Use only FuelAdjustedLapTime\n",
    "    'TireDegAbsolute',         # Redundant with FuelAdjustedDegPercent\n",
    "    'TireDegPercent',          # Redundant with FuelAdjustedDegPercent\n",
    "    'FuelAdjustedDegAbsolute', # Redundant with FuelAdjustedDegPercent\n",
    "    'CompoundName',            # Redundant with CompoundID\n",
    "    'LapsFromBaseline',        # Perfect correlation\n",
    "    'FuelEffect'               # Perfect correlation\n",
    "]\n",
    "\n",
    "# Drop columns from the dataframe\n",
    "tire_deg_data = tire_deg_data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Show how many variables were removed and display a new correlation matrix\n",
    "print(f\"Removed {len(columns_to_remove)} redundant variables.\")\n",
    "print(f\"The dataframe now has {tire_deg_data.shape[1]} columns.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. New Correlation Matrix to see interesting Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the new correlation matrix\n",
    "updated_correlation_matrix = tire_deg_data.corr()\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.heatmap(updated_correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Updated Correlation Matrix after Removing Redundant Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing the data and applying fuel adjustment, the correlation matrix reveals critical relationships that will inform our LSTM model. Here are the main points:\n",
    "\n",
    "- **Key Correlations:**\n",
    "  - **FuelAdjustedLapTime & FuelAdjustedDegPercent:** Strong correlation (0.59) indicates similar degradation dynamics from different perspectives.\n",
    "  - **TyreAge & LapsSincePitStop:** Almost perfect correlation (0.90), as both track similar information.\n",
    "  - **CompoundID & FuelAdjustedDegPercent:** Moderate correlation (0.40) shows that tire compound significantly influences degradation.\n",
    "  - **FuelLoad & Performance Metrics:** Strong negative correlation with Stint (-0.86) and moderate correlation with adjusted lap time (0.42), emphasizing the importance of fuel weight.\n",
    "  - **DegradationRate:** Low correlation with most variables, including FuelAdjustedDegPercent (0.19), suggesting it captures unique degradation dynamics.\n",
    "\n",
    "- **LSTM Model Input Selection:**\n",
    "  - **Primary Variables:**\n",
    "    - **FuelAdjustedLapTime:** Core performance metric without fuel effects.\n",
    "    - **FuelAdjustedDegPercent:** Best metric for comparing compound performance.\n",
    "    - **DegradationRate:** Captures lap-to-lap changes and critical inflection points.\n",
    "    - **TyreAge:** Fundamental predictor of tire condition.\n",
    "    - **CompoundID:** Essential for distinguishing between different tire compounds.\n",
    "  - **Supporting Variables:**\n",
    "    - **Speed Metrics (SpeedI1, SpeedI2, SpeedFL):** Capture sector-specific effects.\n",
    "    - **FuelLoad:** Used as a control variable.\n",
    "    - **Position:** Provides contextual race information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Saving my final Dataframe as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Guardar el DataFrame procesado con las mÃ©tricas de degradaciÃ³n ajustadas por combustible\n",
    "output_path = \"../../outputs/week5/tire_degradation_fuel_adjusted.csv\"\n",
    "\n",
    "# Guardar el DataFrame\n",
    "tire_deg_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Creating Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../outputs/week5/tire_degradation_fuel_adjusted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencing Process for LSTM\n",
    "\n",
    "### Creating Temporal Sequences:\n",
    "- We need to transform our tabular data into chronologically ordered sequences.\n",
    "- Each sequence should contain a sliding window of N consecutive laps (typically 5 laps).\n",
    "\n",
    "### Data Format for LSTM:\n",
    "**Input:** [lap_t-5, lap_t-4, lap_t-3, lap_t-2, lap_t-1]  \n",
    "\n",
    "**Output:** [lap_t, lap_t+1, lap_t+2]\n",
    "\n",
    "### Variables to Include in Each Sequence Element:\n",
    "- FuelAdjustedLapTime\n",
    "- FuelAdjustedDegPercent\n",
    "- DegradationRate\n",
    "- TyreAge\n",
    "- CompoundID\n",
    "- Contextual variables (FuelLoad, position, sector speeds)\n",
    "\n",
    "It is crucial to ensure that the sequences maintain temporal integrity and do not mix data from different stints or pit stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, input_length=5, prediction_horizon=3, target_column='DegradationRate'):\n",
    "    \"\"\"\n",
    "    Create sequences for any sequential model (LSTM, TCN) from the tire degradation data.\n",
    "    Groups by driver, stint and compound to ensure proper sequencing.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with tire degradation data\n",
    "        input_length: Number of consecutive laps to include in input sequence\n",
    "        prediction_horizon: Number of future laps to predict\n",
    "        target_column: Column to predict\n",
    "        \n",
    "    Returns:\n",
    "        sequences: List of DataFrame sequences\n",
    "        targets: List of target arrays\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Group by DriverNumber, Stint, and CompoundID\n",
    "    groupby_columns = ['DriverNumber', 'Stint', 'CompoundID']\n",
    "    \n",
    "    # Process each driver-stint-compound group separately\n",
    "    for name, group in df.groupby(groupby_columns):\n",
    "        # Sort by TyreAge to ensure chronological order\n",
    "        sorted_group = group.sort_values('TyreAge').reset_index(drop=True)\n",
    "        \n",
    "        # Skip if we don't have enough laps for a sequence\n",
    "        if len(sorted_group) < input_length + prediction_horizon:\n",
    "            continue\n",
    "        \n",
    "        # Create sliding window sequences\n",
    "        for i in range(len(sorted_group) - input_length - prediction_horizon + 1):\n",
    "            # Get input sequence (all features)\n",
    "            seq = sorted_group.iloc[i:i+input_length]\n",
    "            \n",
    "            # Get target values (future values to predict)\n",
    "            target = sorted_group.iloc[i+input_length:i+input_length+prediction_horizon][target_column].values\n",
    "            \n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "    \n",
    "    print(f\"Created {len(sequences)} sequences of {input_length} laps each\")\n",
    "    return sequences, targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, targets = create_sequences(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Sequences and Targets\n",
    "#### What does the `create_sequences` function do?\n",
    "The function creates data in a sequential format, necessary for training LSTM models. Specifically:\n",
    "\n",
    "**Sequences:**\n",
    "- They are \"sliding windows\" of consecutive data from the same set of tires.\n",
    "- Each sequence contains data from 5 consecutive laps (all DataFrame columns).\n",
    "- They represent the \"recent history\" that the model will use to make predictions.\n",
    "\n",
    "**Targets:**\n",
    "- These are the values we want to predict in the future.\n",
    "- Each target contains the degradation values for the next 3 laps after the sequence.\n",
    "- It only includes the column we want to predict (`FuelAdjustedDegPercent`).\n",
    "\n",
    "#### Concrete Example\n",
    "Imagine we have data from 10 laps with the same tire:\n",
    "\n",
    "- **Sequence 1:** Laps 1-5\n",
    "  - **Target 1:** Degradation in laps 6-8\n",
    "- **Sequence 2:** Laps 2-6\n",
    "  - **Target 2:** Degradation in laps 7-9\n",
    "- **Sequence 3:** Laps 3-7\n",
    "  - **Target 3:** Degradation in laps 8-10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1 New Approach: Divide sequences by compound ID\n",
    "\n",
    "A good approach to try to improve the performance of our predictions is divide the data sequences by different tire compounds. In that way, I can try to develop 3 different models, each for every compound ID, alongside a global model. \n",
    "\n",
    "Like this, I can make the predictions using the specific model for that tire, compare it with the global prediction and try to do an emsemble between the two ones for improving robustness of the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide sequences by compound tire\n",
    "def split_by_compound(df, sequences, targets):\n",
    "    \"\"\"\n",
    "    Divide sequences and targets per compound ID\n",
    "    \n",
    "    \"\"\"\n",
    "    # Identify unique tire id\n",
    "    compounds = df['CompoundID'].unique()\n",
    "    \n",
    "    # Create dictionaries for storing sequences per compound\n",
    "    compound_sequences = {c: [] for c in compounds}\n",
    "    compound_targets = {c: [] for c in compounds}\n",
    "    \n",
    "    # Assign each sequence to the corresponend dictionary\n",
    "    for i, seq in enumerate(sequences):\n",
    "        compound = seq['CompoundID'].iloc[0]\n",
    "        compound_sequences[compound].append(seq)\n",
    "        compound_targets[compound].append(targets[i])\n",
    "    \n",
    "    # Informar sobre la distribuciÃ³n de secuencias\n",
    "    for compound in compounds:\n",
    "        compound_name = compound_names.get(compound, f\"Compound {compound}\")\n",
    "        print(f\"{compound_name}: {len(compound_sequences[compound])} sequences\")\n",
    "    \n",
    "    return compound_sequences, compound_targets, compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data per compound\n",
    "compound_sequences, compound_targets, unique_compounds = split_by_compound(df, sequences, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Verifying the Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_sequences_with_targets(sequences, targets, num_to_check=3):\n",
    "    print(\"COMPLETE SEQUENCE VERIFICATION (WITH TARGETS):\")\n",
    "    print(\"=============================================\")\n",
    "    \n",
    "    # Check a few consecutive sequences from the same group\n",
    "    driver_stint_compounds = []\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        # Get identifier for this sequence\n",
    "        identifier = (seq['DriverNumber'].iloc[0], seq['Stint'].iloc[0], seq['CompoundID'].iloc[0])\n",
    "        driver_stint_compounds.append(identifier)\n",
    "    \n",
    "    # Find groups with consecutive sequences\n",
    "    for i in range(len(sequences)-1):\n",
    "        # Check if consecutive sequences are from same driver-stint-compound\n",
    "        if driver_stint_compounds[i] == driver_stint_compounds[i+1]:\n",
    "            seq1 = sequences[i]\n",
    "            seq2 = sequences[i+1]\n",
    "            \n",
    "            # Get tire ages and targets\n",
    "            ages1 = seq1['TyreAge'].values\n",
    "            ages2 = seq2['TyreAge'].values\n",
    "            target1 = targets[i]\n",
    "            target2 = targets[i+1]\n",
    "            \n",
    "            # Calculate what the next tire ages should be (for targets)\n",
    "            expected_target_ages1 = np.array([ages1[-1] + j + 1 for j in range(len(target1))])\n",
    "            expected_target_ages2 = np.array([ages2[-1] + j + 1 for j in range(len(target2))])\n",
    "            \n",
    "            # Check if sliding window pattern is correct\n",
    "            sliding_window_correct = np.array_equal(ages1[1:], ages2[:-1])\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\nSequences {i} and {i+1}:\")\n",
    "            print(f\"Driver: {seq1['DriverNumber'].iloc[0]}, Stint: {seq1['Stint'].iloc[0]}, Compound: {seq1['CompoundID'].iloc[0]}\")\n",
    "            print(f\"Tire ages seq {i}: {ages1}\")\n",
    "            print(f\"TARGET values seq {i}: {target1}\")\n",
    "            print(f\"Expected target ages seq {i}: {expected_target_ages1}\")\n",
    "            print(f\"Tire ages seq {i+1}: {ages2}\")\n",
    "            print(f\"TARGET values seq {i+1}: {target2}\")\n",
    "            print(f\"Expected target ages seq {i+1}: {expected_target_ages2}\")\n",
    "            print(f\"Sliding window pattern: {sliding_window_correct}\")\n",
    "            \n",
    "            # Verify that target1 corresponds to the next values after seq1\n",
    "            # We'd need the original dataframe to check this precisely\n",
    "            \n",
    "            # Only check a limited number\n",
    "            num_to_check -= 1\n",
    "            if num_to_check <= 0:\n",
    "                break\n",
    "    \n",
    "    print(\"\\nVERIFICATION SUMMARY:\")\n",
    "    print(\"1. Each sequence should advance by one lap (sliding window pattern)\")\n",
    "    print(\"2. Targets should contain the FuelAdjustedDegPercent values for the next 3 laps\")\n",
    "    print(\"3. Each target should start exactly where its sequence ends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verirfy sequences with their targets\n",
    "verify_sequences_with_targets(sequences, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of Data Sequencing\n",
    "#### Sliding Window Pattern:\n",
    "- Each sequence advances exactly one lap relative to the previous one.\n",
    "- Example: `[1,2,3,4,5] â†’ [2,3,4,5,6] â†’ [3,4,5,6,7] â†’ [4,5,6,7,8]`\n",
    "\n",
    "#### Consistency in Targets:\n",
    "- Targets always represent the next 3 laps after each sequence.\n",
    "- Example: Sequence 0 ends at lap 5, targets are laps 6, 7, 8.\n",
    "\n",
    "#### Coherence Between Sequences and Targets:\n",
    "- The target values also \"slide\" accordingly:\n",
    "  - **Target of Sequence 0:** `[-3.88, -3.73, -4.36]`\n",
    "  - **Target of Sequence 1:** `[-3.73, -4.36, -3.83]`\n",
    "- The first two values of Target 1 match the last two of Target 0.\n",
    "\n",
    "#### Maintaining Structure by Driver, Stint, and Compound:\n",
    "- All shown sequences belong to the same driver (1), same stint (1.0), and same compound (2).\n",
    "- This ensures we are analyzing the degradation of a single set of tires.\n",
    "\n",
    "### Implications for Our LSTM Model\n",
    "- The model can learn degradation patterns based on consecutive lap sequences.\n",
    "- It can predict degradation for the next 3 laps using the previous 5 laps.\n",
    "- The data structure captures both the absolute degradation level and its rate of change.\n",
    "\n",
    "With this verification, we confirm that the data is correctly prepared for training the LSTM model. We have selected `FuelAdjustedDegPercent` as our target, which is appropriate since it represents degradation adjusted for fuel effects, precisely what we aim to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 LSTM: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_lstm(sequences, targets):\n",
    "    \"\"\"\n",
    "    Convert the list of DataFrames and targets into numpy arrays suitable for LSTM training\n",
    "    \"\"\"\n",
    "    # Get the number of features (columns) in the sequence DataFrames\n",
    "    n_features = len(sequences[0].columns)\n",
    "    sequence_length = len(sequences[0])\n",
    "    prediction_horizon = len(targets[0])\n",
    "    \n",
    "    # Initialize arrays\n",
    "    X = np.zeros((len(sequences), sequence_length, n_features))\n",
    "    y = np.zeros((len(sequences), prediction_horizon))\n",
    "    \n",
    "    # Fill the arrays\n",
    "    for i, (seq, target) in enumerate(zip(sequences, targets)):\n",
    "        X[i] = seq.values\n",
    "        y[i] = target\n",
    "    \n",
    "    print(f\"Prepared data for LSTM with shape: X: {X.shape}, y: {y.shape}\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM (convert to numpy arrays)\n",
    "X, y = prepare_for_lstm(sequences, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1 **Prepared Data:**\n",
    "- **X**: (763, 5, 16) â†’ 763 sequences, each containing 5 laps and 16 features per lap\n",
    "- **y**: (763, 3) â†’ For each sequence, we predict 3 future laps of degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2 **Dataset Split:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation and test sets (70-15-15)\n",
    "# First split: separate test set (15%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: divide remaining data into train (70%) and validation (15%)\n",
    "# The validation should be 17.65% of the temporary set (0.15/0.85)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.15/0.85, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the resulting datasets\n",
    "print(\"Data split:\")\n",
    "print(f\"X_train shape: {X_train.shape} ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"X_val shape: {X_val.shape} ({len(X_val)/len(X):.1%})\")\n",
    "print(f\"X_test shape: {X_test.shape} ({len(X_test)/len(X):.1%})\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Train**: 533 sequences (69.9%) - very close to the 70% target\n",
    "- **Validation**: 115 sequences (15.1%) - very close to the 15% target\n",
    "- **Test**: 115 sequences (15.1%) - very close to the 15% target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Pytorch Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.1 Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "# Validation\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "# Test\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.2 Creating Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.3 Creating Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size= BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.3 Prepare Data by Compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_compound_data(compound_sequences, compound_targets):\n",
    "    \"\"\"\n",
    "    \n",
    "    Prepare data of each compound for the model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    compound_data = {}\n",
    "    \n",
    "    for compound in unique_compounds:\n",
    "        if len(compound_sequences[compound]) > 0:\n",
    "            # Convert sequences to numpy format\n",
    "            X, y = prepare_for_lstm(compound_sequences[compound], compound_targets[compound])\n",
    "            compound_data[compound] = (X, y)\n",
    "            \n",
    "            print(f\"Compound {compound_names.get(compound, compound)}: X shape {X.shape}, y shape {y.shape}\")\n",
    "    \n",
    "    return compound_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare specific data for each tire\n",
    "compound_data = prepare_compound_data(compound_sequences, compound_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. LSTM Architecture: Changed to Temporal Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TireDegradationLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size, dropout = 0.3):\n",
    "#         super(TireDegradationLSTM, self).__init__()\n",
    "\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "\n",
    "#         # LSTM Layer for Capturing sequential patterns\n",
    "#         # RUN 2 : Bidirectional lstm for camputiring more patterns\n",
    "\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size= input_size,\n",
    "#             hidden_size= hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first= True,\n",
    "#             dropout= dropout if num_layers > 1 else 0 ,\n",
    "#             bidirectional= True # With this parameter it becomes bidirecional\n",
    "\n",
    "#         )\n",
    "\n",
    "#         # Layer normalization for stabilizing training\n",
    "#         self.layer_norm1 = nn.LayerNorm(hidden_size * 2)  # *2 because it is bidirectional\n",
    "        \n",
    "#         # RUN 2: deeper dense layer system with redidual conections\n",
    "#         self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "#         self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "#         self.layer_norm3 = nn.LayerNorm(hidden_size // 2)\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "#         self.fc3 = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "#         # Activations\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         # x shape must be (batch_size, sequence_length, input_size)\n",
    "\n",
    "#         # LSTM output\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "\n",
    "#         # Use only last time step output\n",
    "#         lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "#         # RUN 2 Normalization and First FC layer\n",
    "#         lstm_out = self.layer_norm1(lstm_out)\n",
    "\n",
    "#         # First dense layer with normalization and residual\n",
    "#         out = self.fc1(lstm_out)\n",
    "#         out = self.layer_norm2(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.dropout1(out)\n",
    "\n",
    "#         # Second dense layer\n",
    "#         residual = out  # We store it for residual conection\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.layer_norm3(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.dropout2(out)\n",
    "\n",
    "#         # Output layer\n",
    "\n",
    "#         out = self.fc3(out)\n",
    "        \n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=64, dropout=0.3):\n",
    "        super(EnhancedTCN, self).__init__()\n",
    "        \n",
    "        # 1. Increase capacity and add regularization\n",
    "        # Project the input to a higher-dimensional space using a 1D convolution.\n",
    "        self.input_proj = nn.Conv1d(input_size, hidden_size, kernel_size=1)\n",
    "        # Batch normalization on the projected input.\n",
    "        self.bn_input = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # 2. Multi-Scale Block with Exponential Dilations\n",
    "        # Residual layers with different dilation rates to capture multi-scale patterns.\n",
    "        # First dilated convolution with dilation=1.\n",
    "        self.dilated_conv1 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding='same', dilation=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        # Second dilated convolution with dilation=2.\n",
    "        self.dilated_conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding='same', dilation=2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        # Third dilated convolution with dilation=4.\n",
    "        self.dilated_conv4 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding='same', dilation=4)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        # Fourth dilated convolution with dilation=8.\n",
    "        self.dilated_conv8 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding='same', dilation=8)\n",
    "        self.bn8 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # 3. Simple Temporal Attention Mechanism\n",
    "        # Use a 1D convolution to compute attention scores over the sequence.\n",
    "        self.attention = nn.Conv1d(hidden_size, 1, kernel_size=1)\n",
    "        # Softmax is applied along the time dimension (seq_len).\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        # 4. Output Layer with Higher Dropout for Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Fully connected layers for final prediction.\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, features]\n",
    "        # Transpose the input to [batch, features, seq_len] for Conv1d operations.\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Initial projection: apply convolution, batch norm and ReLU activation.\n",
    "        x = F.relu(self.bn_input(self.input_proj(x)))\n",
    "        \n",
    "        # Apply dilated convolutions with residual connections.\n",
    "        # First residual block with dilation=1.\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.dilated_conv1(x)))\n",
    "        x = x + residual  # Residual connection\n",
    "        \n",
    "        # Second residual block with dilation=2.\n",
    "        residual = x\n",
    "        x = F.relu(self.bn2(self.dilated_conv2(x)))\n",
    "        x = x + residual  # Residual connection\n",
    "        \n",
    "        # Third residual block with dilation=4.\n",
    "        residual = x\n",
    "        x = F.relu(self.bn4(self.dilated_conv4(x)))\n",
    "        x = x + residual  # Residual connection\n",
    "        \n",
    "        # Fourth residual block with dilation=8.\n",
    "        residual = x\n",
    "        x = F.relu(self.bn8(self.dilated_conv8(x)))\n",
    "        x = x + residual  # Residual connection\n",
    "        \n",
    "        # Apply the attention mechanism.\n",
    "        # Compute attention weights for each time step.\n",
    "        attn_weights = self.softmax(self.attention(x))  # Shape: [batch, 1, seq_len]\n",
    "        # Multiply feature maps with the attention weights.\n",
    "        x = x * attn_weights\n",
    "        \n",
    "        # Global pooling: sum over the time dimension.\n",
    "        x = torch.sum(x, dim=2)  # Resulting shape: [batch, channels]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through fully connected layers with dropout and ReLU activations.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.1. Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################ HYPERPARAMETER DEFINITION #######################################\n",
    "\n",
    "# ### RUN 2 \n",
    "# input_size = 16 # Number of features\n",
    "# hidden_size = 128  # Hidden layer size: from 64 to 128\n",
    "# num_layers = 3 # Number of LSTM layers: from 2 to 3\n",
    "# output_size = 3 # Number of future laps to predict\n",
    "# learning_rate = 0.0005 # Reduced from 0.001 for more stable training\n",
    "# weight_decay = 1e-5  # Added l2 regularization\n",
    "\n",
    "\n",
    "# num_epochs = 150 # From 100, more for allowing convergence with new architecture\n",
    "# patience = 20 # Apply early stopping. If no improvements after 15 epochs, stop training. Augmented to 20 for more convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### TCN HYPERPARAMETERS ##################################\n",
    "input_size = 16\n",
    "output_size = 3\n",
    "learning_rate = 0.0001  # Lower learning rate\n",
    "weight_decay = 1e-6\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "patience = 25\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.1.1. Hyperparameters for specific compound models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter training cicles for specific models\n",
    "compound_epochs = 100  \n",
    "compound_patience = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.2 Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### LSTM ###################################\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")          #\n",
    "# model = TireDegradationLSTM(input_size, hidden_size, num_layers, output_size)  #\n",
    "# model.to(device)                                                               #\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### TCN ######################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EnhancedTCN(input_size, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.3 Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# criterion = nn.MSELoss()\n",
    "# # RUN 2: added L2 regularization\n",
    "# optimizer = optim.Adam(model.parameters(), lr= learning_rate, weight_decay= weight_decay)\n",
    "\n",
    "# #RUN 2 : learning rate programmer\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TCN #####################################\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.0005,  #  Initial higher learning rate\n",
    "    weight_decay=1e-4  # Stronger regularization\n",
    ")\n",
    "# Scheduler with cicled warming\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, \n",
    "    T_0=5,  # Initial 5 epoch cicling\n",
    "    T_mult=2,  # Duplicate cicle in each restart\n",
    "    eta_min=1e-6  # Minimum learning rate value\n",
    ")\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1 Training Function for a single Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.1.1 LSTM Train Epoch Commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### LSTM ####################################\n",
    "# def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "#\n",
    "#         # Forward pass\n",
    "#\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#\n",
    "#         # Backpropagation and Optimization\n",
    "#\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#\n",
    "#        \n",
    "#\n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#\n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#     return epoch_loss\n",
    "####################################################################################\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.1.2 Train Epoch for TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backpropagation y optimizaciÃ³n\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # AÃ±adir gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1 Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    total_loss = running_loss / len(data_loader.dataset)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Training with Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19.1 LSTM Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_val_loss = float(\"inf\")\n",
    "# counter = 0 # Counter for epochs without improvement\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "\n",
    "# print(\"Starting Training...\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "#     val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "\n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "\n",
    "#     # Check early stopping condition\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         counter = 0\n",
    "#         # Store best model\n",
    "\n",
    "#         torch.save(model.state_dict(), \"../../outputs/week5/models/tire_degradation_lstm.pth\")\n",
    "#         print(f\" Saving model (val_loss improved: {best_val_loss:.6f})\")\n",
    "\n",
    "#     else:\n",
    "#         counter += 1\n",
    "#         if counter >= patience:\n",
    "#             print(f\"Early stopping on epoch {epoch + 1}\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19.2 TCN Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop with early stopping and proper model saving\n",
    "\n",
    "# Initialize best validation loss as infinity for tracking improvement\n",
    "best_val_loss = float('inf')\n",
    "counter = 0  # Counter for early stopping\n",
    "train_losses = []  # Store training loss per epoch\n",
    "val_losses = []  # Store validation loss per epoch\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch and get the training loss\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    # Evaluate on the validation set and get the validation loss\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Store losses for later analysis\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduler updates the LR based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print training progress\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "    # Early stopping and model saving logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss  # Update best validation loss\n",
    "        counter = 0  # Reset early stopping counter\n",
    "        # Save the model in a DIFFERENT file than the LSTM model\n",
    "        torch.save(model.state_dict(), '../../outputs/week5/models/tire_degradation_tcn.pth')\n",
    "        print(f\"Saving model (improved val_loss: {best_val_loss:.6f})\")\n",
    "    else:\n",
    "        counter += 1  # Increment early stopping counter\n",
    "        if counter >= patience:  # If no improvement for 'patience' epochs\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break  # Stop training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.3 Training models by Compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19.a: Function to train models for each compound\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def train_compound_model(compound_id, X, y, device):\n",
    "    \"\"\"Trains a specific model for a given compound\"\"\"\n",
    "    \n",
    "    # Split data into train, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "    \n",
    "    # Dataloaders\n",
    "    compound_train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    compound_val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    compound_test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model\n",
    "    compound_model = EnhancedTCN(input_size, output_size)\n",
    "    compound_model.to(device)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        compound_model.parameters(), \n",
    "        lr=0.0005,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=5,  # Initial restart period (5 epochs)\n",
    "        T_mult=2,  # Multiplier for increasing restart periods\n",
    "        eta_min=1e-6  # Minimum learning rate\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss function\n",
    "    \n",
    "    # Training with early stopping\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss as infinity\n",
    "    counter = 0  # Early stopping counter\n",
    "    train_losses = []  # Store training losses per epoch\n",
    "    val_losses = []  # Store validation losses per epoch\n",
    "    \n",
    "    for epoch in range(compound_epochs):\n",
    "        # Train for one epoch and compute training loss\n",
    "        train_loss = train_epoch(compound_model, compound_train_loader, criterion, optimizer, device)\n",
    "        # Evaluate on validation set and compute validation loss\n",
    "        val_loss = evaluate(compound_model, compound_val_loader, criterion, device)\n",
    "        \n",
    "        # Store loss values\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{compound_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "        \n",
    "        # Early stopping and model saving logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss  # Update best validation loss\n",
    "            counter = 0  # Reset early stopping counter\n",
    "            # Save the model with a unique filename per compound\n",
    "            torch.save(compound_model.state_dict(), f'../../outputs/week5/models/tcn_compound_{compound_id}.pth')\n",
    "            print(f\"Saving model (improved val_loss: {best_val_loss:.6f})\")\n",
    "        else:\n",
    "            counter += 1  # Increment early stopping counter\n",
    "            if counter >= compound_patience:  # If no improvement for 'compound_patience' epochs\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break  # Stop training early\n",
    "    \n",
    "    # Load the best model version before testing\n",
    "    compound_model.load_state_dict(torch.load(f'../../outputs/week5/models/tcn_compound_{compound_id}.pth'))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss = evaluate(compound_model, compound_test_loader, criterion, device)\n",
    "    \n",
    "    # Model evaluation metrics\n",
    "    compound_model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in compound_test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = compound_model(inputs)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            actuals.append(targets.numpy())\n",
    "    \n",
    "    if len(predictions) > 0:\n",
    "        # Convert lists into arrays for metric calculation\n",
    "        predictions = np.vstack(predictions)\n",
    "        actuals = np.vstack(actuals)\n",
    "        \n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        \n",
    "        metrics = {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        }\n",
    "    else:\n",
    "        # Handle case where no predictions are generated\n",
    "        metrics = {'mse': float('nan'), 'rmse': float('nan'), 'mae': float('nan')}\n",
    "    \n",
    "    return compound_model, metrics, (train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop per Compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialized_models = {}\n",
    "compound_performance = {}\n",
    "compound_loss_curves = {}\n",
    "\n",
    "for compound in compound_data:\n",
    "    compound_name = compound_names.get(compound, f\"Compound {compound}\")\n",
    "    print(f\"\\n=== Training model for {compound_name} ===\")\n",
    "    \n",
    "    X, y = compound_data[compound]\n",
    "    \n",
    "    # Only train if enough data\n",
    "    if len(X) > 10:  \n",
    "        model, metrics, loss_curves = train_compound_model(compound, X, y, device)\n",
    "        \n",
    "        specialized_models[compound] = model\n",
    "        compound_performance[compound] = metrics\n",
    "        compound_loss_curves[compound] = loss_curves\n",
    "    else:\n",
    "        print(f\"Not enough data for {compound_name} tire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain Combined Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learning curves for plotting\n",
    "train_losses_combined = []\n",
    "val_losses_combined = []\n",
    "\n",
    "# Determine the maximum length among all learning curves\n",
    "max_length = 0\n",
    "for compound in compound_loss_curves:\n",
    "    train_curve, val_curve = compound_loss_curves[compound]\n",
    "    max_length = max(max_length, len(train_curve))\n",
    "\n",
    "# Compute the average of the learning curves\n",
    "for i in range(max_length):\n",
    "    train_sum = 0\n",
    "    val_sum = 0\n",
    "    count = 0\n",
    "    \n",
    "    for compound in compound_loss_curves:\n",
    "        train_curve, val_curve = compound_loss_curves[compound]\n",
    "        if i < len(train_curve):\n",
    "            train_sum += train_curve[i]\n",
    "            val_sum += val_curve[i]\n",
    "            count += 1\n",
    "    \n",
    "    if count > 0:\n",
    "        train_losses_combined.append(train_sum / count)\n",
    "        val_losses_combined.append(val_sum / count)\n",
    "\n",
    "# Use these combined curves for plotting\n",
    "train_losses = train_losses_combined\n",
    "val_losses = val_losses_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"../../outputs/week5/models/tire_degradation_lstm.pth\"))\n",
    "model.load_state_dict(torch.load('../../outputs/week5/models/tire_degradation_tcn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Evaluating on test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making predictions on test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        actuals.append(targets.numpy())\n",
    "\n",
    "predictions = np.vstack(predictions)\n",
    "actuals = np.vstack(actuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(actuals, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test MSE: {mse:.6f}\")\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "print(f\"Test MAE: {mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.1 Losses in training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('../../outputs/week5/training_validation_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.2 Predictions vs Real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Select some samples for visualizing\n",
    "sample_indices = np.random.choice(len(predictions), size=5, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    horizon = range(1, 4)  # 3 future laps\n",
    "    \n",
    "    plt.plot(horizon, actuals[idx], 'o-', label='Actual', color='blue')\n",
    "    plt.plot(horizon, predictions[idx], 'o--', label='Predicted', color='red')\n",
    "    \n",
    "    plt.title(f'Sample {idx}')\n",
    "    plt.xlabel('Future Lap')\n",
    "    plt.ylabel('Degradation (s/lap)')\n",
    "    plt.grid(True)\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/week5/predictions_vs_actual.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.3 Performance Comparison by Compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display table header for metrics per compound\n",
    "print(\"\\nPerformance by Compound:\")\n",
    "# Format the header with columns for Compound, MSE, RMSE, and MAE\n",
    "print(\"{:<15} {:<12} {:<12} {:<12}\".format(\"Compound\", \"MSE\", \"RMSE\", \"MAE\"))\n",
    "print(\"-\" * 55)  # Print a separator line\n",
    "\n",
    "# Create a dictionary for the global metrics from previously computed values\n",
    "global_metrics = {\"mse\": mse, \"rmse\": rmse, \"mae\": mae}\n",
    "\n",
    "# Loop through each compound and its corresponding performance metrics\n",
    "for compound, metrics in compound_performance.items():\n",
    "    # Retrieve the compound name from the dictionary, or default to \"Compound <id>\"\n",
    "    compound_name = compound_names.get(compound, f\"Compound {compound}\")\n",
    "    \n",
    "    # Print the metrics for the current compound, formatted with 6 decimal places\n",
    "    print(\"{:<15} {:<12.6f} {:<12.6f} {:<12.6f}\".format(\n",
    "        compound_name, \n",
    "        metrics['mse'], \n",
    "        metrics['rmse'], \n",
    "        metrics['mae']\n",
    "    ))\n",
    "\n",
    "# Add the global model metrics for comparison, using the same formatting\n",
    "print(\"{:<15} {:<12.6f} {:<12.6f} {:<12.6f}\".format(\n",
    "    \"GLOBAL MODEL\", \n",
    "    global_metrics['mse'], \n",
    "    global_metrics['rmse'], \n",
    "    global_metrics['mae']\n",
    "))\n",
    "\n",
    "# Create a bar chart to visually compare the RMSE of each compound model against the global model\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Extract the compound identifiers from the performance dictionary keys\n",
    "compounds = list(compound_performance.keys())\n",
    "# Create a list of RMSE values for each compound from the performance metrics\n",
    "rmse_values = [metrics['rmse'] for metrics in compound_performance.values()]\n",
    "# Create a list of compound labels using the compound_names dictionary, defaulting if not found\n",
    "compound_labels = [compound_names.get(c, f\"Compound {c}\") for c in compounds]\n",
    "\n",
    "# Define colors for each compound using a dictionary, defaulting to 'gray' if the compound is not specified\n",
    "colors = [compound_colors.get(c, 'gray') for c in compounds]\n",
    "\n",
    "# Plot a bar chart where each bar corresponds to the RMSE value of a compound model\n",
    "plt.bar(compound_labels, rmse_values, color=colors)\n",
    "\n",
    "# Draw a horizontal dashed red line representing the global model's RMSE for easy comparison\n",
    "plt.axhline(y=rmse, color='red', linestyle='--', label=f'Global Model (RMSE={rmse:.3f})')\n",
    "\n",
    "# Label the y-axis with a description and the measurement units (seconds/round)\n",
    "plt.ylabel('RMSE (seconds/round)')\n",
    "# Set the title of the chart to describe the comparison being made\n",
    "plt.title('RMSE Comparison by Tire Type')\n",
    "# Add a legend to differentiate between the global model and compound-specific models\n",
    "plt.legend()\n",
    "# Enable grid lines for the y-axis to improve readability\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Save the figure to the specified output path\n",
    "plt.savefig('../../outputs/week5/compound_specialized_performance.png')\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.4 Visualizations of Predictions vs Actual Values by Compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_compound_predictions(compound_id, compound_model, X_test, y_test, device):\n",
    "    \"\"\"\n",
    "    Visualizes predictions versus actual values for a specific compound.\n",
    "    \"\"\"\n",
    "    # Filter test data indices corresponding to this compound.\n",
    "    compound_test_indices = []\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        # Check if the current sequence belongs to the given compound using the 'CompoundID'\n",
    "        if sequence['CompoundID'].iloc[0] == compound_id:\n",
    "            compound_test_indices.append(i)\n",
    "    \n",
    "    # Convert the test data to a PyTorch tensor and send it to the specified device (CPU or GPU).\n",
    "    X_compound = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    # Set the model to evaluation mode and compute predictions without tracking gradients.\n",
    "    compound_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = compound_model(X_compound).cpu().numpy()\n",
    "    \n",
    "    # y_actual remains the provided ground truth values for comparison.\n",
    "    y_actual = y_test\n",
    "    \n",
    "    # Determine the number of samples to visualize (up to 5 samples, or less if fewer predictions exist)\n",
    "    n_samples = min(5, len(y_pred))\n",
    "    if n_samples > 0:\n",
    "        # Randomly select sample indices without replacement.\n",
    "        sample_indices = np.random.choice(len(y_pred), size=n_samples, replace=False)\n",
    "        \n",
    "        # Create a figure with a grid of subplots (2 rows and 3 columns) to display each sample.\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(14, 8))\n",
    "        axes = axes.flatten()  # Flatten the 2D array of axes to a 1D list for easier indexing.\n",
    "        \n",
    "        # Retrieve the compound name from the dictionary, or default to \"Compound <compound_id>\".\n",
    "        compound_name = compound_names.get(compound_id, f\"Compound {compound_id}\")\n",
    "        fig.suptitle(f'Predictions vs. Actual Values for {compound_name}', fontsize=16)\n",
    "        \n",
    "        # Plot each selected sample in a separate subplot.\n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            if i < len(axes):\n",
    "                ax = axes[i]\n",
    "                \n",
    "                # Define the forecast horizon; here, we assume predictions for 3 future laps.\n",
    "                horizon = range(1, 4)  # Future laps: 1, 2, 3\n",
    "                \n",
    "                # Plot the actual values as a blue solid line with markers.\n",
    "                ax.plot(horizon, y_actual[idx], 'o-', label='Actual', color='blue')\n",
    "                # Plot the predicted values as a red dashed line with markers.\n",
    "                ax.plot(horizon, y_pred[idx], 'o--', label='Predicted', color='red')\n",
    "                \n",
    "                # Set subplot title, labels, and enable grid lines.\n",
    "                ax.set_title(f'Sample {idx}')\n",
    "                ax.set_xlabel('Future Lap')\n",
    "                ax.set_ylabel('Degradation (s/lap)')\n",
    "                ax.grid(True)\n",
    "                \n",
    "                # Only add a legend to the first subplot.\n",
    "                if i == 0:\n",
    "                    ax.legend()\n",
    "        \n",
    "        # Turn off any unused subplots if less than the available axes.\n",
    "        for i in range(n_samples, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Adjust the layout to accommodate the figure title and subplots.\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        # Save the figure to the specified output path, including the compound name in the filename.\n",
    "        plt.savefig(f'../../outputs/week5/predictions_{compound_name}.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"There are not enough samples to visualize for compound {compound_name}\")\n",
    "\n",
    "# Visualize predictions for each compound in the specialized models.\n",
    "for compound in specialized_models:\n",
    "    # Retrieve the compound name using the compound_names dictionary.\n",
    "    compound_name = compound_names.get(compound, f\"Compound {compound}\")\n",
    "    print(f\"\\nVisualizing predictions for {compound_name}...\")\n",
    "    \n",
    "    # Filter the test data indices for this compound.\n",
    "    compound_indices_test = []\n",
    "    for i, seq in enumerate(sequences):\n",
    "        # Ensure that the index is within the bounds of the test data and matches the compound.\n",
    "        if i < len(X_test) and seq['CompoundID'].iloc[0] == compound:\n",
    "            compound_indices_test.append(i)\n",
    "    \n",
    "    # If there are test samples for this compound, proceed with visualization.\n",
    "    if len(compound_indices_test) > 0:\n",
    "        # Filter the test data (both features and labels) using the collected indices.\n",
    "        X_compound_test = X_test[compound_indices_test]\n",
    "        y_compound_test = y_test[compound_indices_test]\n",
    "        \n",
    "        # Call the visualization function with the filtered data.\n",
    "        visualize_compound_predictions(compound, specialized_models[compound], \n",
    "                                      X_compound_test, y_compound_test, device)\n",
    "    else:\n",
    "        print(f\"There is no test data for compound {compound_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.5 Compare Learning Curves by Compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))  # Set the size of the figure\n",
    "\n",
    "# Loop through each compound's loss curves data\n",
    "for compound in compound_loss_curves:\n",
    "    # Retrieve the compound name from the dictionary, defaulting to \"Compound <id>\" if not found\n",
    "    compound_name = compound_names.get(compound, f\"Compound {compound}\")\n",
    "    # Get the color for the compound; default to black if not specified\n",
    "    color = compound_colors.get(compound, 'black')\n",
    "    \n",
    "    # Each entry in compound_loss_curves is a tuple: (training loss curve, validation loss curve)\n",
    "    train_curve, val_curve = compound_loss_curves[compound]\n",
    "    \n",
    "    # Plot the training loss curve using a dashed line with some transparency\n",
    "    plt.plot(train_curve, '--', color=color, alpha=0.7, \n",
    "             label=f'{compound_name} (Train)')\n",
    "    # Plot the validation loss curve using a solid line\n",
    "    plt.plot(val_curve, '-', color=color, \n",
    "             label=f'{compound_name} (Val)')\n",
    "\n",
    "# Set the x-axis label to \"Epoch\"\n",
    "plt.xlabel('Epoch')\n",
    "# Set the y-axis label to \"Loss\"\n",
    "plt.ylabel('Loss')\n",
    "# Set the title for the plot\n",
    "plt.title('Learning Curves by Tire Type')\n",
    "# Display a legend to differentiate between compounds and data types (training vs validation)\n",
    "plt.legend()\n",
    "# Enable grid lines for better readability\n",
    "plt.grid(True)\n",
    "# Save the figure to the specified output path\n",
    "plt.savefig('../../outputs/week5/learning_curves_by_compound.png')\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Ensemble System for Combined Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(X, compound_id, global_model, specialized_models, device, \n",
    "                     weights=None):\n",
    "    \"\"\"\n",
    "    Makes predictions using an ensemble of the global model and the specialized model.\n",
    "    \n",
    "    Args:\n",
    "        X: Input data (tensor)\n",
    "        compound_id: ID of the compound for which the prediction is made\n",
    "        global_model: Global model trained on all data\n",
    "        specialized_models: Dictionary containing specialized models for each compound\n",
    "        device: Device for computation (CPU/GPU)\n",
    "        weights: Weights to combine predictions [global_weight, specialized_weight]\n",
    "                 If None, weights are calculated based on the inverse RMSE.\n",
    "    \n",
    "    Returns:\n",
    "        Ensemble predictions and the weights used for combining\n",
    "    \"\"\"\n",
    "    # Convert X to a tensor if it is not already one.\n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.FloatTensor(X)\n",
    "    \n",
    "    # Move the input tensor to the specified device (CPU/GPU)\n",
    "    X = X.to(device)\n",
    "    \n",
    "    # Get predictions from the global model.\n",
    "    # Set the model to evaluation mode and disable gradient computation.\n",
    "    global_model.eval()\n",
    "    with torch.no_grad():\n",
    "        global_pred = global_model(X).cpu().numpy()\n",
    "    \n",
    "    # Check if there is a specialized model for the given compound.\n",
    "    if compound_id in specialized_models:\n",
    "        specialized_model = specialized_models[compound_id]\n",
    "        specialized_model.eval()\n",
    "        with torch.no_grad():\n",
    "            specialized_pred = specialized_model(X).cpu().numpy()\n",
    "            \n",
    "        # If weights are not provided, calculate them based on the inverse of the RMSE.\n",
    "        if weights is None:\n",
    "            global_rmse = 0.355017  # RMSE of the global model\n",
    "            specialized_rmse = compound_performance[compound_id]['rmse']\n",
    "            \n",
    "            # Use the inverse of the RMSE as the weighting factor (better performance = higher weight)\n",
    "            global_weight = 1 / global_rmse\n",
    "            specialized_weight = 1 / specialized_rmse\n",
    "            \n",
    "            # Normalize the weights so that their sum is equal to 1.\n",
    "            total = global_weight + specialized_weight\n",
    "            global_weight /= total\n",
    "            specialized_weight /= total\n",
    "            \n",
    "            weights = [global_weight, specialized_weight]\n",
    "        \n",
    "        # Combine the predictions from the global and specialized models using the calculated weights.\n",
    "        ensemble_pred = weights[0] * global_pred + weights[1] * specialized_pred\n",
    "        \n",
    "        return ensemble_pred, weights\n",
    "    else:\n",
    "        # If no specialized model exists, return only the global prediction.\n",
    "        return global_pred, [1.0, 0.0]\n",
    "\n",
    "# Evaluate the performance of the ensemble system.\n",
    "ensemble_predictions = []      # List to store ensemble predictions for each compound.\n",
    "ensemble_weights_used = {}     # Dictionary to store average weights used for each compound.\n",
    "\n",
    "print(\"\\nEvaluating Ensemble System:\")\n",
    "test_indices_by_compound = {}  # Dictionary to group test indices by compound.\n",
    "\n",
    "# Group test indices by compound.\n",
    "for i in range(len(X_test)):\n",
    "    # Get the compound associated with the i-th test example.\n",
    "    # We need to link it with the original sequence.\n",
    "    test_sequence_idx = i % len(sequences)  # Approximation; adjust if necessary.\n",
    "    if test_sequence_idx < len(sequences):\n",
    "        compound_id = sequences[test_sequence_idx]['CompoundID'].iloc[0]\n",
    "        \n",
    "        if compound_id not in test_indices_by_compound:\n",
    "            test_indices_by_compound[compound_id] = []\n",
    "        \n",
    "        test_indices_by_compound[compound_id].append(i)\n",
    "\n",
    "# Evaluate the ensemble for each compound.\n",
    "for compound_id, indices in test_indices_by_compound.items():\n",
    "    # Retrieve compound name from dictionary; default if not found.\n",
    "    compound_name = compound_names.get(compound_id, f\"Compound {compound_id}\")\n",
    "    # Filter the test data (features and labels) for the current compound.\n",
    "    X_compound = X_test[indices]\n",
    "    y_compound = y_test[indices]\n",
    "    \n",
    "    compound_predictions = []  # List to store predictions for this compound.\n",
    "    weights_sum = [0, 0]       # To accumulate the weights used over all samples.\n",
    "    \n",
    "    # Iterate through each test sample for the current compound.\n",
    "    for i, x in enumerate(X_compound):\n",
    "        # Reshape the sample to have a batch dimension of 1.\n",
    "        x_tensor = torch.FloatTensor(x.reshape(1, x.shape[0], x.shape[1]))\n",
    "        # Get the ensemble prediction and the weights used for this sample.\n",
    "        pred, weights = ensemble_predict(x_tensor, compound_id, model, \n",
    "                                         specialized_models, device)\n",
    "        compound_predictions.append(pred[0])\n",
    "        # Sum the weights for averaging later.\n",
    "        weights_sum[0] += weights[0]\n",
    "        weights_sum[1] += weights[1]\n",
    "    \n",
    "    # Calculate average weights used across all samples for the compound.\n",
    "    if len(X_compound) > 0:\n",
    "        avg_weights = [w / len(X_compound) for w in weights_sum]\n",
    "        ensemble_weights_used[compound_id] = avg_weights\n",
    "    \n",
    "    # Convert the predictions list to a NumPy array.\n",
    "    compound_predictions = np.array(compound_predictions)\n",
    "    \n",
    "    # Calculate evaluation metrics: Mean Squared Error, RMSE, and Mean Absolute Error.\n",
    "    mse = mean_squared_error(y_compound, compound_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_compound, compound_predictions)\n",
    "    \n",
    "    # Retrieve the RMSE of the global and specialized models.\n",
    "    global_rmse = 0.355017\n",
    "    specialized_rmse = compound_performance.get(compound_id, {'rmse': float('inf')})['rmse']\n",
    "    \n",
    "    # Calculate the percentage improvement of the ensemble over the global and specialized models.\n",
    "    improvement_over_global = (global_rmse - rmse) / global_rmse * 100\n",
    "    improvement_over_specialized = (specialized_rmse - rmse) / specialized_rmse * 100\n",
    "    \n",
    "    # Print performance metrics for the current compound.\n",
    "    print(f\"\\n{compound_name}:\")\n",
    "    print(f\"  Ensemble RMSE: {rmse:.6f}\")\n",
    "    print(f\"  Global RMSE: {global_rmse:.6f} ({improvement_over_global:.2f}% improvement)\")\n",
    "    print(f\"  Specialized RMSE: {specialized_rmse:.6f} ({improvement_over_specialized:.2f}% improvement)\")\n",
    "    print(f\"  Weights used: Global={avg_weights[0]:.2f}, Specialized={avg_weights[1]:.2f}\")\n",
    "    \n",
    "    # Save predictions for visualization.\n",
    "    ensemble_predictions.append((compound_id, compound_predictions, y_compound))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ensemble_predictions(ensemble_results):\n",
    "    \"\"\"\n",
    "    Visualizes the ensemble model's predictions against the actual values.\n",
    "    \n",
    "    Args:\n",
    "        ensemble_results: List of tuples (compound_id, predictions, actuals)\n",
    "    \"\"\"\n",
    "    # Iterate over each compound's ensemble results\n",
    "    for compound_id, predictions, actuals in ensemble_results:\n",
    "        # Retrieve the compound name from the dictionary, or default if not found\n",
    "        compound_name = compound_names.get(compound_id, f\"Compound {compound_id}\")\n",
    "        \n",
    "        # Select up to 5 samples to visualize\n",
    "        n_samples = min(5, len(predictions))\n",
    "        if n_samples == 0:\n",
    "            continue  # Skip if there are no samples\n",
    "        \n",
    "        # Randomly choose sample indices without replacement\n",
    "        sample_indices = np.random.choice(len(predictions), size=n_samples, replace=False)\n",
    "        \n",
    "        # Create a figure with a grid of subplots (2 rows x 3 columns)\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(14, 8))\n",
    "        axes = axes.flatten()  # Flatten to simplify indexing\n",
    "        \n",
    "        # Set the overall title for the figure\n",
    "        fig.suptitle(f'Ensemble Predictions vs. Actual Values for {compound_name}', fontsize=16)\n",
    "        \n",
    "        # Loop over each selected sample index to plot\n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            if i < len(axes):\n",
    "                ax = axes[i]\n",
    "                \n",
    "                # Define the forecast horizon (here, 3 future laps)\n",
    "                horizon = range(1, 4)\n",
    "                \n",
    "                # Prepare the input sample for prediction by reshaping and sending to device\n",
    "                # Assuming each sample has a shape that should be reshaped to (1, 5, 16)\n",
    "                x_tensor = torch.FloatTensor(X_test[idx].reshape(1, 5, 16)).to(device)\n",
    "                \n",
    "                # Obtain the global model prediction for the sample\n",
    "                with torch.no_grad():\n",
    "                    global_pred = model(x_tensor).cpu().numpy()[0]\n",
    "                \n",
    "                # Check if there is a specialized model for this compound; if so, use it\n",
    "                if compound_id in specialized_models:\n",
    "                    specialized_model = specialized_models[compound_id]\n",
    "                    with torch.no_grad():\n",
    "                        specialized_pred = specialized_model(x_tensor).cpu().numpy()[0]\n",
    "                else:\n",
    "                    specialized_pred = global_pred  # Fallback to global prediction\n",
    "                \n",
    "                # Plot the actual values as a blue line with markers\n",
    "                ax.plot(horizon, actuals[idx], 'o-', label='Actual', color='blue')\n",
    "                # Plot the ensemble prediction as a purple dashed line with markers\n",
    "                ax.plot(horizon, predictions[idx], 'o--', label='Ensemble', color='purple')\n",
    "                # Plot the global model prediction as a red dashed line with a different marker, semi-transparent\n",
    "                ax.plot(horizon, global_pred, 'x--', label='Global', color='red', alpha=0.4)\n",
    "                # Plot the specialized model prediction as a green dashed line with a different marker, semi-transparent\n",
    "                ax.plot(horizon, specialized_pred, '*--', label='Specialized', color='green', alpha=0.4)\n",
    "                \n",
    "                # Set subplot title and axis labels\n",
    "                ax.set_title(f'Sample {idx}')\n",
    "                ax.set_xlabel('Future Lap')\n",
    "                ax.set_ylabel('Degradation (s/lap)')\n",
    "                ax.grid(True)\n",
    "                \n",
    "                # Only add a legend in the first subplot for clarity\n",
    "                if i == 0:\n",
    "                    ax.legend()\n",
    "        \n",
    "        # Hide any unused subplots in the grid\n",
    "        for i in range(n_samples, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Adjust layout to ensure the overall title does not overlap with subplots\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        # Save the figure to the specified file path\n",
    "        plt.savefig(f'../../outputs/week5/ensemble_predictions_{compound_name}.png')\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "# Visualize ensemble predictions using the results from the ensemble evaluation\n",
    "visualize_ensemble_predictions(ensemble_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26. Making prediction function\n",
    "\n",
    "For being able to use all these models fast and as part of a bigger system, it is necessary to implement a big function that uses all the steps defined in this notebook. \n",
    "\n",
    "Then, I will implement a new notebook, called `N02_model_tire_predictions`, where I will make this. This is important for the future to convert this notebook to a Python module that I will call in the logic agent for the rules related with degradation where the models will need to predict based on the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
