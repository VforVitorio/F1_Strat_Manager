{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e033e6",
   "metadata": {},
   "source": [
    "# Notebook for making a Module for making predictions of any data \n",
    "\n",
    "This notebook is intented to implement, through all the code designed in `N01_tire_prediction.ipynb`, a function that we could use in any other notebook for making fast predictions with our tire models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361db311",
   "metadata": {},
   "source": [
    "## 1. Importing neccesary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2815d09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt  # Optional, for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b470117f",
   "metadata": {},
   "source": [
    "## 2. Redefining the modelÂ´s class.\n",
    "\n",
    "This step is important for loading the models, as `.pth` files do not save the model instantiation. This can be good as we can have more options for making further changes in the code, but I will just re-implement the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bfe8a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TCN model architecture class\n",
    "class EnhancedTCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=64, dropout=0.3):\n",
    "        super(EnhancedTCN, self).__init__()\n",
    "        \n",
    "        # Input projection to higher-dimensional space\n",
    "        self.input_proj = nn.Conv1d(input_size, hidden_size, kernel_size=1)\n",
    "        self.bn_input = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Multi-scale block with exponential dilations\n",
    "        self.dilated_conv1 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding='same', dilation=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dilated_conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding='same', dilation=2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dilated_conv4 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding='same', dilation=4)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dilated_conv8 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding='same', dilation=8)\n",
    "        self.bn8 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Simple temporal attention mechanism\n",
    "        self.attention = nn.Conv1d(hidden_size, 1, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        # Output layer with higher dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, features]\n",
    "        x = x.transpose(1, 2)  # Transform to [batch, features, seq_len] for Conv1d\n",
    "        \n",
    "        # Initial projection\n",
    "        x = F.relu(self.bn_input(self.input_proj(x)))\n",
    "        \n",
    "        # Apply dilated convolutions with residual connections\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.dilated_conv1(x)))\n",
    "        x = x + residual  # Residual connection\n",
    "        \n",
    "        residual = x\n",
    "        x = F.relu(self.bn2(self.dilated_conv2(x)))\n",
    "        x = x + residual  # Residual connection\n",
    "        \n",
    "        residual = x\n",
    "        x = F.relu(self.bn4(self.dilated_conv4(x)))\n",
    "        x = x + residual  # Residual connection\n",
    "        \n",
    "        residual = x\n",
    "        x = F.relu(self.bn8(self.dilated_conv8(x)))\n",
    "        x = x + residual  # Residual connection\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_weights = self.softmax(self.attention(x))  # Shape: [batch, 1, seq_len]\n",
    "        x = x * attn_weights\n",
    "        \n",
    "        # Global pooling: sum over time dimension\n",
    "        x = torch.sum(x, dim=2)  # Shape: [batch, channels]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ca1c9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fb76f",
   "metadata": {},
   "source": [
    "## 3. Load and Validate Data\n",
    "\n",
    "\n",
    "In this function we ensure that the model is going to receive the data disposed in the way it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861ae7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_data(input_data):\n",
    "    \"\"\"\n",
    "    Loads and validates input data, ensuring it contains all necessary columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_data: Can be a DataFrame or a path to a CSV file\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with validated data\n",
    "    \n",
    "    Raises:\n",
    "    - ValueError: If required columns are missing or contain null values\n",
    "    \"\"\"\n",
    "    # Load data - handle both DataFrame and file path inputs\n",
    "    if isinstance(input_data, str):\n",
    "        # If input is a string, assume it's a file path and read the CSV\n",
    "        df = pd.read_csv(input_data)\n",
    "    else:\n",
    "        # If input is already a DataFrame, make a copy to avoid modifying the original\n",
    "        df = input_data.copy()\n",
    "    \n",
    "    # Define required columns for the model\n",
    "    # These are the essential columns needed for prediction\n",
    "    required_columns = [\n",
    "        'LapTime',       # Time taken to complete the lap\n",
    "        'Stint',         # Current stint number (resets after pit stop)\n",
    "        'CompoundID',    # ID of the tire compound (1=Soft, 2=Medium, 3=Hard)\n",
    "        'TyreAge',       # Number of laps the current set of tires has completed\n",
    "        'FuelLoad',      # Estimated fuel load in kg\n",
    "        'DriverNumber',  # Driver's race number\n",
    "        'Position',      # Current race position\n",
    "        'SpeedI1',       # Speed at first intermediate point\n",
    "        'SpeedI2',       # Speed at second intermediate point\n",
    "        'SpeedFL'        # Speed at flying lap point\n",
    "    ]\n",
    "    \n",
    "    # Verify all required columns are present in the DataFrame\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        # If any required columns are missing, raise an error with details\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Check for null values in required columns\n",
    "    null_counts = df[required_columns].isnull().sum()\n",
    "    if null_counts.sum() > 0:\n",
    "        # If null values are found, print a warning with details\n",
    "        print(f\"Warning: Null values found in data:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        \n",
    "        # For TyreAge and LapTime, we cannot have null values as they are critical\n",
    "        if df['TyreAge'].isnull().any() or df['LapTime'].isnull().any():\n",
    "            raise ValueError(\"Cannot have null values in 'TyreAge' or 'LapTime'\")\n",
    "    \n",
    "    # Ensure data types are correct for model processing\n",
    "    # Convert categorical IDs to integers\n",
    "    df['CompoundID'] = df['CompoundID'].astype(int)\n",
    "    df['DriverNumber'] = df['DriverNumber'].astype(int)\n",
    "    \n",
    "    # Ensure numeric columns are properly typed\n",
    "    numeric_columns = ['LapTime', 'TyreAge', 'FuelLoad', 'SpeedI1', 'SpeedI2', 'SpeedFL']\n",
    "    for col in numeric_columns:\n",
    "        # Convert to numeric, coercing errors (invalid values become NaN)\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Log successful validation\n",
    "    print(f\"Data loaded and validated: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    # Return the validated DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d44e6",
   "metadata": {},
   "source": [
    "## 4. Calculate Degradation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fd100",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_degradation_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculates degradation metrics adjusted for the fuel effect.\n",
    "    \n",
    "    This function processes lap data to compute tire degradation metrics after removing\n",
    "    the effect of fuel burn, which naturally makes cars faster as the race progresses.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with lap data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with added degradation metrics:\n",
    "        - FuelAdjustedLapTime: Lap time with fuel effect removed\n",
    "        - FuelAdjustedDegPercent: Percentage degradation compared to baseline\n",
    "        - DegradationRate: Rate of degradation (seconds per lap)\n",
    "    \"\"\"\n",
    "    # Constant for lap time improvement due to fuel reduction\n",
    "    # Based on empirical F1 data: cars improve ~0.055s per lap due to fuel burning\n",
    "    LAP_TIME_IMPROVEMENT_PER_LAP = 0.055  # seconds per lap\n",
    "    \n",
    "    # Create a copy of the DataFrame to store results\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Define mapping of compound names for logging\n",
    "    compound_names = {\n",
    "        0: 'Unknown',\n",
    "        1: 'Soft',\n",
    "        2: 'Medium',\n",
    "        3: 'Hard'\n",
    "    }\n",
    "    \n",
    "    # Process each tire compound separately\n",
    "    for compound_id in df['CompoundID'].unique():\n",
    "        compound_name = compound_names.get(compound_id, f\"Unknown ({compound_id})\")\n",
    "        print(f\"Processing {compound_name} tires (ID: {compound_id})...\")\n",
    "        \n",
    "        # Filter data for this compound\n",
    "        compound_data = df[df['CompoundID'] == compound_id].copy()\n",
    "        \n",
    "        # Check if we have enough data for meaningful analysis\n",
    "        if len(compound_data) < 5:\n",
    "            print(f\"  Not enough data for {compound_name} tires, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Establish baseline information\n",
    "        # Ideally use new tires (TyreAge=1) as baseline, or minimum available age\n",
    "        if 1 in compound_data['TyreAge'].values:\n",
    "            # Get baseline from new tires (TyreAge=1)\n",
    "            baseline_data = compound_data[compound_data['TyreAge'] == 1]\n",
    "            baseline_lap_time = baseline_data['LapTime'].mean()\n",
    "            baseline_tire_age = 1\n",
    "        else:\n",
    "            # If no data with new tires, use minimum available age\n",
    "            min_age = compound_data['TyreAge'].min()\n",
    "            baseline_data = compound_data[compound_data['TyreAge'] == min_age]\n",
    "            baseline_lap_time = baseline_data['LapTime'].mean()\n",
    "            baseline_tire_age = min_age\n",
    "            print(f\"  No laps with new tires for {compound_name}, using TyreAge={min_age} as baseline\")\n",
    "        \n",
    "        # Calculate fuel adjustment based on laps from baseline\n",
    "        # Each lap burned fuel makes the car ~0.055s faster per lap\n",
    "        compound_data['LapsFromBaseline'] = compound_data['TyreAge'] - baseline_tire_age\n",
    "        compound_data['FuelEffect'] = compound_data['LapsFromBaseline'] * LAP_TIME_IMPROVEMENT_PER_LAP\n",
    "        \n",
    "        # Calculate fuel-adjusted lap time\n",
    "        # Add fuel effect back to compensate for the artificial improvement\n",
    "        compound_data['FuelAdjustedLapTime'] = compound_data['LapTime'] + compound_data['FuelEffect']\n",
    "        \n",
    "        # Calculate degradation percentage (compared to baseline)\n",
    "        # This shows how much slower (in %) the car is compared to baseline performance\n",
    "        baseline_adjusted_lap_time = baseline_lap_time  # For new tires, no adjustment needed\n",
    "        compound_data['FuelAdjustedDegPercent'] = (compound_data['FuelAdjustedLapTime'] / baseline_adjusted_lap_time - 1) * 100\n",
    "        \n",
    "        # Add calculated metrics to the result DataFrame\n",
    "        for col in ['LapsFromBaseline', 'FuelEffect', 'FuelAdjustedLapTime', 'FuelAdjustedDegPercent']:\n",
    "            idx = compound_data.index\n",
    "            result_df.loc[idx, col] = compound_data[col]\n",
    "    \n",
    "    # Calculate degradation rate (lap-to-lap changes in performance)\n",
    "    # This requires grouping by driver, stint, and compound\n",
    "    groupby_columns = ['DriverNumber', 'Stint', 'CompoundID']\n",
    "    \n",
    "    # Initialize degradation rate column\n",
    "    result_df['DegradationRate'] = 0\n",
    "    \n",
    "    # Process each driver-stint-compound group separately\n",
    "    for name, group in result_df.groupby(groupby_columns):\n",
    "        # Sort by TyreAge to ensure correct calculation\n",
    "        group = group.sort_values('TyreAge')\n",
    "        \n",
    "        # Calculate differences between consecutive laps\n",
    "        if len(group) > 1:\n",
    "            group_idx = group.index\n",
    "            lap_times = group['FuelAdjustedLapTime'].values\n",
    "            \n",
    "            # Calculate lap time differences (how much slower each lap is than the previous)\n",
    "            # Using np.diff with prepend to keep array length consistent\n",
    "            diffs = np.diff(lap_times, prepend=lap_times[0])\n",
    "            # First value is invalid (self-comparison), set to 0\n",
    "            diffs[0] = 0\n",
    "            \n",
    "            # Assign to the result DataFrame\n",
    "            result_df.loc[group_idx, 'DegradationRate'] = diffs\n",
    "    \n",
    "    # Fill any remaining null values in DegradationRate\n",
    "    # New tires (first lap of a stint) have no degradation rate yet\n",
    "    result_df['DegradationRate'] = result_df['DegradationRate'].fillna(0)\n",
    "    \n",
    "    print(\"Degradation metrics successfully calculated\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2accef65",
   "metadata": {},
   "source": [
    "## 5. Variable Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae46369",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de572ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_redundant_variables(df):\n",
    "    \"\"\"\n",
    "    Ensures data format matches exactly what the model was trained on.\n",
    "    \n",
    "    Our model expects exactly 16 features in a specific order.\n",
    "    This function ensures we provide data in the expected format.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with degradation metrics calculated\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with exact 16 features needed for model\n",
    "    \"\"\"\n",
    "    # Check which columns are available\n",
    "    available_columns = set(df.columns)\n",
    "    \n",
    "    # Define required columns in the exact order expected by the model\n",
    "    # These must match exactly what was used during training\n",
    "    required_columns = [\n",
    "        # Core metrics\n",
    "        'FuelAdjustedLapTime',\n",
    "        'FuelAdjustedDegPercent',\n",
    "        'DegradationRate',\n",
    "        'TyreAge',\n",
    "        'CompoundID',\n",
    "        \n",
    "        # Speed metrics\n",
    "        'SpeedI1', \n",
    "        'SpeedI2', \n",
    "        'SpeedFL',\n",
    "        'SpeedST',  # This was missing in our original list\n",
    "        \n",
    "        # Race context\n",
    "        'Position',\n",
    "        'FuelLoad',\n",
    "        'DriverNumber',\n",
    "        'Stint',\n",
    "        'LapsSincePitStop',  # This was missing\n",
    "        'DRSUsed',           # This was missing\n",
    "        'TeamID'             # This was missing\n",
    "    ]\n",
    "    \n",
    "    # Ensure we have all required columns available\n",
    "    missing_columns = [col for col in required_columns if col not in available_columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: Missing columns required by model: {missing_columns}\")\n",
    "        print(\"Adding these columns with default values.\")\n",
    "        \n",
    "        # Add missing columns with sensible defaults\n",
    "        for col in missing_columns:\n",
    "            if col == 'SpeedST':\n",
    "                # Use average of other speeds\n",
    "                df[col] = df[['SpeedI1', 'SpeedI2', 'SpeedFL']].mean(axis=1)\n",
    "            elif col == 'DRSUsed':\n",
    "                # Default to 0 (not used)\n",
    "                df[col] = 0\n",
    "            elif col == 'TeamID':\n",
    "                # Default to 0 \n",
    "                df[col] = 0\n",
    "            elif col == 'LapsSincePitStop':\n",
    "                # Set to same as TyreAge\n",
    "                df[col] = df['TyreAge']\n",
    "            else:\n",
    "                # Default to 0 for any other missing column\n",
    "                df[col] = 0\n",
    "    \n",
    "    # Create DataFrame with exactly the required columns in the correct order\n",
    "    cleaned_df = df[required_columns].copy()\n",
    "    \n",
    "    print(f\"Processed data format: {cleaned_df.shape[1]} features\")\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55031fd",
   "metadata": {},
   "source": [
    "## 6. Create sequences for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c870fb",
   "metadata": {},
   "source": [
    "### Strategic Tire Degradation Monitoring\n",
    "\n",
    "#### Why Monitor Tire Degradation from Specific Lap Thresholds?\n",
    "\n",
    "Formula 1 tire performance follows a predictable pattern throughout its lifecycle:\n",
    "\n",
    "1. **Initial Phase (First Few Laps)**: \n",
    "   - Tires are in their optimal window with minimal degradation\n",
    "   - Performance is relatively stable and predictable\n",
    "   - Strategic decisions rarely needed during this phase\n",
    "\n",
    "2. **Critical Monitoring Phase**:\n",
    "   - Begins when tires start showing meaningful degradation patterns\n",
    "   - Different compounds reach this phase at different times:\n",
    "     - **Soft Compounds**: ~6 laps (faster degradation)\n",
    "     - **Medium Compounds**: ~12 laps (moderate degradation)\n",
    "     - **Hard Compounds**: ~25 laps (slower degradation)\n",
    "   - This is when predictive models become most valuable for strategy decisions\n",
    "\n",
    "3. **End-of-Life Phase**:\n",
    "   - Severe performance drop-off (\"cliff\")\n",
    "   - Critical for pit stop timing decisions\n",
    "\n",
    "By focusing our monitoring on the compound-specific critical phases, we:\n",
    "- Reduce noise from initial break-in laps\n",
    "- Focus computational resources on strategically relevant predictions\n",
    "- Improve model accuracy by training on more consistent degradation patterns\n",
    "- Better align predictions with real-world strategic decision points\n",
    "\n",
    "This approach mirrors how F1 teams monitor tires during races, where they typically start considering tire strategy once compounds reach their respective monitoring thresholds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce3c7ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278cf441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_for_prediction(df, input_length=5, compound_start_laps=None):\n",
    "    \"\"\"\n",
    "    Creates sequences for prediction starting from specific lap thresholds based on tire compound.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Clean DataFrame with relevant variables\n",
    "    - input_length: Number of consecutive laps to include in each sequence (default: 5)\n",
    "    - compound_start_laps: Dictionary mapping compound IDs to starting lap numbers\n",
    "                          Example: {1: 6, 2: 12, 3: 25} for monitoring soft tires from lap 6,\n",
    "                          medium from lap 12, and hard from lap 25\n",
    "    \n",
    "    Returns:\n",
    "    - sequences: List of DataFrames, each containing a sequence of consecutive laps\n",
    "    - metadata: List of dictionaries with metadata for each sequence\n",
    "    \"\"\"\n",
    "    # Initialize lists to store sequences and their metadata\n",
    "    sequences = []\n",
    "    metadata = []\n",
    "    \n",
    "    # Default thresholds if none provided\n",
    "    if compound_start_laps is None:\n",
    "        compound_start_laps = {1: 1, 2: 1, 3: 1}  # Default: monitor all from lap 1\n",
    "    \n",
    "    # Group data by driver, stint, and compound\n",
    "    groupby_columns = ['DriverNumber', 'Stint', 'CompoundID']\n",
    "    \n",
    "    # Process each group separately\n",
    "    for name, group in df.groupby(groupby_columns):\n",
    "        # Unpack the group identifier\n",
    "        driver, stint, compound = name\n",
    "        \n",
    "        # Get the starting lap threshold for this compound\n",
    "        min_tyre_age = compound_start_laps.get(compound, 1)\n",
    "        \n",
    "        # Sort by TyreAge to ensure chronological order\n",
    "        sorted_group = group.sort_values('TyreAge').reset_index(drop=True)\n",
    "        \n",
    "        # Filter group to only include laps at or after the starting threshold\n",
    "        filtered_group = sorted_group[sorted_group['TyreAge'] >= min_tyre_age]\n",
    "        \n",
    "        # Skip if we don't have enough laps for a complete sequence\n",
    "        if len(filtered_group) < input_length:\n",
    "            continue\n",
    "        \n",
    "        # Create sliding window sequences\n",
    "        for i in range(len(filtered_group) - input_length + 1):\n",
    "            # Extract sequence of 'input_length' consecutive laps\n",
    "            seq = filtered_group.iloc[i:i+input_length]\n",
    "            \n",
    "            # Add sequence to the list\n",
    "            sequences.append(seq)\n",
    "            \n",
    "            # Store metadata\n",
    "            meta = {\n",
    "                'DriverNumber': driver,\n",
    "                'Stint': stint,\n",
    "                'CompoundID': compound,\n",
    "                'StartLap': seq['TyreAge'].iloc[0],\n",
    "                'EndLap': seq['TyreAge'].iloc[-1],\n",
    "                'LatestLapTime': seq['FuelAdjustedLapTime'].iloc[-1]\n",
    "            }\n",
    "            metadata.append(meta)\n",
    "    \n",
    "    # Log information about created sequences\n",
    "    print(f\"Created {len(sequences)} sequences of {input_length} laps each\")\n",
    "    print(f\"Sequences by compound: {pd.Series([m['CompoundID'] for m in metadata]).value_counts().to_dict()}\")\n",
    "    \n",
    "    return sequences, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb159e",
   "metadata": {},
   "source": [
    "## 7. Loading the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6f1ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(models_path):\n",
    "    \"\"\"\n",
    "    Loads pre-trained models (global and compound-specialized).\n",
    "    \n",
    "    This function loads the global TCN model that was trained on all data, as well as\n",
    "    any available specialized models that were trained on specific tire compounds.\n",
    "    \n",
    "    Parameters:\n",
    "    - models_path: Path where trained models are stored\n",
    "    \n",
    "    Returns:\n",
    "    - global_model: Trained TCN model for all compounds\n",
    "    - specialized_models: Dictionary mapping compound IDs to specialized models\n",
    "    \n",
    "    Raises:\n",
    "    - FileNotFoundError: If global model cannot be found\n",
    "    - ImportError: If PyTorch is not available\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Check if the models directory exists\n",
    "    if not os.path.exists(models_path):\n",
    "        raise ValueError(f\"Models path does not exist: {models_path}\")\n",
    "    \n",
    "    # Define model architecture parameters (must match training configuration)\n",
    "    input_size = 16  # Number of features in input sequence\n",
    "    output_size = 3  # Number of future laps to predict\n",
    "    \n",
    "    # Determine computation device (GPU if available, otherwise CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize global model with correct architecture\n",
    "    global_model = EnhancedTCN(input_size, output_size)\n",
    "    \n",
    "    # Path to global model weights\n",
    "    global_model_path = os.path.join(models_path, 'tire_degradation_tcn.pth')\n",
    "    \n",
    "    # Load global model weights\n",
    "    if os.path.exists(global_model_path):\n",
    "        global_model.load_state_dict(torch.load(global_model_path, map_location=device))\n",
    "        global_model.to(device)  # Move model to the correct device\n",
    "        global_model.eval()      # Set model to evaluation mode\n",
    "        print(f\"Global model loaded from: {global_model_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Global model not found at: {global_model_path}\")\n",
    "    \n",
    "    # Initialize dictionary to store specialized models\n",
    "    specialized_models = {}\n",
    "    \n",
    "    # Try to load specialized models for each compound\n",
    "    # We only consider compounds 1-3 (Soft, Medium, Hard)\n",
    "    compound_ids = [1, 2, 3]  # Soft, Medium, Hard\n",
    "    \n",
    "    for compound_id in compound_ids:\n",
    "        # Path to specialized model for this compound\n",
    "        model_path = os.path.join(models_path, f'tcn_compound_{compound_id}.pth')\n",
    "        \n",
    "        # If specialized model exists, load it\n",
    "        if os.path.exists(model_path):\n",
    "            # Initialize model with the same architecture\n",
    "            specialized_model = EnhancedTCN(input_size, output_size)\n",
    "            \n",
    "            # Load weights and move to device\n",
    "            specialized_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            specialized_model.to(device)\n",
    "            specialized_model.eval()\n",
    "            \n",
    "            # Add to our dictionary of specialized models\n",
    "            specialized_models[compound_id] = specialized_model\n",
    "            print(f\"Specialized model for compound {compound_id} loaded\")\n",
    "    \n",
    "    # Summarize what was loaded\n",
    "    print(f\"Models loaded: 1 global model and {len(specialized_models)} specialized models\")\n",
    "    \n",
    "    return global_model, specialized_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a33ac5",
   "metadata": {},
   "source": [
    "## 8. Preparing sequences for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a0486",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce23eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_for_model(sequences):\n",
    "    \"\"\"\n",
    "    Converts DataFrame sequences into PyTorch tensors suitable for the TCN model.\n",
    "    \n",
    "    This function takes a list of DataFrame sequences and converts them into a\n",
    "    single tensor with shape [num_sequences, sequence_length, num_features].\n",
    "    \n",
    "    Parameters:\n",
    "    - sequences: List of DataFrames, each containing a sequence of consecutive laps\n",
    "    \n",
    "    Returns:\n",
    "    - PyTorch tensor with shape [num_sequences, sequence_length, num_features]\n",
    "    \n",
    "    Raises:\n",
    "    - ValueError: If the sequences list is empty\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Check if we have sequences to process\n",
    "    if not sequences:\n",
    "        raise ValueError(\"No sequences to process\")\n",
    "    \n",
    "    # Get dimensions from the first sequence\n",
    "    n_features = len(sequences[0].columns)  # Number of features (columns)\n",
    "    sequence_length = len(sequences[0])     # Length of each sequence (rows)\n",
    "    \n",
    "    # Initialize a numpy array to hold all sequences\n",
    "    # Shape: [num_sequences, sequence_length, num_features]\n",
    "    X = np.zeros((len(sequences), sequence_length, n_features))\n",
    "    \n",
    "    # Fill the array with data from each sequence\n",
    "    for i, seq in enumerate(sequences):\n",
    "        X[i] = seq.values\n",
    "    \n",
    "    # Convert numpy array to PyTorch tensor\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    \n",
    "    print(f\"Prepared tensor for model: shape={X_tensor.shape}\")\n",
    "    return X_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766b0e5",
   "metadata": {},
   "source": [
    "## 9. Making the ensemble predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed0631",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932863a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ensemble_predictions(sequences, metadata, global_model, specialized_models, device=None):\n",
    "    \"\"\"\n",
    "    Makes predictions using an ensemble of global and compound-specialized models.\n",
    "    \n",
    "    This function:\n",
    "    1. Converts sequences to tensor format\n",
    "    2. Obtains predictions from the global model\n",
    "    3. For sequences where a specialized model exists, gets specialized predictions\n",
    "    4. Combines predictions using a weighted average based on model performance\n",
    "    \n",
    "    Parameters:\n",
    "    - sequences: List of DataFrames, each containing a sequence of consecutive laps\n",
    "    - metadata: List of dictionaries with metadata for each sequence\n",
    "    - global_model: Trained TCN model for all compounds\n",
    "    - specialized_models: Dictionary mapping compound IDs to specialized models\n",
    "    - device: Computing device (CPU/GPU); if None, will be auto-detected\n",
    "    \n",
    "    Returns:\n",
    "    - List of dictionaries containing predictions and metadata for each sequence\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    # Determine device if not provided\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Convert sequences to tensor format suitable for the model\n",
    "    X = prepare_sequences_for_model(sequences)\n",
    "    X = X.to(device)\n",
    "    \n",
    "    # Reference RMSE values for each model\n",
    "    # These values come from model evaluation in the notebook\n",
    "    global_rmse = 0.355017  # RMSE of the global model\n",
    "    compound_rmse = {\n",
    "        1: 0.334325,  # Soft\n",
    "        2: 0.392661,  # Medium\n",
    "        3: 0.295417   # Hard\n",
    "    }\n",
    "    \n",
    "    # Initialize list to store all prediction results\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Get predictions from the global model for all sequences at once\n",
    "    global_model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        global_preds = global_model(X).cpu().numpy()\n",
    "    \n",
    "    # Process each sequence individually to apply the appropriate ensemble weights\n",
    "    for i, meta in enumerate(metadata):\n",
    "        compound_id = meta['CompoundID']\n",
    "        \n",
    "        # Initialize result dictionary with global prediction\n",
    "        result = {\n",
    "            'metadata': meta,\n",
    "            'global_prediction': global_preds[i],\n",
    "            'global_weight': 1.0,\n",
    "            'specialized_prediction': None,\n",
    "            'specialized_weight': 0.0,\n",
    "            'ensemble_prediction': global_preds[i]  # Default to global prediction\n",
    "        }\n",
    "        \n",
    "        # If a specialized model exists for this compound, get its prediction\n",
    "        if compound_id in specialized_models:\n",
    "            specialized_model = specialized_models[compound_id]\n",
    "            specialized_model.eval()\n",
    "            \n",
    "            # Extract single sequence with batch dimension preserved\n",
    "            sequence_tensor = X[i:i+1]\n",
    "            \n",
    "            # Get prediction from specialized model\n",
    "            with torch.no_grad():\n",
    "                specialized_pred = specialized_model(sequence_tensor).cpu().numpy()[0]\n",
    "            \n",
    "            # Calculate ensemble weights based on inverse RMSE\n",
    "            # Lower RMSE = higher weight (better model gets more influence)\n",
    "            specialized_rmse = compound_rmse.get(compound_id, global_rmse)\n",
    "            global_weight = 1 / global_rmse\n",
    "            specialized_weight = 1 / specialized_rmse\n",
    "            \n",
    "            # Normalize weights to sum to 1\n",
    "            total_weight = global_weight + specialized_weight\n",
    "            global_weight /= total_weight\n",
    "            specialized_weight /= total_weight\n",
    "            \n",
    "            # Weighted combination of predictions\n",
    "            ensemble_pred = (global_weight * global_preds[i] + \n",
    "                             specialized_weight * specialized_pred)\n",
    "            \n",
    "            # Update result with specialized model info\n",
    "            result['specialized_prediction'] = specialized_pred\n",
    "            result['specialized_weight'] = specialized_weight\n",
    "            result['global_weight'] = global_weight\n",
    "            result['ensemble_prediction'] = ensemble_pred\n",
    "        \n",
    "        # Add to complete results list\n",
    "        all_predictions.append(result)\n",
    "    \n",
    "    print(f\"Generated ensemble predictions for {len(all_predictions)} sequences\")\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb5e93",
   "metadata": {},
   "source": [
    "## 10. Prediction formating "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc72f82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe90660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions(predictions, original_df):\n",
    "    \"\"\"\n",
    "    Formats raw ensemble predictions into a structured, analysis-ready DataFrame\n",
    "    with simplified columns and controlled decimal precision.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: List of dictionaries with ensemble predictions and metadata\n",
    "    - original_df: Original DataFrame for reference information\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with formatted predictions for each future lap\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Define mapping of compound IDs to names for better readability\n",
    "    compound_names = {\n",
    "        1: 'Soft',\n",
    "        2: 'Medium',\n",
    "        3: 'Hard'\n",
    "    }\n",
    "    \n",
    "    # Initialize list to store formatted results\n",
    "    results = []\n",
    "    \n",
    "    # Process each prediction (one per sequence)\n",
    "    for pred in predictions:\n",
    "        # Extract metadata for this sequence\n",
    "        meta = pred['metadata']\n",
    "        # Get the ensemble prediction (array of 3 values for future laps)\n",
    "        ensemble_prediction = pred['ensemble_prediction']\n",
    "        # Get compound name for readability\n",
    "        compound_name = compound_names.get(meta['CompoundID'], f\"Unknown {meta['CompoundID']}\")\n",
    "        \n",
    "        # Create base information common to all future laps\n",
    "        base_info = {\n",
    "            'DriverNumber': meta['DriverNumber'],         # Driver identifier\n",
    "            'Stint': meta['Stint'],                       # Current stint\n",
    "            'CompoundID': meta['CompoundID'],             # Numeric compound ID\n",
    "            'CompoundName': compound_name,                # Human-readable compound name\n",
    "            'CurrentTyreAge': meta['EndLap'],             # Current age of tires\n",
    "            'CurrentLapTime': round(meta['LatestLapTime'], 3)  # Most recent lap time (3 decimals)\n",
    "        }\n",
    "        \n",
    "        # Create a row for each future lap prediction\n",
    "        for i, future_value in enumerate(ensemble_prediction):\n",
    "            # Calculate the future lap number\n",
    "            lap_number = meta['EndLap'] + i + 1\n",
    "            \n",
    "            # Create a result entry for this future lap\n",
    "            result = base_info.copy()\n",
    "            result.update({\n",
    "                'FutureLap': lap_number,                # Future lap number (TyreAge)\n",
    "                'LapsAheadPred': i + 1,                 # How many laps ahead of current\n",
    "                'PredictedDegradationRate': round(future_value, 3)  # Predicted degradation rate (3 decimals)\n",
    "            })\n",
    "            \n",
    "            # Add to results list\n",
    "            results.append(result)\n",
    "    \n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort for easier analysis\n",
    "    # First by driver, then by stint, then by future lap\n",
    "    results_df = results_df.sort_values(['DriverNumber', 'Stint', 'FutureLap'])\n",
    "    \n",
    "    # Log summary statistics\n",
    "    print(f\"Formatted results: {len(results_df)} predictions for {results_df['DriverNumber'].nunique()} drivers\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe2355d",
   "metadata": {},
   "source": [
    "# 11. The great Function: Predicting tire degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934acf7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58eacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tire_degradation(input_data, models_path='../../outputs/week5/models/', compound_start_laps=None):\n",
    "    \"\"\"\n",
    "    Complete function to predict tire degradation starting from specified tire age thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_data: DataFrame or path to CSV with lap data\n",
    "    - models_path: Path where the saved models are located\n",
    "    - compound_start_laps: Dictionary mapping compound IDs to starting lap numbers\n",
    "                          Example: {1: 6, 2: 12, 3: 25}\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with degradation predictions for the next 3 laps\n",
    "    \"\"\"\n",
    "    # Step 1: Load and validate data\n",
    "    df = load_and_validate_data(input_data)\n",
    "    \n",
    "    # Step 2: Calculate metrics\n",
    "    df = calculate_degradation_metrics(df)\n",
    "    \n",
    "    # Step 3: Clean redundant variables\n",
    "    df = clean_redundant_variables(df)\n",
    "    \n",
    "    # Step 4-5: Create sequences\n",
    "    sequences, metadata = create_sequences_for_prediction(df, compound_start_laps=compound_start_laps)\n",
    "    \n",
    "    # Step 6: Load models\n",
    "    global_model, specialized_models = load_models(models_path)\n",
    "    \n",
    "    # Step 7: Ensemble prediction\n",
    "    predictions = make_ensemble_predictions(sequences, metadata, global_model, specialized_models)\n",
    "    \n",
    "    # Step 8: Format results\n",
    "    results = format_predictions(predictions, df)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae7d91",
   "metadata": {},
   "source": [
    "## 12. Example for Calling it "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c49f8f",
   "metadata": {},
   "source": [
    "#### Disclaimer for compound start laps\n",
    "\n",
    "\n",
    "For the example usage, as the data is from the 2023 Spanish Grand Prix, I will base my starting predictions from the pitstops made in the Grand Prix.\n",
    "<p align=\"center\">\n",
    "  <img src=\"../ML_tyre_pred/ML_utils/pitstops_bar.jpg\" alt=\"Texto alternativo\" width=\"800\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28323314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to the CSV file\n",
    "csv_path = '../../outputs/week3/lap_prediction_data.csv'\n",
    "\n",
    "# Define path to models\n",
    "models_path = '../../outputs/week5/models/'\n",
    "\n",
    "\n",
    "# Define monitoring thresholds by compound\n",
    "compound_start_laps = {\n",
    "    1: 6,   # Soft tires: monitor from lap 6 onwards\n",
    "    2: 12,  # Medium tires: monitor from lap 12 onwards\n",
    "    3: 25   # Hard tires: monitor from lap 25 onwards\n",
    "}\n",
    "\n",
    "\n",
    "# Call the prediction function\n",
    "predictions = predict_tire_degradation(\n",
    "    csv_path, \n",
    "    models_path,\n",
    "    compound_start_laps=compound_start_laps\n",
    ")\n",
    "# Display the predictions\n",
    "predictions.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deddebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_real_time_predictions(csv_path, models_path, interval=5, compound_start_laps=None, max_rows=None, prediction_horizon=3):\n",
    "    \"\"\"\n",
    "    Simulates a real-time racing environment by processing a CSV row by row\n",
    "    and running the complete prediction model at regular intervals.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_path: Path to CSV with lap data\n",
    "    - models_path: Path where models are stored\n",
    "    - interval: Time interval between updates in seconds (default: 5)\n",
    "    - compound_start_laps: Lap thresholds for each compound\n",
    "    - max_rows: Maximum number of rows to process (None = all)\n",
    "    - prediction_horizon: Number of laps ahead to highlight in predictions (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with complete prediction history\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    # Read the full CSV for incremental processing\n",
    "    full_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Limit the number of rows if specified\n",
    "    if max_rows is not None:\n",
    "        full_df = full_df.iloc[:max_rows]\n",
    "    \n",
    "    # Accumulative DataFrame to simulate real-time data arrival\n",
    "    accumulated_df = pd.DataFrame(columns=full_df.columns)\n",
    "    \n",
    "    # DataFrame to store all historical predictions\n",
    "    all_predictions = pd.DataFrame()\n",
    "    \n",
    "    print(f\"Starting simulation with {len(full_df)} data rows, interval: {interval}s\")\n",
    "    \n",
    "    # Process row by row\n",
    "    for i, row in full_df.iterrows():\n",
    "        # Add the new row to the accumulative DataFrame\n",
    "        accumulated_df = pd.concat([accumulated_df, pd.DataFrame([row])], ignore_index=True)\n",
    "        \n",
    "        # Execute complete prediction with accumulated data\n",
    "        try:\n",
    "            # Use existing function with all accumulated data so far\n",
    "            latest_predictions = predict_tire_degradation(\n",
    "                accumulated_df,  # Pass DataFrame instead of CSV path\n",
    "                models_path,\n",
    "                compound_start_laps\n",
    "            )\n",
    "            \n",
    "            # Clear all previous output now that we have new predictions\n",
    "            clear_output(wait=False)\n",
    "            \n",
    "            # If there are predictions, display and save them\n",
    "            if not latest_predictions.empty:\n",
    "                # Add timestamp for tracking (without DataRowsProcessed)\n",
    "                latest_predictions['PredictionTimestamp'] = pd.Timestamp.now()\n",
    "                \n",
    "                # First show key degradation predictions\n",
    "                print(\"=== KEY DEGRADATION PREDICTIONS ===\")\n",
    "                \n",
    "                # Group by driver\n",
    "                for driver, driver_group in latest_predictions.groupby('DriverNumber'):\n",
    "                    # Get the latest stint for this driver\n",
    "                    latest_stint = driver_group['Stint'].max()\n",
    "                    stint_data = driver_group[driver_group['Stint'] == latest_stint]\n",
    "                    \n",
    "                    # Get predictions for the requested horizon\n",
    "                    near_future = stint_data[stint_data['LapsAheadPred'] <= prediction_horizon]\n",
    "                    \n",
    "                    if not near_future.empty:\n",
    "                        # Get only the unique LapsAheadPred values, sorted\n",
    "                        lap_ahead_values = sorted(near_future['LapsAheadPred'].unique())\n",
    "                        \n",
    "                        # Get the most recent CurrentTyreAge (sort by CurrentTyreAge descending and take first)\n",
    "                        most_recent_data = near_future.sort_values('CurrentTyreAge', ascending=False).iloc[0]\n",
    "                        compound = most_recent_data['CompoundName']\n",
    "                        current_age = most_recent_data['CurrentTyreAge']\n",
    "                        \n",
    "                        print(f\"\\nDriver #{driver} - {compound} tires (Current age: {current_age})\")\n",
    "                        \n",
    "                        # For each unique lap ahead value, get only the most recent prediction\n",
    "                        for lap_ahead in lap_ahead_values:\n",
    "                            # Get predictions for this specific lap ahead\n",
    "                            lap_preds = near_future[near_future['LapsAheadPred'] == lap_ahead]\n",
    "                            \n",
    "                            # Sort by current tyre age (most recent window)\n",
    "                            lap_preds = lap_preds.sort_values('CurrentTyreAge', ascending=False)\n",
    "                            \n",
    "                            # Take only the first (most recent) prediction\n",
    "                            latest_pred = lap_preds.iloc[0]\n",
    "                            \n",
    "                            print(f\"  Lap +{int(lap_ahead)}: Predicted degradation = {latest_pred['PredictedDegradationRate']:.3f} s/lap\")\n",
    "                \n",
    "                # Then show processing status\n",
    "                print(f\"\\nProcessing row {i+1} of {len(full_df)} ({(i+1)/len(full_df)*100:.1f}%)\")\n",
    "                \n",
    "                # Display DataFrame for reference\n",
    "                print(\"\\n=== PREDICTION DATAFRAME ===\")\n",
    "                display(latest_predictions.head(10))\n",
    "                \n",
    "                # Save to history\n",
    "                all_predictions = pd.concat([all_predictions, latest_predictions], ignore_index=True)\n",
    "                print(f\"Total accumulated predictions: {len(all_predictions)}\")\n",
    "            else:\n",
    "                # Clear output and show status even if no predictions\n",
    "                clear_output(wait=False)\n",
    "                print(f\"Processing row {i+1} of {len(full_df)} ({(i+1)/len(full_df)*100:.1f}%)\")\n",
    "                print(\"No predictions generated with current data (possibly insufficient sequences)\")\n",
    "        except Exception as e:\n",
    "            # Clear output and show error\n",
    "            clear_output(wait=False)\n",
    "            print(f\"Processing row {i+1} of {len(full_df)} ({(i+1)/len(full_df)*100:.1f}%)\")\n",
    "            print(f\"Error during processing: {str(e)}\")\n",
    "        \n",
    "        # Wait for the specified interval before next update\n",
    "        if i < len(full_df) - 1:  # Don't wait after the last row\n",
    "            print(f\"Waiting {interval} seconds for next update...\")\n",
    "            time.sleep(interval)\n",
    "    \n",
    "    print(\"\\n--- Simulation completed ---\")\n",
    "    print(f\"Total rows processed: {len(full_df)}\")\n",
    "    print(f\"Total predictions generated: {len(all_predictions)}\")\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds by compound\n",
    "compound_thresholds = {\n",
    "    1: 6,   # Soft tires: monitor from lap 6 onwards\n",
    "    2: 12,  # Medium tires: monitor from lap 12 onwards\n",
    "    3: 25   # Hard tires: monitor from lap 25 onwards\n",
    "}\n",
    "\n",
    "# Run simulation\n",
    "predictions_history = simulate_real_time_predictions(\n",
    "    csv_path='../../outputs/week3/lap_prediction_data.csv',\n",
    "    models_path='../../outputs/week5/models/',\n",
    "    interval=0.1,  # 5 seconds between updates\n",
    "    compound_start_laps=compound_thresholds,\n",
    "    max_rows=200,  # Optional: limit number of rows for testing\n",
    "    prediction_horizon=3  # Show predictions for next 3 laps\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
