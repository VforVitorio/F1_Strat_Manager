{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lap Time prediction for Formula 1 Spanish Grand Prix\n",
    "\n",
    "This notebook implements predictive models to estimate F1 car lap times based on various factors such as tire type, weather conditions, and track state.\n",
    "\n",
    "## Objectives\n",
    "1. Load and preprocess data from FastF1 and OpenF1  \n",
    "2. Perform feature engineering to enhance predictive capability  \n",
    "3. Include analysis of tire degradation and pit stops  \n",
    "4. Train predictive models (XGBoost and optionally a Neural Network)  \n",
    "5. Evaluate performance and visualize results  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modules Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import fastf1\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# Plot configs\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "# FastF1 config\n",
    "fastf1.Cache.enable_cache('../f1-strategy/f1_cache')  \n",
    "# Necessary dirs created if not exist\n",
    "os.makedirs('../outputs/week3', exist_ok=True)\n",
    "os.makedirs('../models/week3', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Pytorch Model\n",
    "class LapTimeNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LapTimeNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preparation\n",
    "\n",
    "We will load the data from the available Parquet files:\n",
    "\n",
    "- **laps.parquet**: Contains information about individual laps  \n",
    "- **weather.parquet**: Contains weather data  \n",
    "- **intervals.parquet**: Contains information about gaps and race states  \n",
    "- **pitstops.parquet**: Contains information about pit stops  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Load all the datasets from parquet arvhives, verifying duplicated values\n",
    "    and returning them as Dataframes\n",
    "    \"\"\"\n",
    "    # Define paths of the archives\n",
    "    laps_path = \"../f1-strategy/data/raw/Spain_2023_laps.parquet\"\n",
    "    weather_path = \"../f1-strategy/data/raw/Spain_2023_weather.parquet\"\n",
    "    intervals_path = \"../f1-strategy/data/raw/Spain_2023_openf1_intervals.parquet\"\n",
    "    pitstops_path = \"../f1-strategy/data/raw/Spain_2023_pitstops.parquet\"\n",
    "    \n",
    "    # Load Dataframes\n",
    "    laps_df = pd.read_parquet(laps_path)\n",
    "    weather_df = pd.read_parquet(weather_path)\n",
    "    intervals_df = pd.read_parquet(intervals_path)\n",
    "    pitstops_df = pd.read_parquet(pitstops_path)\n",
    "    \n",
    "    # Print info about possible duplicated values\n",
    "    print(\"Verifying possible duplicated columns between datasets\")\n",
    "    \n",
    "    # Compare columns \n",
    "    all_dfs = {\n",
    "        'laps_df': laps_df,\n",
    "        'weather_df': weather_df, \n",
    "        'intervals_df': intervals_df, \n",
    "        'pitstops_df': pitstops_df\n",
    "    }\n",
    "    \n",
    "    for name1, df1 in all_dfs.items():\n",
    "        for name2, df2 in all_dfs.items():\n",
    "            if name1 >= name2:  # Avoid duplicated comparisons\n",
    "                continue\n",
    "                \n",
    "            common_cols = set(df1.columns).intersection(set(df2.columns))\n",
    "            if common_cols:\n",
    "                print(f\"Common columns between {name1} and {name2}: {common_cols}\")\n",
    "                \n",
    "                # Verifying if columns have the same data\n",
    "                for col in common_cols:\n",
    "                    if col in df1.columns and col in df2.columns:\n",
    "                        # For verifying only if they are in both datasets and have shared values\n",
    "                        # For simplicity, only some values are compared \n",
    "                            value1 = df1[col].iloc[0] if len(df1) > 0 else None\n",
    "                            value2 = df2[col].iloc[0] if len(df2) > 0 else None\n",
    "                            print(f\"  - Column '{col}': Example value in{name1} ------ {value1}, in {name2}: {value2}\")\n",
    "        \n",
    "    \n",
    "    return laps_df, weather_df, intervals_df, pitstops_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and unpack de data in different dataframes\n",
    "laps_df, weather_df, intervals_df, pitstops_df = load_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show info about shape of the datafarmes\n",
    "\n",
    "print(f\"laps_df shape: {laps_df.shape} \\n\")\n",
    "\n",
    "print(f\"weather_df shape: {weather_df.shape} \\n\")\n",
    "\n",
    "print(f\"intervals_df shape: {intervals_df.shape} \\n\")\n",
    "\n",
    "print(f\"pitstops_df shape: {pitstops_df.shape} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    \"laps\": \"../f1-strategy/data/raw/Spain_2023_laps.parquet\",\n",
    "    \"weather\": \"../f1-strategy/data/raw/Spain_2023_weather.parquet\",\n",
    "    \"intervals\": \"../f1-strategy/data/raw/Spain_2023_openf1_intervals.parquet\",\n",
    "    \"pitstops\": \"../f1-strategy/data/raw/Spain_2023_pitstops.parquet\"\n",
    "}\n",
    "\n",
    "for name, path in paths.items():\n",
    "    print(f\"Columns of  {name}: {pd.read_parquet(path).columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial Data Exploration\n",
    "\n",
    "* First registers of all dataframes.\n",
    "* Verify that all columns are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore first registers\n",
    "print(\"First laps_df registers:\")\n",
    "display(laps_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n First weather_df register:\")\n",
    "display(weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n First pitstops_df registers:\")\n",
    "display(pitstops_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n First intervals_df registers:\")\n",
    "display(intervals_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all the columns for laps\n",
    "\n",
    "expected_laps_columns = [\n",
    "    'LapTime', 'LapNumber', 'Stint', 'PitOutTime', 'PitInTime', 'Sector1Time', \n",
    "    'Sector2Time', 'Sector3Time', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', \n",
    "    'Position', 'TyreLife', 'TrackStatus', 'IsAccurate', 'Compound', 'Driver'\n",
    "]\n",
    "\n",
    "print(\"\\nAvailable columns in laps df\")\n",
    "for col in expected_laps_columns:\n",
    "    if col in laps_df.columns:\n",
    "        dtype = pitstops_df[col].dtype\n",
    "        print(f\"✓ {col} ----- {dtype}\")\n",
    "    else:\n",
    "        print(f\"✗ {col}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all weather columns\n",
    "expected_weather_columns = [\n",
    "    'Time', 'AirTemp', 'Humidity', 'Pressure', 'Rainfall', \n",
    "    'TrackTemp', 'WindDirection', 'WindSpeed'\n",
    "]\n",
    "\n",
    "print(\"\\n Available columns in weather_df:\")\n",
    "for col in expected_weather_columns:\n",
    "    if col in weather_df.columns:\n",
    "        dtype = weather_df[col].dtype\n",
    "        print(f\"✓ {col} ----- {dtype}\")\n",
    "    else:\n",
    "        print(f\"✗ {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify pitstops columns\n",
    "expected_pitstop_columns = [\n",
    "    'Time', 'Driver', 'LapNumber', 'PitInTime', 'Compound', 'TyreLife', 'FreshTyre'\n",
    "]\n",
    "\n",
    "print(\"\\n Available columns in pitstop_df:\") \n",
    "for col in expected_pitstop_columns:\n",
    "    if col in pitstops_df.columns:\n",
    "        dtype = pitstops_df[col].dtype\n",
    "        print(f\"✓ {col} ----- {dtype}\")\n",
    "    else:\n",
    "        print(f\"✗ {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data merging and preprocessing\n",
    "\n",
    "We need to merge all the dataframes into a single one if we want to give it to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Join laps and weather dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_laps_and_weather(laps_df, weather_df):\n",
    "    \"\"\"Merge weather and laps dataframe into a single one\"\"\"\n",
    "    \n",
    "    # Eliminate Time column in weather_df to avoid conflicts\n",
    "    if 'Time' in weather_df.columns:\n",
    "        weather_df = weather_df.drop(columns=['Time'])\n",
    "    # Find a common column or another strategy if there is not a single one\n",
    "    if 'LapNumber' in weather_df.columns:\n",
    "        # If LapNumber present in both Dataframes, merging by this column\n",
    "        merged_df = pd.merge(laps_df, weather_df, on='LapNumber', how='left')\n",
    "    else:\n",
    "        # If no common column, using first time measeure for all laps\n",
    "        merged_df = laps_df.copy()\n",
    "        for col in weather_df.columns:\n",
    "            if col not in merged_df.columns:\n",
    "                # Uising the first available value\n",
    "                merged_df[col] = weather_df[col].iloc[0]\n",
    "    \n",
    "    print(f\"Result: {merged_df.shape[0]} rows, {merged_df.shape[1]} columns\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Calculate laps since last stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_laps_since_pitstop(df):\n",
    "    \"\"\"Calculate laps since last pitstop for evey driver\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Only if we have PitNextLap column\n",
    "    if 'PitNextLap' in result_df.columns:\n",
    "        # FOr each driver, find their stops\n",
    "        for driver in result_df['Driver'].unique():\n",
    "            driver_mask = result_df['Driver'] == driver\n",
    "            \n",
    "            # Identify laps with stop\n",
    "            pit_laps = result_df.loc[driver_mask & (result_df['PitNextLap'] == 1), 'LapNumber'].values\n",
    "            \n",
    "            # Calculate laps since last stop\n",
    "            for idx in result_df[driver_mask].index:\n",
    "                lap_num = result_df.loc[idx, 'LapNumber']\n",
    "                previous_pits = [p for p in pit_laps if p < lap_num]\n",
    "                \n",
    "                if previous_pits:\n",
    "                    result_df.loc[idx, 'LapsSincePitStop'] = lap_num - max(previous_pits)\n",
    "                else:\n",
    "                    # If no previous pitstop (first stint), we use LapNumber\n",
    "                    # Si no hay parada previa, usar LapNumber\n",
    "                    result_df.loc[idx, 'LapsSincePitStop'] = lap_num\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Process and add pitstop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pitstop_data(merged_df, pitstops_df):\n",
    "    \"\"\"Add pitstop data\"\"\"\n",
    "    essential_columns = ['Driver', 'LapNumber', 'Compound', 'FreshTyre']\n",
    "    pitstops_essential = pitstops_df[essential_columns].copy()\n",
    "\n",
    "    pitstops_essential = pitstops_essential.rename(columns={\n",
    "        'Compound': 'NextCompound',\n",
    "        'FreshTyre': 'FreshTyreAfterStop'\n",
    "    })\n",
    "\n",
    "    merged_result = pd.merge(\n",
    "        merged_df, \n",
    "        pitstops_essential,\n",
    "        on=['Driver', 'LapNumber'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    merged_result['PitNextLap'] = merged_result['NextCompound'].notna().astype(int)\n",
    "\n",
    "    merged_result = calculate_laps_since_pitstop(merged_result)\n",
    "\n",
    "    return merged_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(intervals_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Add interval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_interval_data(merged_df, intervals_df):\n",
    "    \"\"\"\n",
    "    Función mejorada que:\n",
    "    1. Extrae información de DRS del DataFrame intervals_df\n",
    "    2. Calcula gaps realistas basados en tiempos acumulados\n",
    "    3. Define ventanas de undercut realistas basadas en gaps entre coches\n",
    "    \n",
    "    Args:\n",
    "        merged_df: DataFrame principal con datos de vueltas\n",
    "        intervals_df: DataFrame con datos de intervalos capturados cada 4 segundos\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con características estratégicas añadidas\n",
    "    \"\"\"\n",
    "    print(\"Procesando datos de intervalos y creando características estratégicas...\")\n",
    "    \n",
    "    # Trabajar con una copia para no modificar el original\n",
    "    data = merged_df.copy()\n",
    "    \n",
    "    # 1. TRANSFERIR DRS DESDE INTERVALS_DF\n",
    "    # Verificar si podemos extraer información de DRS desde intervals_df\n",
    "    if 'drs_window' in intervals_df.columns and 'Driver' in intervals_df.columns:\n",
    "        print(\"Extrayendo información de DRS desde intervals_df...\")\n",
    "        \n",
    "        # Agregar datos de DRS por piloto y vuelta (si el piloto tuvo DRS en algún momento de la vuelta)\n",
    "        if 'LapNumber' in intervals_df.columns:\n",
    "            # Si tenemos LapNumber en intervals_df, podemos agregar directamente\n",
    "            drs_by_lap = intervals_df.groupby(['Driver', 'LapNumber'])['drs_window'].max().reset_index()\n",
    "            drs_by_lap = drs_by_lap.rename(columns={'drs_window': 'DRSUsed'})\n",
    "            \n",
    "            # Unir con el DataFrame principal\n",
    "            data = pd.merge(data, drs_by_lap, on=['Driver', 'LapNumber'], how='left')\n",
    "            \n",
    "            # Rellenar NaN con 0\n",
    "            data['DRSUsed'] = data['DRSUsed'].fillna(0).astype(int)\n",
    "        else:\n",
    "            # Si no tenemos LapNumber, tenemos que hacer una aproximación\n",
    "            print(\"ADVERTENCIA: No se encontró 'LapNumber' en intervals_df, creando DRSUsed aproximado...\")\n",
    "            \n",
    "            # Identificar líderes por vuelta\n",
    "            leaders = data['Position'] == 1\n",
    "            \n",
    "            # Para no líderes, asignar DRS con probabilidad\n",
    "            data['DRSUsed'] = 0\n",
    "            non_leaders = ~leaders\n",
    "            data.loc[non_leaders, 'DRSUsed'] = np.random.choice(\n",
    "                [0, 1], \n",
    "                size=non_leaders.sum(), \n",
    "                p=[0.3, 0.7]  # 70% probabilidad de DRS para coches que no son líderes\n",
    "            )\n",
    "    else:\n",
    "        print(\"No se encontró información de DRS en intervals_df, creando DRSUsed aproximado...\")\n",
    "        # Identificar líderes por vuelta\n",
    "        leaders = data['Position'] == 1\n",
    "        \n",
    "        # Crear columna DRSUsed\n",
    "        data['DRSUsed'] = 0\n",
    "        non_leaders = ~leaders\n",
    "        data.loc[non_leaders, 'DRSUsed'] = np.random.choice(\n",
    "            [0, 1], \n",
    "            size=non_leaders.sum(), \n",
    "            p=[0.3, 0.7]\n",
    "        )\n",
    "    \n",
    "    # 2. CALCULAR GAPS REALISTAS Y VENTANAS DE UNDERCUT\n",
    "    # Verificar que tenemos las columnas necesarias\n",
    "    required_cols = ['LapNumber', 'Driver', 'Position', 'LapTime']\n",
    "    if all(col in data.columns for col in required_cols):\n",
    "        print(\"Calculando gaps realistas basados en tiempos acumulados...\")\n",
    "        \n",
    "        # Convertir LapTime a segundos si es un timedelta\n",
    "        if pd.api.types.is_timedelta64_dtype(data['LapTime']):\n",
    "            data['LapTime'] = data['LapTime'].dt.total_seconds()\n",
    "        \n",
    "        # Calcular tiempos acumulados por piloto\n",
    "        data = data.sort_values(['Driver', 'LapNumber'])\n",
    "        data['CumulativeTime'] = data.groupby('Driver')['LapTime'].cumsum()\n",
    "        \n",
    "        # Calcular gap al líder y al coche de adelante para cada vuelta\n",
    "        lap_groups = []\n",
    "        \n",
    "        for lap_num, lap_data in data.groupby('LapNumber'):\n",
    "            # Ordenar por posición\n",
    "            lap_data = lap_data.sort_values('Position')\n",
    "            \n",
    "            # Identificar líder\n",
    "            if len(lap_data) > 0:\n",
    "                leader_time = lap_data.iloc[0]['CumulativeTime']\n",
    "                \n",
    "                # Calcular gap al líder\n",
    "                lap_data['GapToLeader'] = (lap_data['CumulativeTime'] - leader_time).round(3)\n",
    "                \n",
    "                # Calcular gap al coche delante\n",
    "                lap_data['GapToAhead'] = 0.0  # Valor default para el líder\n",
    "                \n",
    "                # Para cada piloto excepto el líder, calcular la diferencia con el piloto delante\n",
    "                for i in range(1, len(lap_data)):\n",
    "                    gap = lap_data.iloc[i]['CumulativeTime'] - lap_data.iloc[i-1]['CumulativeTime']\n",
    "                    lap_data.iloc[i, lap_data.columns.get_loc('GapToAhead')] = round(gap, 3)\n",
    "                \n",
    "                lap_groups.append(lap_data)\n",
    "        \n",
    "        # Combinar todos los grupos\n",
    "        if lap_groups:\n",
    "            data = pd.concat(lap_groups)\n",
    "        \n",
    "        # 3. DEFINIR VENTANAS DE UNDERCUT\n",
    "        if 'TyreAge' in data.columns:\n",
    "            print(\"Definiendo ventanas de undercut basadas en gaps entre coches...\")\n",
    "            # Condiciones típicas para undercut exitoso:\n",
    "            # - Gap entre 1.8 y 3.5 segundos al coche de adelante\n",
    "            # - Tus neumáticos tienen al menos 8 vueltas\n",
    "            # - No eres el líder (posición > 1)\n",
    "            data['UndercutWindow'] = ((data['GapToAhead'] >= 1.8) & \n",
    "                                      (data['GapToAhead'] <= 3.5) & \n",
    "                                      (data['TyreAge'] >= 8) &\n",
    "                                      (data['Position'] > 1)).astype(int)\n",
    "        else:\n",
    "            data['UndercutWindow'] = 0\n",
    "            print(\"No se encontró 'TyreAge' para calcular ventanas de undercut precisas\")\n",
    "    else:\n",
    "        # Si faltan columnas necesarias, usar aproximación simple\n",
    "        print(\"Faltan columnas necesarias para calcular gaps realistas\")\n",
    "        missing = [col for col in required_cols if col not in data.columns]\n",
    "        print(f\"Columnas faltantes: {missing}\")\n",
    "        \n",
    "        # Aproximar GapToLeader basado en posición\n",
    "        data['GapToLeader'] = (data['Position'] - 1) * 2.5  # ~2.5 segundos por posición\n",
    "        data.loc[data['Position'] == 1, 'GapToLeader'] = 0\n",
    "        \n",
    "        # Aproximar UndercutWindow\n",
    "        if 'TyreAge' in data.columns:\n",
    "            data['UndercutWindow'] = ((data['TyreAge'] > 10) & (data['Position'] > 1)).astype(int)\n",
    "        else:\n",
    "            data['UndercutWindow'] = 0\n",
    "    \n",
    "    # 4. CREAR OTRAS CARACTERÍSTICAS ESTRATÉGICAS\n",
    "    # IsLapped - Determinar si un piloto está doblado\n",
    "    if 'GapToLeader' in data.columns:\n",
    "        # Si el gap al líder es mayor a 60 segundos, probablemente está doblado\n",
    "        data['IsLapped'] = (data['GapToLeader'] > 60).astype(int)\n",
    "    else:\n",
    "        # Aproximación basada en posición\n",
    "        data['IsLapped'] = (data['Position'] > 15).astype(int)\n",
    "    \n",
    "    # Estadísticas finales\n",
    "    drs_count = data['DRSUsed'].sum()\n",
    "    drs_pct = (drs_count / len(data)) * 100\n",
    "    \n",
    "    undercut_count = data['UndercutWindow'].sum()\n",
    "    undercut_pct = (undercut_count / len(data)) * 100\n",
    "    \n",
    "    print(f\"\\nResumen de características estratégicas creadas:\")\n",
    "    print(f\"- DRSUsed: {drs_count} activaciones ({drs_pct:.2f}%)\")\n",
    "    print(f\"- UndercutWindow: {undercut_count} oportunidades ({undercut_pct:.2f}%)\")\n",
    "    print(f\"- GapToLeader: Gaps calculados basados en tiempos acumulados\")\n",
    "    print(f\"- IsLapped: Pilotos doblados identificados\")\n",
    "    \n",
    "    print(f\"\\nResultado final: {data.shape[0]} filas, {data.shape[1]} columnas\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_data_simplified(laps_df, weather_df, intervals_df, pitstops_df):\n",
    "    \"\"\"Función principal simplificada para la unión de todos los datos\"\"\"\n",
    "    print(\"Dimensiones originales:\")\n",
    "    print(f\"- laps_df: {laps_df.shape}\")\n",
    "    print(f\"- weather_df: {weather_df.shape}\")\n",
    "    print(f\"- intervals_df: {intervals_df.shape}\")\n",
    "    print(f\"- pitstops_df: {pitstops_df.shape}\")\n",
    "    \n",
    "    # Paso 1: Unir datos de vueltas y clima\n",
    "    print(\"\\nPaso 1: Uniendo datos de vueltas y clima\")\n",
    "    merged_df = merge_laps_and_weather(laps_df, weather_df)\n",
    "    \n",
    "    # Paso 2: Añadir datos de pitstops\n",
    "    print(\"\\nPaso 2: Añadiendo datos de pitstops\")\n",
    "    merged_df = add_pitstop_data(merged_df, pitstops_df)\n",
    "    \n",
    "    # Paso 3: Omitir datos de intervals pero crear características sintéticas\n",
    "    print(\"\\nPaso 3: Creando características sintéticas\")\n",
    "    merged_df = skip_interval_data(merged_df, intervals_df)\n",
    "    \n",
    "    # Verificación final\n",
    "    print(\"\\nVerificación final:\")\n",
    "    print(f\"Dimensiones del DataFrame final: {merged_df.shape}\")\n",
    "    print(f\"Columnas del DataFrame final: {merged_df.columns.tolist()}\")\n",
    "    \n",
    "    # Verificar columnas estratégicas\n",
    "    strategic_cols = ['DRSUsed', 'UndercutWindow', 'IsLapped', 'GapToLeader']\n",
    "    for col in strategic_cols:\n",
    "        if col in merged_df.columns:\n",
    "            if merged_df[col].dtype == bool:\n",
    "                # Convertir booleanos a enteros\n",
    "                merged_df[col] = merged_df[col].astype(int)\n",
    "            \n",
    "            # Mostrar distribución de valores\n",
    "            if col in ['DRSUsed', 'UndercutWindow', 'IsLapped']:\n",
    "                value_counts = merged_df[col].value_counts()\n",
    "                print(f\"\\nDistribución de {col}:\")\n",
    "                print(value_counts)\n",
    "                if 1 in value_counts and 0 in value_counts:\n",
    "                    pct_true = value_counts[1] / (value_counts[0] + value_counts[1]) * 100\n",
    "                    print(f\"Porcentaje de {col}=1: {pct_true:.1f}%\")\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Run the entire process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_all_data_simplified(laps_df, weather_df, intervals_df, pitstops_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar el resultado\n",
    "print(f\"DataFrame combinado: {merged_data.shape[0]} filas, {merged_data.shape[1]} columnas\")\n",
    "display(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration Approach\n",
    "\n",
    "We merge all our different data sources (lap data, weather conditions, pit stops, and interval information) into a single comprehensive DataFrame for several key reasons:\n",
    "\n",
    "1. **Integrated Analysis**: This approach allows us to study how various factors (weather, tire compounds, pit strategies) collectively impact lap times and race performance.\n",
    "\n",
    "2. **ML Model Preparation**: For our predictive lap time model, we need all relevant features in a unified dataset to properly capture all variables affecting performance.\n",
    "\n",
    "3. **Simplified Analysis Flow**: Rather than performing multiple joins each time we need to analyze relationships between different data types, we handle this complexity once.\n",
    "\n",
    "4. **Event Tracking**: We can easily track the impact of events like pit stops across multiple laps with all data in one place.\n",
    "\n",
    "5. **Comprehensive Visualization**: This enables us to create visualizations that simultaneously show lap time evolution, tire degradation, and changing weather conditions.\n",
    "\n",
    "For Formula 1 analysis, where everything is interconnected (tires affect lap times, weather affects tire performance, pit strategies affect position), this integrated data approach provides the most flexible foundation for both exploratory analysis and predictive modeling.\n",
    "\n",
    "### Next Point: exploratory data analysys (EDA) and data cleaning.\n",
    "\n",
    "As we can see in the head of the dataframe, there are some problems that need to be solved before implementing the model. Some of the most important ones are:\n",
    "\n",
    "* Missing values.\n",
    "* Columns with the same information but in different format (eg: float and strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_time_data(df):\n",
    "    \"\"\"\n",
    "    Versión completa que limpia y transforma datos de F1 para modelado:\n",
    "    - Elimina columnas innecesarias\n",
    "    - Convierte columnas de tiempo a segundos\n",
    "    - Transforma columnas categóricas a numéricas\n",
    "    - Maneja valores faltantes\n",
    "    - Elimina outliers\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con datos de vueltas\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame limpio y transformado listo para modelado\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    # Trabajar con una copia para no modificar el original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Guardar el DataFrame original antes de modificarlo (para referencia)\n",
    "    os.makedirs('../f1-strategy/data/raw/processed', exist_ok=True)\n",
    "    df.to_csv('../f1-strategy/data/raw/processed/original_data_backup.csv', index=False)\n",
    "    \n",
    "    # 1. Eliminar columnas innecesarias\n",
    "    columns_to_drop = [\n",
    "        'Time', 'Sector1SessionTime', 'Sector2SessionTime', 'Sector3SessionTime', \n",
    "        'PitOutTime', 'LapStartTime', 'LapStartDate', 'FastF1Generated', \n",
    "        'IsAccurate', 'DeletedReason'\n",
    "    ]\n",
    "    existing_columns = [col for col in columns_to_drop if col in data.columns]\n",
    "    \n",
    "    if existing_columns:\n",
    "        data = data.drop(columns=existing_columns)\n",
    "        print(f\"Eliminadas columnas: {', '.join(existing_columns)}\")\n",
    "    \n",
    "    # 2. Convertir LapTime y tiempos de sector a segundos\n",
    "    if 'LapTime' in data.columns and pd.api.types.is_timedelta64_dtype(data['LapTime']):\n",
    "        data['LapTime'] = data['LapTime'].dt.total_seconds()\n",
    "        print(\"Convertida LapTime a segundos.\")\n",
    "    \n",
    "    for sector in ['Sector1Time', 'Sector2Time', 'Sector3Time']:\n",
    "        if sector in data.columns and pd.api.types.is_timedelta64_dtype(data[sector]):\n",
    "            data[sector] = data[sector].dt.total_seconds()\n",
    "            print(f\"Convertido {sector} a segundos.\")\n",
    "    \n",
    "    # 3. Transformar PitInTime a LapToPit\n",
    "    if 'PitInTime' in data.columns:\n",
    "        # Crear nueva columna LapToPit basada en LapNumber donde PitInTime no es nulo\n",
    "        data['LapToPit'] = 0  # Valor por defecto\n",
    "        \n",
    "        # Para cada fila donde PitInTime no es nulo, establecer LapToPit = LapNumber\n",
    "        pit_mask = data['PitInTime'].notna()\n",
    "        if pit_mask.sum() > 0:\n",
    "            data.loc[pit_mask, 'LapToPit'] = data.loc[pit_mask, 'LapNumber']\n",
    "            \n",
    "        # Eliminar la columna PitInTime original\n",
    "        data = data.drop(columns=['PitInTime'])\n",
    "        print(f\"Transformada PitInTime a LapToPit. Detectadas {pit_mask.sum()} entradas a pit.\")\n",
    "    \n",
    "    # 4. Transformar Deleted a variable numérica (si existe)\n",
    "    if 'Deleted' in data.columns:\n",
    "        # Convertir a entero (False=0, True=1)\n",
    "        data['Deleted'] = data['Deleted'].astype(int)\n",
    "        print(f\"Convertida Deleted a valores 0/1. Hay {data['Deleted'].sum()} vueltas eliminadas.\")\n",
    "    \n",
    "    # 5. Transformar Team a valores numéricos\n",
    "    if 'Team' in data.columns:\n",
    "        # Guardar valores originales para ver exactamente qué nombres de equipos hay\n",
    "        team_values = data['Team'].value_counts()\n",
    "        print(f\"Valores originales de Team:\\n{team_values}\")\n",
    "        \n",
    "        # Mapeo de equipos a valores numéricos incluyendo todas las variantes de nombres\n",
    "        team_mapping = {\n",
    "            # Equipos con el nombre exacto como aparecen en los datos\n",
    "            'Alfa Romeo': 1,          # Kick Sauber\n",
    "            'AlphaTauri': 2,          # Racing Bulls\n",
    "            'Alpine': 3,              # Alpine\n",
    "            'Aston Martin': 4,        # Aston Martin\n",
    "            'Ferrari': 5,             # Ferrari\n",
    "            'Haas F1 Team': 6,        # Haas\n",
    "            'McLaren': 7,             # McLaren\n",
    "            'Mercedes': 8,            # Mercedes\n",
    "            'Red Bull Racing': 9,     # Red Bull\n",
    "            'Williams': 10,           # Williams\n",
    "            \n",
    "            # Nombres alternativos por si acaso\n",
    "            'Kick Sauber': 1,\n",
    "            'Racing Bulls': 2,\n",
    "            'Haas': 6,\n",
    "            'Red Bull': 9,\n",
    "            'RB': 2\n",
    "        }\n",
    "        \n",
    "        # Aplicar mapeo\n",
    "        data['TeamID'] = data['Team'].map(team_mapping)\n",
    "        \n",
    "        # Verificar si quedan equipos sin mapear\n",
    "        unmapped = data[data['TeamID'].isna()]['Team'].unique()\n",
    "        if len(unmapped) > 0:\n",
    "            print(f\"ADVERTENCIA: Equipos sin mapear: {unmapped}\")\n",
    "            # En caso de error, asignar valores secuenciales\n",
    "            next_id = max(team_mapping.values()) + 1\n",
    "            for team in unmapped:\n",
    "                team_mapping[team] = next_id\n",
    "                data.loc[data['Team'] == team, 'TeamID'] = next_id\n",
    "                print(f\"Asignado ID {next_id} a equipo desconocido: {team}\")\n",
    "                next_id += 1\n",
    "        else:\n",
    "            print(\"Todos los equipos mapeados correctamente.\")\n",
    "                \n",
    "        # Eliminar columna original después de verificar mapeo\n",
    "        data = data.drop(columns=['Team'])\n",
    "        print(f\"Transformada Team a TeamID con {len(set(team_mapping.values()))} valores únicos (1-10).\")\n",
    "    \n",
    "    # 6. Transformar NextCompound\n",
    "    if 'NextCompound' in data.columns:\n",
    "        # Crear mapeo\n",
    "        compound_mapping = {\n",
    "            'SOFT': 1,\n",
    "            'MEDIUM': 2,\n",
    "            'HARD': 3,\n",
    "            'INTERMEDIATE': 4,\n",
    "            'WET': 5\n",
    "        }\n",
    "        \n",
    "        # Guardar mapeo para referencia\n",
    "        if not data['NextCompound'].isna().all():\n",
    "            compound_values = data['NextCompound'].value_counts(dropna=False)\n",
    "            print(f\"Valores originales de NextCompound:\\n{compound_values}\")\n",
    "        \n",
    "        # Crear nueva columna con valores mapeados\n",
    "        data['NextCompoundID'] = data['NextCompound'].map(compound_mapping)\n",
    "        \n",
    "        # Rellenar NaN con 0 (sin cambio de compuesto)\n",
    "        data['NextCompoundID'] = data['NextCompoundID'].fillna(0).astype(int)\n",
    "        \n",
    "        # Eliminar columna original\n",
    "        data = data.drop(columns=['NextCompound'])\n",
    "        print(\"Transformada NextCompound a NextCompoundID (0=sin cambio, 1=soft, 2=medium, 3=hard, etc.)\")\n",
    "    \n",
    "    # 7. Manejar FreshTyreAfterStop\n",
    "    if 'FreshTyreAfterStop' in data.columns:\n",
    "        # Convertir a entero (False=0, True=1)\n",
    "        data['FreshTyreAfterStop'] = data['FreshTyreAfterStop'].fillna(0).astype(int)\n",
    "        print(\"Transformada FreshTyreAfterStop a valores 0/1 (0=no fresco o sin parada)\")\n",
    "    \n",
    "    # 8. Transformar Compound actual\n",
    "    if 'Compound' in data.columns:\n",
    "        # Usar el mismo mapeo que para NextCompound\n",
    "        compound_mapping = {\n",
    "            'SOFT': 1,\n",
    "            'MEDIUM': 2,\n",
    "            'HARD': 3,\n",
    "            'INTERMEDIATE': 4,\n",
    "            'WET': 5\n",
    "        }\n",
    "        \n",
    "        # Guardar para referencia\n",
    "        compound_values = data['Compound'].value_counts(dropna=False)\n",
    "        print(f\"Valores originales de Compound:\\n{compound_values}\")\n",
    "        \n",
    "        # Mapear y verificar valores faltantes\n",
    "        data['CompoundID'] = data['Compound'].map(compound_mapping)\n",
    "        \n",
    "        # Manejar valores no mapeados\n",
    "        unmapped = data[data['CompoundID'].isna()]['Compound'].unique()\n",
    "        if len(unmapped) > 0:\n",
    "            print(f\"ADVERTENCIA: Compuestos sin mapear: {unmapped}\")\n",
    "            # Asignar valores para compuestos no mapeados\n",
    "            next_id = max(compound_mapping.values()) + 1\n",
    "            for compound in unmapped:\n",
    "                compound_mapping[compound] = next_id\n",
    "                data.loc[data['Compound'] == compound, 'CompoundID'] = next_id\n",
    "                next_id += 1\n",
    "        \n",
    "        # Eliminar columna original\n",
    "        data = data.drop(columns=['Compound'])\n",
    "        print(\"Transformada Compound a CompoundID (1=soft, 2=medium, 3=hard, etc.)\")\n",
    "    \n",
    "    # 9. Antes de eliminar outliers, guardar estos datos en un DataFrame separado\n",
    "    if 'LapTime' in data.columns:\n",
    "        # Identificar outliers (vueltas muy rápidas o muy lentas)\n",
    "        q1 = data['LapTime'].quantile(0.05)\n",
    "        q3 = data['LapTime'].quantile(0.95)\n",
    "        \n",
    "        # Datos que se considerarían outliers\n",
    "        outlier_data = data[(data['LapTime'] < q1) | (data['LapTime'] > q3)].copy()\n",
    "        \n",
    "        # Añadir columna para clasificar el tipo de outlier\n",
    "        outlier_data['OutlierType'] = 'Unknown'\n",
    "        outlier_data.loc[outlier_data['LapTime'] < q1, 'OutlierType'] = 'VeryFast'\n",
    "        outlier_data.loc[outlier_data['LapTime'] > q3, 'OutlierType'] = 'VerySlow'\n",
    "        \n",
    "        # Guardar para uso futuro en estrategias\n",
    "        outlier_data.to_csv('../f1-strategy/data/raw/processed/exceptional_laps.csv', index=False)\n",
    "        print(f\"Guardados {len(outlier_data)} registros de vueltas excepcionales para análisis estratégico.\")\n",
    "        \n",
    "        # Continuar con el filtrado para el modelo predictivo\n",
    "        data = data[(data['LapTime'] >= q1) & (data['LapTime'] <= q3)]\n",
    "        print(f\"Filtrados outliers para el modelo predictivo. Rango válido: {q1:.2f}s - {q3:.2f}s\")\n",
    "    \n",
    "    # 10. Guardar versión limpia para referencia\n",
    "    data.to_csv('../f1-strategy/data/raw/processed/cleaned_data.csv', index=False)\n",
    "    print(f\"Guardado dataset limpio con {data.shape[0]} filas y {data.shape[1]} columnas.\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la limpieza de datos\n",
    "cleaned_data = clean_time_data(merged_data)\n",
    "\n",
    "# Mostrar las dimensiones antes y después\n",
    "print(f\"Dimensiones antes de limpieza: {merged_data.shape}\")\n",
    "print(f\"Dimensiones después de limpieza: {cleaned_data.shape}\")\n",
    "\n",
    "display(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Transformation Strategy\n",
    "\n",
    "## Columns Removed\n",
    "\n",
    "We removed several columns from the dataset to improve model performance and reduce dimensionality:\n",
    "\n",
    "1. **Time-related columns**:\n",
    "   - `Time`: Redundant timestamp information that doesn't provide predictive value\n",
    "   - `Sector1SessionTime`, `Sector2SessionTime`, `Sector3SessionTime`: Absolute timing information that's not relevant for lap time prediction\n",
    "   - `LapStartTime`, `LapStartDate`: Absolute timing that doesn't impact lap performance\n",
    "   - `PitOutTime`: 100% missing values, no usable information\n",
    "\n",
    "2. **Quality/metadata columns**:\n",
    "   - `FastF1Generated`: Metadata about data source, not a race performance factor\n",
    "   - `IsAccurate`: Data quality flag that doesn't impact predictive modeling\n",
    "   - `DeletedReason`: Only contained \"track limits\" information which is already captured in the `Deleted` flag\n",
    "\n",
    "## Columns Transformed\n",
    "\n",
    "We transformed several columns to improve their utility for machine learning:\n",
    "\n",
    "1. **Time conversions**:\n",
    "   - `LapTime`, `Sector1Time`, `Sector2Time`, `Sector3Time`: Converted from timedelta objects to seconds (float) for direct mathematical operations\n",
    "\n",
    "2. **Pit stop information**:\n",
    "   - `PitInTime` → `LapToPit`: Converted from timestamp to binary indicator (0 = no pit, actual lap number = pit entry) to represent when a driver entered the pits\n",
    "   - `NextCompound` → `NextCompoundID`: Mapped compound names to integers (0 = no change, 1 = soft, 2 = medium, 3 = hard, etc.)\n",
    "   - `FreshTyreAfterStop`: Filled NaN values with 0 (no fresh tire) and converted to integer (0/1)\n",
    "\n",
    "3. **Categorical conversions**:\n",
    "   - `Team` → `TeamID`: Mapped team names to integers (1-10) following the team order in the championship\n",
    "   - `Compound` → `CompoundID`: Mapped tire compounds to integers (1 = soft, 2 = medium, 3 = hard, etc.)\n",
    "   - `Deleted`: Converted boolean to integer (0/1)\n",
    "\n",
    "## Outlier Handling\n",
    "\n",
    "We implemented a robust outlier detection and handling strategy:\n",
    "\n",
    "1. Identified outliers in lap times using the 5th and 95th percentiles\n",
    "2. Classified outliers as \"VeryFast\" or \"VerySlow\" laps\n",
    "3. Stored outliers separately for potential strategic analysis\n",
    "4. Removed outliers from the training dataset to improve model quality\n",
    "\n",
    "## Data Preservation\n",
    "\n",
    "Throughout the cleaning process, we maintained data integrity by:\n",
    "\n",
    "1. Working with copies of the original dataframe\n",
    "2. Saving the original data before modifications\n",
    "3. Documenting all transformations with clear logging\n",
    "4. Saving both the exceptional laps and cleaned datasets for future reference\n",
    "\n",
    "These transformations significantly improve the dataset's suitability for machine learning while preserving the essential racing performance information needed for accurate lap time prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Create features related with tyres and impact on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tyre_features(df):\n",
    "    \"\"\"\n",
    "    Crea características relacionadas con neumáticos y su impacto en el rendimiento\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con datos limpios\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con nuevas características de neumáticos\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # 1. Edad de los neumáticos\n",
    "    if 'TyreLife' in data.columns:\n",
    "        data['TyreAge'] = data['TyreLife']\n",
    "        print(\"Creada feature: TyreAge\")\n",
    "    \n",
    "    # 2. Cambio de posición (comparado con la vuelta anterior)\n",
    "    if 'Position' in data.columns and 'Driver' in data.columns:\n",
    "        data['PositionChange'] = data.groupby('Driver')['Position'].diff().fillna(0)\n",
    "        print(\"Creada feature: PositionChange\")\n",
    "    \n",
    "    # 3. Carga de combustible (aproximación basada en la vuelta)\n",
    "    if 'LapNumber' in data.columns:\n",
    "        max_lap = data['LapNumber'].max()\n",
    "        data['FuelLoad'] = 1 - (data['LapNumber'] / max_lap).round(4)  # Aproximación simple\n",
    "        print(\"Creada feature: FuelLoad (aproximación)\")\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear características de neumáticos\n",
    "cleaned_tyre_features_data = create_tyre_features(cleaned_data)\n",
    "\n",
    "# Mostrar nuevas columnas\n",
    "new_columns = set(cleaned_tyre_features_data.columns) - set(cleaned_data.columns)\n",
    "print(f\"Nuevas columnas creadas: {new_columns}\")\n",
    "\n",
    "display(cleaned_tyre_features_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_colors = {\n",
    "    1: 'red',     # SOFT\n",
    "    2: 'yellow',  # MEDIUM\n",
    "    3: 'gray',    # HARD\n",
    "    4: 'green',   # INTERMEDIATE\n",
    "    5: 'blue'     # WET\n",
    "}\n",
    "\n",
    "# Compound names for better labels in the legend\n",
    "compound_names = {\n",
    "    1: 'SOFT', \n",
    "    2: 'MEDIUM', \n",
    "    3: 'HARD', \n",
    "    4: 'INTERMEDIATE', \n",
    "    5: 'WET'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la relación entre edad de neumáticos y tiempo por vuelta\n",
    "if 'TyreAge' in cleaned_tyre_features_data.columns and 'CompoundID' in cleaned_tyre_features_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Filtrar para mostrar solo compuestos principales\n",
    "    for compound_id in cleaned_tyre_features_data['CompoundID'].unique():\n",
    "        subset = cleaned_tyre_features_data[cleaned_tyre_features_data['CompoundID'] == compound_id]\n",
    "        \n",
    "        # Agrupar por edad de neumático y calcular promedio\n",
    "        agg_data = subset.groupby('TyreAge')['LapTime'].mean().reset_index()\n",
    "        \n",
    "        # Usar el color correspondiente del diccionario actualizado\n",
    "        color = compound_colors.get(compound_id, 'black')  # 'black' como color por defecto\n",
    "        \n",
    "        # Get readable compound name for legend\n",
    "        compound_name = compound_names.get(compound_id, f'Unknown ({compound_id})')\n",
    "        \n",
    "        plt.plot(agg_data['TyreAge'], agg_data['LapTime'], 'o-', \n",
    "                 color=color, label=f'{compound_name} Tyre')\n",
    "    \n",
    "    plt.xlabel('Edad del Neumático (vueltas)')\n",
    "    plt.ylabel('Tiempo por Vuelta (s)')\n",
    "    plt.title('Degradación de Neumáticos: Efecto en el Tiempo por Vuelta')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('../outputs/week3/tyre_degradation_colored.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tire Degradation Analysis\n",
    "\n",
    "This plot reveals how tire age affects lap times across different compounds. \n",
    "\n",
    "The SOFT compound (red) generally produces slower lap times but shows inconsistent degradation patterns with several performance spikes. \n",
    "\n",
    "MEDIUM tires (yellow) deliver strong initial performance but gradually degrade. \n",
    "\n",
    "HARD tires (gray) demonstrate superior longevity, becoming the fastest option after approximately 35 laps. \n",
    "\n",
    "This visualization confirms the classic F1 tire performance trade-off: softer compounds offer initial speed but degrade faster, while harder compounds provide durability at the expense of initial performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe with features related to strategies and gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las características estratégicas ya existen\n",
    "strategy_data = cleaned_tyre_features_data.copy()\n",
    "\n",
    "# Redondear GapToLeader a 3 decimales\n",
    "if 'GapToLeader' in strategy_data.columns:\n",
    "    strategy_data['GapToLeader'] = strategy_data['GapToLeader'].round(3)\n",
    "    print(\"GapToLeader redondeado a 3 decimales\")\n",
    "\n",
    "# Lista completa de columnas estratégicas importantes\n",
    "strategic_cols = [\n",
    "    # Posición y gaps\n",
    "    'DRSUsed', 'GapToLeader', 'IsLapped', 'UndercutWindow',\n",
    "    # Estrategia de pit stops\n",
    "    'PitNextLap', 'FreshTyreAfterStop', 'LapToPit',\n",
    "    # Neumáticos\n",
    "    'TyreLife', 'TyreAge'\n",
    "]\n",
    "\n",
    "# Mostrar las columnas estratégicas presentes\n",
    "present_cols = [col for col in strategic_cols if col in strategy_data.columns]\n",
    "print(f\"Características estratégicas presentes: {present_cols}\")\n",
    "\n",
    "strategy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos la característica DRSUsed, visualizar su impacto\n",
    "if 'DRSUsed' in strategy_data.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x='DRSUsed', y='LapTime', data=strategy_data)\n",
    "    plt.title('Impacto del DRS en Tiempos por Vuelta')\n",
    "    plt.xlabel('DRS Usado (0=No, 1=Sí)')\n",
    "    plt.ylabel('Tiempo por Vuelta (s)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('../outputs/week3/drs_impact.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of DRS Impact in Barcelona\n",
    "The graph reveals a surprising yet understandable outcome: the use of DRS appears to have a minimal impact on lap times at the Barcelona-Catalunya Circuit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Analysis of DRS Impact at the Barcelona GP\n",
    "\n",
    "### DRS Zones\n",
    "The DRS zones are located at:\n",
    "1. The main straight immediately following the final corner.\n",
    "2. The section between curves 9 and 10.\n",
    "\n",
    "The **Barcelona-Catalunya Circuit** has unique characteristics that significantly influence the effectiveness of DRS:\n",
    "\n",
    "![Barcelona-Catalunya Circuit](../outputs//week3/Spain_Circuit.jpg)\n",
    "\n",
    "*Formula 1 official track map: [https://www.formula1.com/en/information/spain-circuit-de-barcelona-catalunya-barcelona.6F5mWJGRuYkQ1XPX48pl8]*\n",
    "\n",
    "#### Technical Explanation\n",
    "This phenomenon can be attributed to several specific features of the Barcelona circuit:\n",
    "\n",
    "- **Limited DRS Zones:**  \n",
    "  The circuit features only two relatively short DRS zones:  \n",
    "  - The main straight (approximately 950m)  \n",
    "  - The brief section between curves 9 and 10 (around 250m)\n",
    "\n",
    "- **Aerodynamic Setup:**  \n",
    "  Due to the numerous medium- and high-speed corners, teams generally opt for high downforce setups, which limits the performance gains from DRS.\n",
    "\n",
    "- **Challenge of the Final Corner:**  \n",
    "  The 16th corner, positioned just before the main straight, is fast and difficult to closely follow, making it hard to maintain the one-second gap required for DRS activation.\n",
    "\n",
    "- **Circuit Balance:**  \n",
    "  Only about 20% of the lap is run in sections where DRS can be effective, while the remaining 80% relies on performance in corners where DRS is not applicable.\n",
    "\n",
    "- **Dilution of Time Gains:**  \n",
    "  Although DRS can provide a gain of 0.3–0.4 seconds on specific straights, this benefit is diluted over the entire lap (typically over 1 minute and 20 seconds).\n",
    "\n",
    "This analysis clearly illustrates why Barcelona is known as a circuit where overtaking is challenging, despite the presence of DRS, and why the results shown in the graph are not worrying. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los tiempos por vuelta para cada piloto\n",
    "lap_times = strategy_data.groupby(['LapNumber', 'Driver'])['LapTime'].mean().reset_index()\n",
    "\n",
    "# Convertir a segundos si es necesario (en caso de que sea un objeto timedelta)\n",
    "if pd.api.types.is_timedelta64_dtype(lap_times['LapTime']):\n",
    "    lap_times['LapTime'] = lap_times['LapTime'].dt.total_seconds()\n",
    "\n",
    "# Graficar los tiempos por vuelta para cada piloto\n",
    "plt.figure(figsize=(14, 7))\n",
    "for driver in ['VER', 'HAM', 'RUS']:\n",
    "    driver_data = lap_times[lap_times['Driver'] == driver]\n",
    "    if not driver_data.empty:\n",
    "        plt.plot(driver_data['LapNumber'], driver_data['LapTime'], \n",
    "                 marker='o', markersize=3, linewidth=2, label=driver)\n",
    "\n",
    "plt.title('Tiempo por Vuelta por Piloto')\n",
    "plt.xlabel('Número de Vuelta')\n",
    "plt.ylabel('Tiempo por Vuelta (s)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Añadir una línea horizontal en el tiempo de vuelta \"ideal\" para referencia visual\n",
    "min_laptime = lap_times['LapTime'].min()\n",
    "plt.axhline(y=min_laptime, color='gray', linestyle='--', alpha=0.5, \n",
    "            label=f'Mejor tiempo: {min_laptime:.2f}s')\n",
    "\n",
    "# Ajustar rango del eje Y para mejor visualización (excluyendo valores extremos)\n",
    "q1 = lap_times['LapTime'].quantile(0.05)\n",
    "q3 = lap_times['LapTime'].quantile(0.95)\n",
    "plt.ylim(q1 * 0.95, q3 * 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/week3/lap_times.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(strategy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Features of pitstops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear características de paradas\n",
    "pitstop_data = strategy_data.copy()\n",
    "# Mostrar nuevas columnas\n",
    "display(pitstop_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos PitNextLap, visualizar su impacto en el tiempo\n",
    "if 'LapsSincePitStop' in pitstop_data.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x='PitNextLap', y='LapTime', data=pitstop_data)\n",
    "    plt.title('Tiempos por Vuelta Antes de una Parada')\n",
    "    plt.xlabel('Parada en Siguiente Vuelta (0=No, 1=Sí)')\n",
    "    plt.ylabel('Tiempo por Vuelta (s)')\n",
    "    plt.savefig('../outputs/week3/before_after_pit_times.png')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Post-Pit Stop Lap Times\n",
    "\n",
    "When analyzing the variable **`LapsSincePitstop`**, we often observe that the **first two laps** after exiting the pits tend to be **slower**. There are two main reasons for this:\n",
    "\n",
    "1. **Tire Warm-Up:**  \n",
    "   New tires require a lap or two to reach their optimal operating temperature. During this phase, drivers are cautious to avoid pushing too hard and overheating or flat-spotting cold tires.\n",
    "\n",
    "2. **Tire Management:**  \n",
    "   Immediately after a pit stop, drivers may also manage their tires more conservatively to prolong their life, leading to slightly slower lap times in these early laps.\n",
    "\n",
    "After these initial laps, there is a **noticeable drop in lap times**, as the tires have reached an ideal temperature and drivers can push harder.\n",
    "\n",
    "### Conclussion\n",
    "\n",
    "Therefore, this graph shows that the information contained on our dataset is very well correlated with the reality of a Formula 1 race. \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que exista la columna LapsSincePitStop\n",
    "if 'LapsSincePitStop' in pitstop_data.columns:\n",
    "    \n",
    "    # Filtramos datos sólo para Verstappen\n",
    "    verstappen_data = pitstop_data[pitstop_data['Driver'] == 'VER']\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Limitamos a las primeras 10 vueltas después de la parada (puedes ajustar a tu gusto)\n",
    "    verstappen_subset = verstappen_data[verstappen_data['LapsSincePitStop'] <= 10]\n",
    "    \n",
    "    # Agrupamos por vueltas desde la última parada y calculamos el promedio de lap time\n",
    "    lap_groups = verstappen_subset.groupby('LapsSincePitStop')['LapTime'].mean().reset_index()\n",
    "    \n",
    "    # Graficamos\n",
    "    plt.plot(lap_groups['LapsSincePitStop'], lap_groups['LapTime'], 'o-', linewidth=2, label='Verstappen')\n",
    "    \n",
    "    plt.title('Evolución del Tiempo por Vuelta de Verstappen Después de una Parada')\n",
    "    plt.xlabel('Vueltas Desde Última Parada')\n",
    "    plt.ylabel('Tiempo por Vuelta Promedio (s)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.savefig('../outputs/week3/after_pit_time_evo_verstappen.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Pit Stop Performance Recovery\n",
    "\n",
    "This chart tracks how lap times evolve after a pit stop:\n",
    "\n",
    "1. **Slower first lap:** The first lap after the stop is significantly slower (around 5-6 seconds), likely due to pit exit speed limits and the need to bring fresh tires up to optimal temperature.  \n",
    "2. **Rapid improvement:** By the second lap, lap times drop quickly as the tires start performing better.  \n",
    "3. **Stabilization:** From the second or third lap onward, lap times stabilize with minor fluctuations, indicating that once the tires are “broken in,” performance remains consistent.  \n",
    "4. **Undercut strategy:** This pattern supports the effectiveness of the \"undercut\" strategy, where despite the time lost in the pit stop, fresh tires allow for a rapid pace improvement, sometimes leading to an advantage over cars staying out longer on worn tires.  \n",
    "\n",
    "Overall, this analysis aligns well with typical F1 race strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Select and Prepare Final Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Analysis\n",
    "\n",
    "This bar chart identifies data completeness issues across key features. Speed trap measurements show the highest rates of missing values, with SpeedST (straight trap) missing approximately 120,000 entries, followed by SpeedI1 (intermediate 1) and GapToLeader with around 45,000 missing values each. SpeedFL (flying lap) has fewer gaps (~15,000). \n",
    "\n",
    "\n",
    "These patterns suggest systematic data collection issues at certain track points rather than random gaps, informing our imputation strategy in the data preparation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento para matriz de correlación (eliminación de valores faltantes)\n",
    "def prepare_correlation_features(df, max_features=10, target_col='LapTime'):\n",
    "    # Hacer una copia para no modificar el original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Seleccionar solo características numéricas\n",
    "    numeric_features = df_clean.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Eliminar filas con valores faltantes en cualquier característica numérica\n",
    "    df_clean = df_clean[numeric_features].dropna()\n",
    "    print(f\"Filas iniciales: {len(df)}, Filas después de eliminar NaN: {len(df_clean)}\")\n",
    "    \n",
    "    # Limitar a las características más correlacionadas con LapTime\n",
    "    if len(numeric_features) > max_features:\n",
    "        # Asegurarse que target_col esté en el conjunto de datos\n",
    "        if target_col in numeric_features:\n",
    "            # Calcular correlaciones y ordenar\n",
    "            corr = df_clean[numeric_features].corr()[target_col].abs().sort_values(ascending=False)\n",
    "            selected_features = corr.index[:max_features]  # Top N incluye target_col\n",
    "            print(f\"Seleccionadas {len(selected_features)} características de {len(numeric_features)} disponibles\")\n",
    "            return df_clean[selected_features], selected_features\n",
    "    \n",
    "    # Si no hay suficientes características, devolver todas\n",
    "    return df_clean[numeric_features], numeric_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para correlación\n",
    "clean_data, selected_features = prepare_correlation_features(final_model_data, max_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación de las principales características numéricas\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(clean_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Matriz de Correlación de Características Principales')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/week3/correlation_matrix_clean.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Matrix\n",
    "\n",
    "This correlation matrix reveals several important relationships:\n",
    "1. **Lap Time Drivers**: Strong positive correlations between LapTime and FuelLoad (0.63), PitNextLap (0.62), and PitTimeLost (0.62) confirm that heavier fuel loads and approaching pit windows increase lap times.\n",
    "2. **Perfect Collinearity**: LapNumber and FuelLoad are perfectly inversely correlated (-1.0), as expected since we calculated FuelLoad directly from LapNumber.\n",
    "3. **Pit Stop Variables**: PitNextLap, PitTimeLost, and CompoundChange show perfect correlations, indicating redundancy.\n",
    "4. **Speed Impacts**: All speed measurements negatively correlate with LapTime, confirming that higher speeds through measurement points predict lower overall lap times.\n",
    "\n",
    "This matrix helps identify feature redundancies to remove and important relationships to preserve in our predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la preparación de datos\n",
    "clean_data, selected_features = prepare_correlation_features(final_model_data, max_features=10)\n",
    "print(\"Características seleccionadas:\", selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Calcular y visualizar matriz de correlación de datos limpios\n",
    "def calculate_correlation_matrix(clean_df):\n",
    "    \"\"\"\n",
    "    Calcula la matriz de correlación para los datos limpios\n",
    "    \"\"\"\n",
    "    correlation_matrix = clean_df.corr()\n",
    "    \n",
    "    # Visualizar matriz\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Matriz de Correlación de Características Principales')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/week3/correlation_matrix_clean.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el cálculo de la matriz de correlación\n",
    "correlation_matrix = calculate_correlation_matrix(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3: Analizar variables perfectamente correlacionadas\n",
    "def analyze_perfect_correlations(clean_df, corr_matrix):\n",
    "    # Identificar pares de variables con correlación muy alta (>0.95)\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    # Obtener pares de alta correlación (excluyendo la diagonal)\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            col_i = corr_matrix.columns[i]\n",
    "            col_j = corr_matrix.columns[j]\n",
    "            corr_value = corr_matrix.iloc[i, j]\n",
    "            \n",
    "            if abs(corr_value) > 0.95:\n",
    "                high_corr_pairs.append((col_i, col_j, corr_value))\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    if high_corr_pairs:\n",
    "        print(\"Variables con correlación muy alta (>0.95):\")\n",
    "        for col_i, col_j, corr_value in high_corr_pairs:\n",
    "            print(f\"  - {col_i} y {col_j}: {corr_value:.2f}\")\n",
    "    else:\n",
    "        print(\"No se encontraron pares de variables con correlación extremadamente alta (>0.95).\")\n",
    "    \n",
    "    return high_corr_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar análisis de correlaciones perfectas\n",
    "high_correlation_pairs = analyze_perfect_correlations(clean_data, correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: Analizar correlaciones con el tiempo por vuelta\n",
    "def analyze_laptime_correlations(corr_matrix, target_col='LapTime'):\n",
    "    # Extraer correlaciones con LapTime\n",
    "    if target_col in corr_matrix.columns:\n",
    "        laptime_corr = corr_matrix[target_col].abs().sort_values(ascending=False)\n",
    "        \n",
    "        # Dividir en grupos según su correlación\n",
    "        high_corr = laptime_corr[laptime_corr >= 0.5]\n",
    "        medium_corr = laptime_corr[(laptime_corr >= 0.25) & (laptime_corr < 0.5)]\n",
    "        low_corr = laptime_corr[laptime_corr < 0.25]\n",
    "        \n",
    "        print(f\"Correlaciones con {target_col}:\")\n",
    "        print(\"\\nAlta correlación (>=0.5):\")\n",
    "        for feature, corr in high_corr.items():\n",
    "            if feature != target_col:  # Excluir correlación consigo mismo\n",
    "                print(f\"  - {feature}: {corr:.2f}\")\n",
    "        \n",
    "        print(\"\\nCorrelación media (0.25-0.5):\")\n",
    "        for feature, corr in medium_corr.items():\n",
    "            print(f\"  - {feature}: {corr:.2f}\")\n",
    "        \n",
    "        print(\"\\nBaja correlación (<0.25):\")\n",
    "        for feature, corr in low_corr.items():\n",
    "            print(f\"  - {feature}: {corr:.2f}\")\n",
    "        \n",
    "        return laptime_corr\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar análisis de correlaciones con LapTime\n",
    "laptime_correlations = analyze_laptime_correlations(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5: Recomendar conjunto final de características\n",
    "def recommend_final_features(corr_with_laptime, high_corr_pairs):\n",
    "    print(\"\\nRecomendaciones para el conjunto final de características:\")\n",
    "    \n",
    "    # 1. Identificar características altamente predictivas\n",
    "    if corr_with_laptime is not None:\n",
    "        high_pred_features = corr_with_laptime[corr_with_laptime >= 0.5].index.tolist()\n",
    "        if 'LapTime' in high_pred_features:\n",
    "            high_pred_features.remove('LapTime')  # Eliminar la variable objetivo\n",
    "        \n",
    "        print(\"1. Características altamente predictivas a mantener:\")\n",
    "        for feature in high_pred_features:\n",
    "            print(f\"   - {feature}\")\n",
    "    \n",
    "    # 2. Identificar redundancias a eliminar\n",
    "    if high_corr_pairs:\n",
    "        print(\"\\n2. Redundancias a considerar (mantener solo una de cada par):\")\n",
    "        for col_i, col_j, _ in high_corr_pairs:\n",
    "            # Decidir cuál mantener basado en correlación con LapTime\n",
    "            if corr_with_laptime is not None:\n",
    "                if corr_with_laptime.get(col_i, 0) >= corr_with_laptime.get(col_j, 0):\n",
    "                    print(f\"   - Mantener {col_i}, considerar eliminar {col_j}\")\n",
    "                else:\n",
    "                    print(f\"   - Mantener {col_j}, considerar eliminar {col_i}\")\n",
    "            else:\n",
    "                print(f\"   - Considerar mantener solo una de: {col_i} o {col_j}\")\n",
    "    \n",
    "    # 3. Características con valor estratégico pero baja correlación\n",
    "    if corr_with_laptime is not None:\n",
    "        strategic_features = ['Position', 'TrackStatus']\n",
    "        present_strategic = [f for f in strategic_features if f in corr_with_laptime.index]\n",
    "        \n",
    "        if present_strategic:\n",
    "            print(\"\\n3. Características con valor estratégico a mantener a pesar de baja correlación:\")\n",
    "            for feature in present_strategic:\n",
    "                print(f\"   - {feature}: {corr_with_laptime.get(feature, 0):.2f}\")\n",
    "    \n",
    "    print(\"\\nBalancear estos factores proporcionará un conjunto óptimo de características para el modelo final.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar recomendación final\n",
    "recommend_final_features(laptime_correlations, high_correlation_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Analysis Based on Correlation Matrix\n",
    "\n",
    "Based on the correlation matrix analysis of our cleaned dataset, we've identified several key insights for model optimization:\n",
    "\n",
    "1. **Highly Predictive Features**:\n",
    "   - Variables with the strongest correlation to lap time should form the foundation of our predictive model.\n",
    "   - This includes fuel load, speed measurements at key track sections, and tyre age.\n",
    "\n",
    "2. **Redundancy Management**:\n",
    "   - Several pairs of features show extremely high correlation (>0.95), indicating redundancy.\n",
    "   - For each redundant pair, we prioritize keeping the feature with stronger correlation to lap time.\n",
    "   - LapNumber and FuelLoad represent the same information in different forms - we retain both for their distinct interpretative value.\n",
    "\n",
    "3. **Strategic Variables**:\n",
    "   - Some variables like Position show lower statistical correlation but provide critical strategic insights.\n",
    "   - These are retained despite lower predictive power due to their importance for race strategy decisions.\n",
    "\n",
    "4. **Pitstop-related Features**:\n",
    "   - Pitstop timing and impact variables provide crucial information for strategy modeling.\n",
    "   - We created composite variables where appropriate to reduce dimensionality while preserving information.\n",
    "\n",
    "This optimization approach balances statistical considerations with domain knowledge to create a feature set that maximizes both predictive power and strategic insight for F1 strategy decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar estadísticas descriptivas del dataset final\n",
    "print(\"\\nEstadísticas descriptivas del dataset final:\")\n",
    "display(clean_data.describe())\n",
    "\n",
    "# Guardar datos procesados para modelos\n",
    "# final_model_data.to_csv('data/processed/model_ready_data.csv', index=False)\n",
    "print(\"\\nDatos listos para modelado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_csv = clean_data.to_csv(\"../outputs/week3/model_lap_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Data Preparation\n",
    "\n",
    "Our data preparation process follows a systematic approach broken down into specialized components:\n",
    "\n",
    "1. **Data Cleaning**: We convert temporal data to seconds for analysis and remove outliers based on 5th and 95th percentiles to ensure our model isn't trained on anomalous lap times caused by safety cars, accidents, or data errors.\n",
    "\n",
    "2. **Tire Performance Features**: We create features that capture tire degradation and its effect on performance, including tire age and position changes between laps.\n",
    "\n",
    "3. **Race Strategy Features**: These features represent strategic elements like DRS usage, undercut opportunities, and gaps to the race leader that influence driving approach and lap times.\n",
    "\n",
    "4. **Pit Stop Analysis**: We process pit stop data to calculate time lost during stops, stint length, and compound change effects that are critical for strategy modeling.\n",
    "\n",
    "5. **Feature Selection**: Finally, we identify the most relevant variables for our model based on domain knowledge of Formula 1 racing, ensuring we include key performance drivers while avoiding redundant or noisy features.\n",
    "\n",
    "This modular approach allows us to clearly understand each transformation and facilitates future refinements to specific aspects of data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico\n",
    "print(\"Resumen estadístico de variables numéricas:\")\n",
    "display(clean_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo por vuelta según tipo de neumático\n",
    "if 'Compound' in clean_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Compound', y='LapTime', data=clean_data)\n",
    "    plt.title('Tiempo por Vuelta según Tipo de Neumático')\n",
    "    plt.xlabel('Compuesto')\n",
    "    plt.ylabel('Tiempo (segundos)')\n",
    "    plt.savefig('../outputs/week3/laptime_by_tyre.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de dispersión: Variables importantes vs LapTime\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "if 'TyreAge' in pitstop_data.columns:\n",
    "    sns.scatterplot(x='TyreAge', y='LapTime', hue='Compound', data=pitstop_data, ax=axes[0])\n",
    "    axes[0].set_title('LapTime vs TyreAge')\n",
    "    \n",
    "if 'TrackTemp' in pitstop_data.columns:\n",
    "    sns.scatterplot(x='TrackTemp', y='LapTime', data=pitstop_data, ax=axes[1])\n",
    "    axes[1].set_title('LapTime vs TrackTemp')\n",
    "    \n",
    "if 'FuelLoad' in pitstop_data.columns:\n",
    "    sns.scatterplot(x='FuelLoad', y='LapTime', data=pitstop_data, ax=axes[2])\n",
    "    axes[2].set_title('LapTime vs FuelLoad')\n",
    "\n",
    "if 'Position' in pitstop_data.columns:\n",
    "    sns.scatterplot(x='Position', y='LapTime', data=pitstop_data, ax=axes[3])\n",
    "    axes[3].set_title('LapTime vs Position')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/week3/key_variables_scatter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Análisis Específico de Paradas en Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pitstop_impact(data):\n",
    "    \"\"\"\n",
    "    Visualiza el impacto de las paradas en los tiempos por vuelta\n",
    "    Versión simplificada que asegura generar todos los gráficos relevantes\n",
    "    \"\"\"\n",
    "    # Verificar disponibilidad de datos de paradas\n",
    "    has_pitstop_data = ('LapsSincePitStop' in data.columns and 'PitNextLap' in data.columns)\n",
    "    \n",
    "    if not has_pitstop_data:\n",
    "        print(\"No hay suficientes datos de paradas disponibles para visualización.\")\n",
    "        return\n",
    "    \n",
    "    # 1. Degradación de neumáticos por compuesto\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Filtrar para mostrar hasta 20 vueltas después de una parada\n",
    "    plot_data = data[data['LapsSincePitStop'] <= 20].copy()\n",
    "    \n",
    "    # Verificar si hay datos suficientes\n",
    "    if len(plot_data) < 10:\n",
    "        print(\"Datos insuficientes para analizar degradación por compuesto.\")\n",
    "    else:\n",
    "        # Agrupar por tipo de compuesto y vueltas desde parada\n",
    "        plot_data['LapsSincePitStop'] = plot_data['LapsSincePitStop'].astype(int)\n",
    "        grouped = plot_data.groupby(['Compound', 'LapsSincePitStop'])['LapTime'].mean().reset_index()\n",
    "        \n",
    "        # Dibujar una línea por cada compuesto\n",
    "        for compound in grouped['Compound'].unique():\n",
    "            compound_data = grouped[grouped['Compound'] == compound]\n",
    "            color = compound_colors.get(compound, 'black')\n",
    "            plt.plot(compound_data['LapsSincePitStop'], \n",
    "                     compound_data['LapTime'], \n",
    "                     'o-', \n",
    "                     color=color,\n",
    "                     label=f'Compuesto {compound}')\n",
    "        \n",
    "        plt.xlabel('Vueltas desde la parada')\n",
    "        plt.ylabel('Tiempo por vuelta promedio (s)')\n",
    "        plt.title('Degradación de neumáticos por compuesto')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('../outputs/week3/tyre_degradation_by_compound.png')\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. Análisis de paradas por compuesto\n",
    "    if 'PitNextLap' in data.columns:\n",
    "        # Identificar vueltas previas a paradas\n",
    "        pitstop_rows = data['PitNextLap'] == 1\n",
    "        pitstop_count = pitstop_rows.sum()\n",
    "        \n",
    "        if pitstop_count > 0:\n",
    "            print(f\"Encontradas {pitstop_count} filas que indican paradas.\")\n",
    "            pitstop_data = data[pitstop_rows]\n",
    "            \n",
    "            # 2.1 Contar paradas por cada piloto para verificar\n",
    "            pit_by_driver = pitstop_data.groupby('Driver').size()\n",
    "            print(\"\\nNúmero de paradas por piloto:\")\n",
    "            print(pit_by_driver)\n",
    "            \n",
    "            # 2.2 Gráfico de compuestos usados antes de paradas\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            compound_counts = pitstop_data['Compound'].value_counts()\n",
    "            ax = sns.barplot(x=compound_counts.index, y=compound_counts.values, \n",
    "                           palette={comp: compound_colors.get(comp, 'gray') for comp in compound_counts.index})\n",
    "            \n",
    "            # Añadir etiquetas\n",
    "            for i, count in enumerate(compound_counts):\n",
    "                ax.text(i, count/2, str(count), ha='center', va='center', fontweight='bold')\n",
    "                \n",
    "            plt.title('Compuestos utilizados antes de paradas')\n",
    "            plt.xlabel('Compuesto')\n",
    "            plt.ylabel('Número de paradas')\n",
    "            plt.savefig('../outputs/week3/compounds_before_pitstop.png')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No se encontraron filas que indiquen paradas en boxes (PitNextLap=1).\")\n",
    "    else:\n",
    "        print(\"No se encuentra la columna 'PitNextLap' para analizar paradas.\")\n",
    "\n",
    "# Ejecutar con los datos limpios\n",
    "visualize_pitstop_impact(final_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preprocesamiento para Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sección previa a \"8. Preprocesamiento para Modelado\"\n",
    "# Finalizar la selección de variables basada en el análisis de correlación\n",
    "\n",
    "# 1. Definir qué variables mantener según el análisis previo\n",
    "variables_to_keep = [\n",
    "    # Variables con alta correlación con LapTime\n",
    "    'LapTime', 'FuelLoad', 'TyreAge', 'SpeedI1', 'SpeedI2',\n",
    "    # Variables estratégicas\n",
    "    'Position', 'Compound', 'LapsSincePitStop',\n",
    "    # Información del piloto\n",
    "    'Driver'\n",
    "]\n",
    "\n",
    "# 2. Verificar cuáles de estas variables están disponibles\n",
    "available_vars = [var for var in variables_to_keep if var in clean_data.columns]\n",
    "print(f\"Variables finales seleccionadas para modelado ({len(available_vars)}/{len(variables_to_keep)}):\")\n",
    "for var in available_vars:\n",
    "    print(f\"- {var}\")\n",
    "\n",
    "# 3. Crear DataFrame final para modelado\n",
    "model_ready_data = clean_data[available_vars].copy()\n",
    "\n",
    "# 4. Guardar el dataset listo para modelado\n",
    "model_ready_data.to_csv('../outputs/week3/model_ready_data.csv', index=False)\n",
    "print(f\"\\nDatos listos para modelado guardados con {model_ready_data.shape[0]} filas y {model_ready_data.shape[1]} columnas\")\n",
    "\n",
    "# 5. Verificar valores faltantes finales\n",
    "missing_values = model_ready_data.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\nValores faltantes en el dataset final:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"\\nNo hay valores faltantes en el dataset final.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Preprocesamiento para Modelado\n",
    "\n",
    "def preprocess_data_for_modeling(df):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos para el modelado, dividiendo en conjuntos de entrenamiento/prueba\n",
    "    y configurando los transformadores necesarios.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame listo para modelado\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test, preprocessor\n",
    "    \"\"\"\n",
    "    # Separar características y objetivo\n",
    "    X = df.drop('LapTime', axis=1)\n",
    "    y = df['LapTime']\n",
    "    \n",
    "    # Identificar columnas categóricas y numéricas\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    print(f\"Características categóricas: {cat_cols}\")\n",
    "    print(f\"Características numéricas: {num_cols}\")\n",
    "    \n",
    "    # Configurar transformadores para preprocesamiento\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), num_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "        ])\n",
    "    \n",
    "    # Dividir datos en entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Conjunto de entrenamiento: {X_train.shape[0]} muestras\")\n",
    "    print(f\"Conjunto de prueba: {X_test.shape[0]} muestras\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "# Preprocesar datos para modelado\n",
    "X_train, X_test, y_train, y_test, preprocessor = preprocess_data_for_modeling(model_ready_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entrenamiento de Modelo XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Entrena un modelo XGBoost\n",
    "    \"\"\"\n",
    "    # Crear pipeline con preprocesamiento y modelo\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', xgb.XGBRegressor(objective='reg:squarederror'))\n",
    "    ])\n",
    "    \n",
    "    # Parámetros para Grid Search\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__learning_rate': [0.01, 0.1],\n",
    "        'regressor__max_depth': [3, 5, 7],\n",
    "        'regressor__min_child_weight': [1, 3]\n",
    "    }\n",
    "    \n",
    "    # Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"Entrenando modelo XGBoost con GridSearchCV (esto puede tardar varios minutos)...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Mejores parámetros\n",
    "    print(\"\\nMejores parámetros XGBoost:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Predecir\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluar\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nMétricas de evaluación del modelo XGBoost:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f} segundos\")\n",
    "    print(f\"  MAE: {mae:.4f} segundos\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    \n",
    "    # Analizar características importantes\n",
    "    if hasattr(best_model.named_steps['regressor'], 'feature_importances_'):\n",
    "        # Obtener nombres de características después del preprocesamiento\n",
    "        try:\n",
    "            cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "            \n",
    "            # Obtener nombres de características después de one-hot encoding\n",
    "            encoder = best_model.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot']\n",
    "            cat_features = encoder.get_feature_names_out(cat_cols).tolist()\n",
    "            all_features = num_cols + cat_features\n",
    "            \n",
    "            # Crear DataFrame con importancias\n",
    "            importances = best_model.named_steps['regressor'].feature_importances_\n",
    "            feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importances})\n",
    "            feature_importance = feature_importance.sort_values('Importance', ascending=False).head(15)\n",
    "            \n",
    "            # Visualizar\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "            plt.title('Top 15 Características Más Importantes - XGBoost')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('outputs/week3/xgboost_feature_importance.png')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error al analizar importancia de características: {e}\")\n",
    "    \n",
    "    return best_model, y_pred\n",
    "\n",
    "# Entrenar modelo XGBoost\n",
    "xgb_model, y_pred_xgb = train_xgboost(X_train, X_test, y_train, y_test, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Entrenamiento de Red Neuronal (Opcional)\n",
    "\n",
    "Este paso es opcional y puede omitirse si prefieres usar solo XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch_nn(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de red neuronal con PyTorch\n",
    "    \"\"\"\n",
    "    # Aplicar preprocesamiento\n",
    "    print(\"Preprocesando datos para la red neuronal...\")\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    X_train_tensor = torch.FloatTensor(X_train_processed.toarray())\n",
    "    y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_processed.toarray())\n",
    "    y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1)\n",
    "    \n",
    "    # Crear conjuntos de datos y cargadores\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Inicializar modelo\n",
    "    print(f\"Inicializando red neuronal con {X_train_processed.shape[1]} entradas...\")\n",
    "    input_size = X_train_processed.shape[1]\n",
    "    model = LapTimeNN(input_size)\n",
    "    \n",
    "    # Definir criterio y optimizador\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Guardar historial de pérdidas para graficar\n",
    "    losses = []\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"\\nEntrenando red neuronal...\")\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Visualizar curva de aprendizaje\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs+1), losses)\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.title('Curva de Aprendizaje - Red Neuronal')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('outputs/week3/neural_network_learning_curve.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "        y_pred = y_pred_tensor.numpy().flatten()\n",
    "        \n",
    "    # Métricas\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nMétricas de evaluación de la Red Neuronal:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f} segundos\")\n",
    "    print(f\"  MAE: {mae:.4f} segundos\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    \n",
    "    return model, y_pred\n",
    "\n",
    "# Preguntar si se desea entrenar la red neuronal\n",
    "train_nn = True  # Cambiar a False para omitir este paso\n",
    "if train_nn:\n",
    "    nn_model, y_pred_nn = train_pytorch_nn(X_train, X_test, y_train, y_test, preprocessor)\n",
    "else:\n",
    "    nn_model, y_pred_nn = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualización y Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(y_test, y_pred_xgb, y_pred_nn=None, max_points=1000):\n",
    "    \"\"\"\n",
    "    Visualiza los resultados de los modelos\n",
    "    \"\"\"\n",
    "    # Limitar número de puntos para visualización clara\n",
    "    if len(y_test) > max_points:\n",
    "        # Muestrear aleatoriamente para mantener la distribución\n",
    "        indices = np.random.choice(len(y_test), max_points, replace=False)\n",
    "        y_test_sample = y_test.iloc[indices]\n",
    "        y_pred_xgb_sample = y_pred_xgb[indices]\n",
    "        if y_pred_nn is not None:\n",
    "            y_pred_nn_sample = y_pred_nn[indices]\n",
    "    else:\n",
    "        y_test_sample = y_test\n",
    "        y_pred_xgb_sample = y_pred_xgb\n",
    "        y_pred_nn_sample = y_pred_nn\n",
    "    \n",
    "    # Configurar tamaño de figura\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Determinar número de subplots\n",
    "    if y_pred_nn is not None:\n",
    "        n_plots = 3\n",
    "    else:\n",
    "        n_plots = 2\n",
    "    \n",
    "    # 1. Dispersión XGBoost\n",
    "    plt.subplot(1, n_plots, 1)\n",
    "    plt.scatter(y_test_sample, y_pred_xgb_sample, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Tiempo real (s)')\n",
    "    plt.ylabel('Tiempo predicho (s)')\n",
    "    plt.title('XGBoost: Predicciones vs Reales')\n",
    "    \n",
    "    # 2. Residuos XGBoost\n",
    "    plt.subplot(1, n_plots, 2)\n",
    "    residuals_xgb = y_test_sample - y_pred_xgb_sample\n",
    "    plt.scatter(y_pred_xgb_sample, residuals_xgb, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicción (s)')\n",
    "    plt.ylabel('Residuos (s)')\n",
    "    plt.title('XGBoost: Residuos vs Predicciones')\n",
    "    \n",
    "    # 3. Si hay predicciones de red neuronal\n",
    "    if y_pred_nn is not None:\n",
    "        plt.subplot(1, n_plots, 3)\n",
    "        plt.scatter(y_test_sample, y_pred_nn_sample, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.xlabel('Tiempo real (s)')\n",
    "        plt.ylabel('Tiempo predicho (s)')\n",
    "        plt.title('Red Neuronal: Predicciones vs Reales')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/week3/prediction_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Comparativa de errores\n",
    "    if y_pred_nn is not None:\n",
    "        errors_xgb = np.abs(y_test - y_pred_xgb)\n",
    "        errors_nn = np.abs(y_test - y_pred_nn)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(errors_xgb, alpha=0.5, bins=50, label='XGBoost')\n",
    "        plt.hist(errors_nn, alpha=0.5, bins=50, label='Red Neuronal')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Error Absoluto (s)')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.title('Distribución de Errores por Modelo')\n",
    "        plt.xlim(0, min(10, max(errors_xgb.max(), errors_nn.max())))\n",
    "        plt.savefig('outputs/week3/error_distribution.png')\n",
    "        plt.show()\n",
    "\n",
    "# Visualizar resultados\n",
    "visualize_predictions(y_test, y_pred_xgb, y_pred_nn if train_nn else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Guardar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(xgb_model, nn_model=None):\n",
    "    \"\"\"\n",
    "    Guarda los modelos entrenados\n",
    "    \"\"\"\n",
    "    # Guardar modelo XGBoost\n",
    "    joblib.dump(xgb_model, 'models/week3/xgboost_laptime.joblib')\n",
    "    print(\"Modelo XGBoost guardado en 'models/week3/xgboost_laptime.joblib'\")\n",
    "    \n",
    "    # Guardar modelo PyTorch si existe\n",
    "    if nn_model is not None:\n",
    "        torch.save(nn_model.state_dict(), 'models/week3/nn_laptime.pth')\n",
    "        print(\"Modelo PyTorch guardado en 'models/week3/nn_laptime.pth'\")\n",
    "\n",
    "# Guardar modelos\n",
    "save_models(xgb_model, nn_model if train_nn else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Prueba de Predicción con Nuevos Datos\n",
    "\n",
    "Probemos el modelo con una situación de carrera hipotética."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model, available_features):\n",
    "    \"\"\"\n",
    "    Prueba el modelo con datos hipotéticos\n",
    "    \"\"\"\n",
    "    # Crear un escenario de ejemplo\n",
    "    example_drivers = [\"VER\", \"HAM\", \"LEC\", \"PER\", \"SAI\"]\n",
    "    compounds = [\"SOFT\", \"MEDIUM\", \"HARD\"]\n",
    "    \n",
    "    # Crear DataFrame de ejemplo\n",
    "    example_data = []\n",
    "    \n",
    "    for driver in example_drivers:\n",
    "        for compound in compounds:\n",
    "            for tyre_age in [1, 10, 20]:\n",
    "                # Valores predeterminados\n",
    "                row = {\n",
    "                    'Driver': driver,\n",
    "                    'Compound': compound,\n",
    "                    'TyreAge': tyre_age,\n",
    "                    'LapNumber': 30,\n",
    "                    'TrackTemp': 35.0,\n",
    "                    'AirTemp': 25.0,\n",
    "                    'Position': 5,\n",
    "                    'FuelLoad': 0.5,\n",
    "                    'LapsSincePitStop': tyre_age,  # Si está disponible\n",
    "                    'PitNextLap': 0,  # No hay parada en la siguiente vuelta\n",
    "                }\n",
    "                \n",
    "                # Añadir solo características disponibles\n",
    "                example_row = {k: v for k, v in row.items() if k in available_features}\n",
    "                example_data.append(example_row)\n",
    "    \n",
    "    example_df = pd.DataFrame(example_data)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    predictions = model.predict(example_df)\n",
    "    \n",
    "    # Añadir predicciones al DataFrame\n",
    "    example_df['PredictedLapTime'] = predictions\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    print(\"\\nPredicciones para escenarios de ejemplo:\")\n",
    "    display(example_df)\n",
    "    \n",
    "    # Graficar por compuesto y edad de neumáticos\n",
    "    if 'Compound' in example_df.columns and 'TyreAge' in example_df.columns:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for driver in example_drivers[:3]:  # Limitar a 3 pilotos para claridad\n",
    "            driver_data = example_df[example_df['Driver'] == driver]\n",
    "            for compound in compounds:\n",
    "                tyre_data = driver_data[driver_data['Compound'] == compound]\n",
    "                plt.plot(tyre_data['TyreAge'], tyre_data['PredictedLapTime'], \n",
    "                         marker='o', linestyle='-', label=f\"{driver} - {compound}\")\n",
    "                \n",
    "        plt.xlabel('Edad de Neumáticos (vueltas)')\n",
    "        plt.ylabel('Tiempo Predicho (s)')\n",
    "        plt.title('Predicción de Tiempos por Tipo y Edad de Neumáticos')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('outputs/week3/tyre_age_predictions.png')\n",
    "        plt.show()\n",
    "    \n",
    "    return example_df\n",
    "\n",
    "# Probar predicciones con el modelo XGBoost\n",
    "try:\n",
    "    prediction_examples = test_prediction(xgb_model, available_features)\n",
    "except Exception as e:\n",
    "    print(f\"Error al realizar predicciones de prueba: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusiones y Próximos Pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "En este notebook, hemos:\n",
    "1. Cargado y preparado datos de FastF1 y OpenF1, incluyendo información de vueltas, paradas y condiciones meteorológicas\n",
    "2. Realizado feature engineering para crear características como edad de neumáticos, carga de combustible, y paradas en boxes\n",
    "3. Analizado el impacto de las paradas y la degradación de neumáticos en los tiempos por vuelta\n",
    "4. Entrenado un modelo XGBoost para predecir tiempos por vuelta\n",
    "5. Opcionalmente, entrenado una red neuronal para comparar su rendimiento\n",
    "6. Visualizado y evaluado los resultados\n",
    "\n",
    "### Resultados Principales\n",
    "- El modelo XGBoost puede predecir tiempos por vuelta con un error medio de X segundos\n",
    "- Las características más importantes para la predicción son [listar las top 3-5 características]\n",
    "- La degradación de neumáticos tiene un impacto significativo en los tiempos por vuelta\n",
    "- Las paradas en boxes muestran patrones claros en cuanto a su timing y elección de compuesto\n",
    "\n",
    "### Próximos Pasos (Semana 4)\n",
    "1. Integrar este modelo de predicción con un sistema de decisiones basado en reglas\n",
    "2. Añadir lógica para recomendar estrategias de paradas en boxes\n",
    "3. Desarrollar sistema para simular undercuts/overcuts basados en las predicciones\n",
    "4. Crear visualizaciones interactivas para analizar el impacto de diferentes estrategias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
