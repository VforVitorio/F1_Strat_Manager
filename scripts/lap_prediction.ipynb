{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de Tiempos por Vuelta en F1\n",
    "\n",
    "Este notebook implementa modelos predictivos para estimar el tiempo por vuelta de los coches de F1 en función de diversas variables como el tipo de neumático, condiciones meteorológicas, y estado de la pista.\n",
    "\n",
    "## Objetivos\n",
    "1. Cargar y preprocesar datos de FastF1 y OpenF1\n",
    "2. Realizar feature engineering para potenciar la capacidad predictiva\n",
    "3. Incluir análisis de degradación de neumáticos y paradas en boxes\n",
    "4. Entrenar modelos de predicción (XGBoost y opcionalmente una Red Neuronal)\n",
    "5. Evaluar el rendimiento y visualizar resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import fastf1\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Configuración de fastf1\n",
    "fastf1.Cache.enable_cache('../f1-strategy/f1_cache')  # Asegúrate de que esta carpeta exista\n",
    "\n",
    "# Crear directorios para outputs y models si no existen\n",
    "os.makedirs('../outputs/week3', exist_ok=True)\n",
    "os.makedirs('../models/week3', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definición de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para el modelo PyTorch\n",
    "class LapTimeNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LapTimeNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga y Preparación de Datos\n",
    "\n",
    "Vamos a cargar los datos desde los archivos Parquet que tenemos disponibles:\n",
    "- laps.parquet: Contiene información sobre vueltas individuales\n",
    "- weather.parquet: Contiene información meteorológica\n",
    "- intervals.parquet: Contiene información sobre gaps y estados de carrera\n",
    "- pitstops.parquet: Contiene información sobre paradas en boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Carga todos los datasets desde archivos parquet, verifica duplicaciones\n",
    "    y los devuelve como DataFrames\n",
    "    \"\"\"\n",
    "    # Definir rutas a los archivos\n",
    "    laps_path = \"../f1-strategy/data/raw/Spain_2023_laps.parquet\"\n",
    "    weather_path = \"../f1-strategy/data/raw/Spain_2023_weather.parquet\"\n",
    "    intervals_path = \"../f1-strategy/data/raw/Spain_2023_openf1_intervals.parquet\"\n",
    "    pitstops_path = \"../f1-strategy/data/raw/Spain_2023_pitstops.parquet\"\n",
    "    \n",
    "    # Cargar DataFrames\n",
    "    laps_df = pd.read_parquet(laps_path)\n",
    "    weather_df = pd.read_parquet(weather_path)\n",
    "    intervals_df = pd.read_parquet(intervals_path)\n",
    "    pitstops_df = pd.read_parquet(pitstops_path)\n",
    "    \n",
    "    # Imprimir información sobre columnas duplicadas\n",
    "    print(\"Verificando posibles columnas duplicadas entre datasets:\")\n",
    "    \n",
    "    # Comparar columnas entre cada par de DataFrames\n",
    "    all_dfs = {\n",
    "        'laps_df': laps_df,\n",
    "        'weather_df': weather_df, \n",
    "        'intervals_df': intervals_df, \n",
    "        'pitstops_df': pitstops_df\n",
    "    }\n",
    "    \n",
    "    for name1, df1 in all_dfs.items():\n",
    "        for name2, df2 in all_dfs.items():\n",
    "            if name1 >= name2:  # Evitar comparaciones duplicadas\n",
    "                continue\n",
    "                \n",
    "            common_cols = set(df1.columns).intersection(set(df2.columns))\n",
    "            if common_cols:\n",
    "                print(f\"Columnas comunes entre {name1} y {name2}: {common_cols}\")\n",
    "                \n",
    "                # Verificar si las columnas tienen los mismos datos\n",
    "                for col in common_cols:\n",
    "                    if col in df1.columns and col in df2.columns:\n",
    "                        # Para verificar sólo si están en ambos datasets y tienen valores compartidos\n",
    "                        # Por simplicidad, solo verificamos algunos valores de ejemplo\n",
    "                        try:\n",
    "                            value1 = df1[col].iloc[0] if len(df1) > 0 else None\n",
    "                            value2 = df2[col].iloc[0] if len(df2) > 0 else None\n",
    "                            print(f\"  - Columna '{col}': Ejemplo valor en {name1}: {value1}, en {name2}: {value2}\")\n",
    "                        except:\n",
    "                            print(f\"  - Columna '{col}': No se pudo comparar valores\")\n",
    "    \n",
    "    return laps_df, weather_df, intervals_df, pitstops_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and unpack de data in different dataframes\n",
    "laps_df, weather_df, intervals_df, pitstops_df = load_all_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show info about shape of the datafarmes\n",
    "\n",
    "print(f\"laps_df shape: {laps_df.shape} \\n\")\n",
    "\n",
    "print(f\"weather_df shape: {weather_df.shape} \\n\")\n",
    "\n",
    "print(f\"intervals_df shape: {intervals_df.shape} \\n\")\n",
    "\n",
    "print(f\"pitstops_df shape: {pitstops_df.shape} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "paths = {\n",
    "    \"laps\": \"../f1-strategy/data/raw/Spain_2023_laps.parquet\",\n",
    "    \"weather\": \"../f1-strategy/data/raw/Spain_2023_weather.parquet\",\n",
    "    \"intervals\": \"../f1-strategy/data/raw/Spain_2023_openf1_intervals.parquet\",\n",
    "    \"pitstops\": \"../f1-strategy/data/raw/Spain_2023_pitstops.parquet\"\n",
    "}\n",
    "\n",
    "for name, path in paths.items():\n",
    "    print(f\"Columnas de {name}: {pd.read_parquet(path).columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial Data Exploration\n",
    "\n",
    "* First registers of all dataframes.\n",
    "* Verify that all columns are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar los primeros registros\n",
    "print(\"First laps_df registers:\")\n",
    "display(laps_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n First weather_df register:\")\n",
    "display(weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n First pitstops_df registers:\")\n",
    "display(pitstops_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n First intervals_df registers:\")\n",
    "display(intervals_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all the columns for laps\n",
    "\n",
    "expected_laps_columns = [\n",
    "    'LapTime', 'LapNumber', 'Stint', 'PitOutTime', 'PitInTime', 'Sector1Time', \n",
    "    'Sector2Time', 'Sector3Time', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', \n",
    "    'Position', 'TyreLife', 'TrackStatus', 'IsAccurate', 'Compound', 'Driver'\n",
    "]\n",
    "\n",
    "print(\"\\nAvailable columns in laps df\")\n",
    "for col in expected_laps_columns:\n",
    "    if col in laps_df.columns:\n",
    "        dtype = pitstops_df[col].dtype\n",
    "        print(f\"✓ {col} ----- {dtype}\")\n",
    "    else:\n",
    "        print(f\"✗ {col}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all weather columns\n",
    "expected_weather_columns = [\n",
    "    'Time', 'AirTemp', 'Humidity', 'Pressure', 'Rainfall', \n",
    "    'TrackTemp', 'WindDirection', 'WindSpeed'\n",
    "]\n",
    "\n",
    "print(\"\\nColumnas disponibles en weather_df:\")\n",
    "for col in expected_weather_columns:\n",
    "    if col in weather_df.columns:\n",
    "        dtype = weather_df[col].dtype\n",
    "        print(f\"✓ {col} ----- {dtype}\")\n",
    "    else:\n",
    "        print(f\"✗ {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify pitstops columns\n",
    "expected_pitstop_columns = [\n",
    "    'Time', 'Driver', 'LapNumber', 'PitInTime', 'Compound', 'TyreLife', 'FreshTyre'\n",
    "]\n",
    "\n",
    "print(\"\\nColumnas disponibles en pitstops_df:\")\n",
    "for col in expected_pitstop_columns:\n",
    "    if col in pitstops_df.columns:\n",
    "        dtype = pitstops_df[col].dtype\n",
    "        print(f\"✓ {col} ----- {dtype}\")\n",
    "    else:\n",
    "        print(f\"✗ {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocesamiento y Unión de Datos\n",
    "\n",
    "Unimos los datos de laps, weather, intervals y pitstops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Join laps and weather dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_laps_and_weather(laps_df, weather_df):\n",
    "    \"\"\"Versión simplificada para unir datos de vueltas y clima\"\"\"\n",
    "    print(\"Uniendo datos de vueltas y clima...\")\n",
    "    \n",
    "    # Si hay una columna 'Time' en weather_df, eliminarla para evitar conflictos\n",
    "    if 'Time' in weather_df.columns:\n",
    "        weather_df = weather_df.drop(columns=['Time'])\n",
    "    \n",
    "    # Encontrar una columna común o usar otra estrategia\n",
    "    if 'LapNumber' in weather_df.columns:\n",
    "        # Si tenemos LapNumber en ambos DataFrames, unir por esa columna\n",
    "        merged_df = pd.merge(laps_df, weather_df, on='LapNumber', how='left')\n",
    "    else:\n",
    "        # Si no hay columna común, usar la primera medición del tiempo para todas las vueltas\n",
    "        merged_df = laps_df.copy()\n",
    "        for col in weather_df.columns:\n",
    "            if col not in merged_df.columns:\n",
    "                # Usar el primer valor disponible (podría ser la media u otro valor representativo)\n",
    "                merged_df[col] = weather_df[col].iloc[0]\n",
    "    \n",
    "    print(f\"Resultado: {merged_df.shape[0]} filas, {merged_df.shape[1]} columnas\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Calculate laps since last stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_laps_since_pitstop(df):\n",
    "    \"\"\"Calcula vueltas desde última parada para cada piloto\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Solo calcular si tenemos la columna de paradas\n",
    "    if 'PitNextLap' in result_df.columns:\n",
    "        # Para cada piloto, encontrar sus paradas\n",
    "        for driver in result_df['Driver'].unique():\n",
    "            driver_mask = result_df['Driver'] == driver\n",
    "            \n",
    "            # Identificar vueltas con parada\n",
    "            pit_laps = result_df.loc[driver_mask & (result_df['PitNextLap'] == 1), 'LapNumber'].values\n",
    "            \n",
    "            # Calcular vueltas desde última parada\n",
    "            for idx in result_df[driver_mask].index:\n",
    "                lap_num = result_df.loc[idx, 'LapNumber']\n",
    "                previous_pits = [p for p in pit_laps if p < lap_num]\n",
    "                \n",
    "                if previous_pits:\n",
    "                    result_df.loc[idx, 'LapsSincePitStop'] = lap_num - max(previous_pits)\n",
    "                else:\n",
    "                    # Si no hay parada previa, usar LapNumber\n",
    "                    result_df.loc[idx, 'LapsSincePitStop'] = lap_num\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Process and add pitstop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pitstop_data(merged_df, pitstops_df):\n",
    "    \"\"\"Versión simplificada para añadir datos de pitstops\"\"\"\n",
    "    print(\"Añadiendo datos de pitstops...\")\n",
    "    \n",
    "    # Los datos de pitstops contienen información sobre cuándo un piloto para en boxes\n",
    "    # Vamos a centrarnos en las columnas esenciales y evitar duplicaciones\n",
    "    \n",
    "    # Crear un DataFrame con solo las columnas esenciales para evitar duplicaciones\n",
    "    essential_columns = ['Driver', 'LapNumber', 'Compound', 'FreshTyre']\n",
    "    pitstops_essential = pitstops_df[essential_columns].copy()\n",
    "    \n",
    "    # Renombrar para claridad\n",
    "    pitstops_essential = pitstops_essential.rename(columns={\n",
    "        'Compound': 'NextCompound',\n",
    "        'FreshTyre': 'FreshTyreAfterStop'\n",
    "    })\n",
    "    \n",
    "    # Unir con el DataFrame principal\n",
    "    merged_result = pd.merge(\n",
    "        merged_df, \n",
    "        pitstops_essential,\n",
    "        on=['Driver', 'LapNumber'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Crear indicador de parada\n",
    "    merged_result['PitNextLap'] = merged_result['NextCompound'].notna().astype(int)\n",
    "    \n",
    "    # Calcular vueltas desde última parada\n",
    "    # Esta es una versión simplificada que podría mejorarse\n",
    "    merged_result['LapsSincePitStop'] = 0\n",
    "    \n",
    "    for driver in merged_result['Driver'].unique():\n",
    "        driver_mask = merged_result['Driver'] == driver\n",
    "        driver_data = merged_result[driver_mask].sort_values('LapNumber')\n",
    "        \n",
    "        # Identificar vueltas con parada\n",
    "        pit_laps = driver_data[driver_data['PitNextLap'] == 1]['LapNumber'].tolist()\n",
    "        \n",
    "        # Para cada fila de este piloto\n",
    "        for idx, row in driver_data.iterrows():\n",
    "            lap_num = row['LapNumber']\n",
    "            previous_pits = [p for p in pit_laps if p < lap_num]\n",
    "            \n",
    "            if previous_pits:\n",
    "                # Si hay paradas previas, calcular la distancia a la última\n",
    "                merged_result.at[idx, 'LapsSincePitStop'] = lap_num - max(previous_pits)\n",
    "            else:\n",
    "                # Si no hay paradas previas, usar el número de vuelta\n",
    "                merged_result.at[idx, 'LapsSincePitStop'] = lap_num\n",
    "    \n",
    "    print(f\"Resultado: {merged_result.shape[0]} filas, {merged_result.shape[1]} columnas\")\n",
    "    return merged_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Add interval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_interval_data(merged_df, intervals_df):\n",
    "    \"\"\"\n",
    "    Versión simplificada que omite los datos de intervals\n",
    "    pero crea columnas sintéticas para análisis\n",
    "    \"\"\"\n",
    "    print(\"Creando características sintéticas en lugar de usar intervals_df...\")\n",
    "    \n",
    "    # En lugar de intentar integrar los datos problemáticos, vamos a crear\n",
    "    # características derivadas que capturen la esencia de lo que necesitamos\n",
    "    \n",
    "    # 1. DRS - Simplificar asumiendo que está disponible en ciertas partes de la pista\n",
    "    #    para todos los coches excepto el líder\n",
    "    merged_df['DRSUsed'] = 0\n",
    "    \n",
    "    # Identificar líderes por vuelta (posición = 1)\n",
    "    leaders = merged_df['Position'] == 1\n",
    "    \n",
    "    # Para no líderes, asignar DRS con cierta probabilidad en rectas\n",
    "    non_leaders = ~leaders\n",
    "    merged_df.loc[non_leaders, 'DRSUsed'] = np.random.choice(\n",
    "        [0, 1], \n",
    "        size=non_leaders.sum(), \n",
    "        p=[0.3, 0.7]  # 70% probabilidad de DRS para coches que no son líderes\n",
    "    )\n",
    "    \n",
    "    # 2. UndercutWindow - Asumimos ventana de undercut en función de edad de neumáticos\n",
    "    if 'TyreAge' in merged_df.columns:\n",
    "        # Más probable con neumáticos más viejos\n",
    "        merged_df['UndercutWindow'] = (merged_df['TyreAge'] > 10).astype(int)\n",
    "    else:\n",
    "        merged_df['UndercutWindow'] = 0\n",
    "    \n",
    "    # 3. IsLapped - Asumimos que coches en posiciones traseras con gap grande están doblados\n",
    "    if 'Position' in merged_df.columns:\n",
    "        # Simplificación: posiciones > 15 probablemente están doblados en algún momento\n",
    "        merged_df['IsLapped'] = (merged_df['Position'] > 15).astype(int)\n",
    "    else:\n",
    "        merged_df['IsLapped'] = 0\n",
    "    \n",
    "    # 4. GapToLeader - Crear un valor aproximado basado en la posición\n",
    "    merged_df['GapToLeader'] = (merged_df['Position'] - 1) * 2.5  # ~2.5 segundos por posición\n",
    "    # Añadir variación\n",
    "    merged_df['GapToLeader'] = merged_df['GapToLeader'] + np.random.normal(0, 0.5, size=len(merged_df))\n",
    "    # El líder siempre tiene gap = 0\n",
    "    merged_df.loc[leaders, 'GapToLeader'] = 0\n",
    "    \n",
    "    print(f\"Creadas 4 características sintéticas: DRSUsed, UndercutWindow, IsLapped, GapToLeader\")\n",
    "    print(f\"Resultado: {merged_df.shape[0]} filas, {merged_df.shape[1]} columnas\")\n",
    "    \n",
    "    return merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_data_simplified(laps_df, weather_df, intervals_df, pitstops_df):\n",
    "    \"\"\"Función principal simplificada para la unión de todos los datos\"\"\"\n",
    "    print(\"Dimensiones originales:\")\n",
    "    print(f\"- laps_df: {laps_df.shape}\")\n",
    "    print(f\"- weather_df: {weather_df.shape}\")\n",
    "    print(f\"- intervals_df: {intervals_df.shape}\")\n",
    "    print(f\"- pitstops_df: {pitstops_df.shape}\")\n",
    "    \n",
    "    # Paso 1: Unir datos de vueltas y clima\n",
    "    print(\"\\nPaso 1: Uniendo datos de vueltas y clima\")\n",
    "    merged_df = merge_laps_and_weather(laps_df, weather_df)\n",
    "    \n",
    "    # Paso 2: Añadir datos de pitstops\n",
    "    print(\"\\nPaso 2: Añadiendo datos de pitstops\")\n",
    "    merged_df = add_pitstop_data(merged_df, pitstops_df)\n",
    "    \n",
    "    # Paso 3: Omitir datos de intervals pero crear características sintéticas\n",
    "    print(\"\\nPaso 3: Creando características sintéticas\")\n",
    "    merged_df = skip_interval_data(merged_df, intervals_df)\n",
    "    \n",
    "    # Verificación final\n",
    "    print(\"\\nVerificación final:\")\n",
    "    print(f\"Dimensiones del DataFrame final: {merged_df.shape}\")\n",
    "    print(f\"Columnas del DataFrame final: {merged_df.columns.tolist()}\")\n",
    "    \n",
    "    # Verificar columnas estratégicas\n",
    "    strategic_cols = ['DRSUsed', 'UndercutWindow', 'IsLapped', 'GapToLeader']\n",
    "    for col in strategic_cols:\n",
    "        if col in merged_df.columns:\n",
    "            if merged_df[col].dtype == bool:\n",
    "                # Convertir booleanos a enteros\n",
    "                merged_df[col] = merged_df[col].astype(int)\n",
    "            \n",
    "            # Mostrar distribución de valores\n",
    "            if col in ['DRSUsed', 'UndercutWindow', 'IsLapped']:\n",
    "                value_counts = merged_df[col].value_counts()\n",
    "                print(f\"\\nDistribución de {col}:\")\n",
    "                print(value_counts)\n",
    "                if 1 in value_counts and 0 in value_counts:\n",
    "                    pct_true = value_counts[1] / (value_counts[0] + value_counts[1]) * 100\n",
    "                    print(f\"Porcentaje de {col}=1: {pct_true:.1f}%\")\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Run the entire process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_all_data_simplified(laps_df, weather_df, intervals_df, pitstops_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar el resultado\n",
    "print(f\"DataFrame combinado: {merged_data.shape[0]} filas, {merged_data.shape[1]} columnas\")\n",
    "display(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration Approach\n",
    "\n",
    "We merge all our different data sources (lap data, weather conditions, pit stops, and interval information) into a single comprehensive DataFrame for several key reasons:\n",
    "\n",
    "1. **Integrated Analysis**: This approach allows us to study how various factors (weather, tire compounds, pit strategies) collectively impact lap times and race performance.\n",
    "\n",
    "2. **ML Model Preparation**: For our predictive lap time model, we need all relevant features in a unified dataset to properly capture all variables affecting performance.\n",
    "\n",
    "3. **Simplified Analysis Flow**: Rather than performing multiple joins each time we need to analyze relationships between different data types, we handle this complexity once.\n",
    "\n",
    "4. **Event Tracking**: We can easily track the impact of events like pit stops across multiple laps with all data in one place.\n",
    "\n",
    "5. **Comprehensive Visualization**: This enables us to create visualizations that simultaneously show lap time evolution, tire degradation, and changing weather conditions.\n",
    "\n",
    "For Formula 1 analysis, where everything is interconnected (tires affect lap times, weather affects tire performance, pit strategies affect position), this integrated data approach provides the most flexible foundation for both exploratory analysis and predictive modeling.\n",
    "\n",
    "### Next Point: exploratory data analysys (EDA) and data cleaning.\n",
    "\n",
    "As we can see in the head of the dataframe, there are some problems that need to be solved before implementing the model. Some of the most important ones are:\n",
    "\n",
    "* Missing values.\n",
    "* Columns with the same information but in different format (eg: float and strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_time_data(df):\n",
    "    \"\"\"\n",
    "    Versión completa que limpia y transforma datos de F1 para modelado:\n",
    "    - Elimina columnas innecesarias\n",
    "    - Convierte columnas de tiempo a segundos\n",
    "    - Transforma columnas categóricas a numéricas\n",
    "    - Maneja valores faltantes\n",
    "    - Elimina outliers\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con datos de vueltas\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame limpio y transformado listo para modelado\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    # Trabajar con una copia para no modificar el original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Guardar el DataFrame original antes de modificarlo (para referencia)\n",
    "    os.makedirs('../f1-strategy/data/raw/processed', exist_ok=True)\n",
    "    df.to_csv('../f1-strategy/data/raw/processed/original_data_backup.csv', index=False)\n",
    "    \n",
    "    # 1. Eliminar columnas innecesarias\n",
    "    columns_to_drop = [\n",
    "        'Time', 'Sector1SessionTime', 'Sector2SessionTime', 'Sector3SessionTime', \n",
    "        'PitOutTime', 'LapStartTime', 'LapStartDate', 'FastF1Generated', \n",
    "        'IsAccurate', 'DeletedReason'\n",
    "    ]\n",
    "    existing_columns = [col for col in columns_to_drop if col in data.columns]\n",
    "    \n",
    "    if existing_columns:\n",
    "        data = data.drop(columns=existing_columns)\n",
    "        print(f\"Eliminadas columnas: {', '.join(existing_columns)}\")\n",
    "    \n",
    "    # 2. Convertir LapTime y tiempos de sector a segundos\n",
    "    if 'LapTime' in data.columns and pd.api.types.is_timedelta64_dtype(data['LapTime']):\n",
    "        data['LapTime'] = data['LapTime'].dt.total_seconds()\n",
    "        print(\"Convertida LapTime a segundos.\")\n",
    "    \n",
    "    for sector in ['Sector1Time', 'Sector2Time', 'Sector3Time']:\n",
    "        if sector in data.columns and pd.api.types.is_timedelta64_dtype(data[sector]):\n",
    "            data[sector] = data[sector].dt.total_seconds()\n",
    "            print(f\"Convertido {sector} a segundos.\")\n",
    "    \n",
    "    # 3. Transformar PitInTime a LapToPit\n",
    "    if 'PitInTime' in data.columns:\n",
    "        # Crear nueva columna LapToPit basada en LapNumber donde PitInTime no es nulo\n",
    "        data['LapToPit'] = 0  # Valor por defecto\n",
    "        \n",
    "        # Para cada fila donde PitInTime no es nulo, establecer LapToPit = LapNumber\n",
    "        pit_mask = data['PitInTime'].notna()\n",
    "        if pit_mask.sum() > 0:\n",
    "            data.loc[pit_mask, 'LapToPit'] = data.loc[pit_mask, 'LapNumber']\n",
    "            \n",
    "        # Eliminar la columna PitInTime original\n",
    "        data = data.drop(columns=['PitInTime'])\n",
    "        print(f\"Transformada PitInTime a LapToPit. Detectadas {pit_mask.sum()} entradas a pit.\")\n",
    "    \n",
    "    # 4. Transformar Deleted a variable numérica (si existe)\n",
    "    if 'Deleted' in data.columns:\n",
    "        # Convertir a entero (False=0, True=1)\n",
    "        data['Deleted'] = data['Deleted'].astype(int)\n",
    "        print(f\"Convertida Deleted a valores 0/1. Hay {data['Deleted'].sum()} vueltas eliminadas.\")\n",
    "    \n",
    "    # 5. Transformar Team a valores numéricos\n",
    "    if 'Team' in data.columns:\n",
    "        # Guardar valores originales para ver exactamente qué nombres de equipos hay\n",
    "        team_values = data['Team'].value_counts()\n",
    "        print(f\"Valores originales de Team:\\n{team_values}\")\n",
    "        \n",
    "        # Mapeo de equipos a valores numéricos incluyendo todas las variantes de nombres\n",
    "        team_mapping = {\n",
    "            # Equipos con el nombre exacto como aparecen en los datos\n",
    "            'Alfa Romeo': 1,          # Kick Sauber\n",
    "            'AlphaTauri': 2,          # Racing Bulls\n",
    "            'Alpine': 3,              # Alpine\n",
    "            'Aston Martin': 4,        # Aston Martin\n",
    "            'Ferrari': 5,             # Ferrari\n",
    "            'Haas F1 Team': 6,        # Haas\n",
    "            'McLaren': 7,             # McLaren\n",
    "            'Mercedes': 8,            # Mercedes\n",
    "            'Red Bull Racing': 9,     # Red Bull\n",
    "            'Williams': 10,           # Williams\n",
    "            \n",
    "            # Nombres alternativos por si acaso\n",
    "            'Kick Sauber': 1,\n",
    "            'Racing Bulls': 2,\n",
    "            'Haas': 6,\n",
    "            'Red Bull': 9,\n",
    "            'RB': 2\n",
    "        }\n",
    "        \n",
    "        # Aplicar mapeo\n",
    "        data['TeamID'] = data['Team'].map(team_mapping)\n",
    "        \n",
    "        # Verificar si quedan equipos sin mapear\n",
    "        unmapped = data[data['TeamID'].isna()]['Team'].unique()\n",
    "        if len(unmapped) > 0:\n",
    "            print(f\"ADVERTENCIA: Equipos sin mapear: {unmapped}\")\n",
    "            # En caso de error, asignar valores secuenciales\n",
    "            next_id = max(team_mapping.values()) + 1\n",
    "            for team in unmapped:\n",
    "                team_mapping[team] = next_id\n",
    "                data.loc[data['Team'] == team, 'TeamID'] = next_id\n",
    "                print(f\"Asignado ID {next_id} a equipo desconocido: {team}\")\n",
    "                next_id += 1\n",
    "        else:\n",
    "            print(\"Todos los equipos mapeados correctamente.\")\n",
    "                \n",
    "        # Eliminar columna original después de verificar mapeo\n",
    "        data = data.drop(columns=['Team'])\n",
    "        print(f\"Transformada Team a TeamID con {len(set(team_mapping.values()))} valores únicos (1-10).\")\n",
    "    \n",
    "    # 6. Transformar NextCompound\n",
    "    if 'NextCompound' in data.columns:\n",
    "        # Crear mapeo\n",
    "        compound_mapping = {\n",
    "            'SOFT': 1,\n",
    "            'MEDIUM': 2,\n",
    "            'HARD': 3,\n",
    "            'INTERMEDIATE': 4,\n",
    "            'WET': 5\n",
    "        }\n",
    "        \n",
    "        # Guardar mapeo para referencia\n",
    "        if not data['NextCompound'].isna().all():\n",
    "            compound_values = data['NextCompound'].value_counts(dropna=False)\n",
    "            print(f\"Valores originales de NextCompound:\\n{compound_values}\")\n",
    "        \n",
    "        # Crear nueva columna con valores mapeados\n",
    "        data['NextCompoundID'] = data['NextCompound'].map(compound_mapping)\n",
    "        \n",
    "        # Rellenar NaN con 0 (sin cambio de compuesto)\n",
    "        data['NextCompoundID'] = data['NextCompoundID'].fillna(0).astype(int)\n",
    "        \n",
    "        # Eliminar columna original\n",
    "        data = data.drop(columns=['NextCompound'])\n",
    "        print(\"Transformada NextCompound a NextCompoundID (0=sin cambio, 1=soft, 2=medium, 3=hard, etc.)\")\n",
    "    \n",
    "    # 7. Manejar FreshTyreAfterStop\n",
    "    if 'FreshTyreAfterStop' in data.columns:\n",
    "        # Convertir a entero (False=0, True=1)\n",
    "        data['FreshTyreAfterStop'] = data['FreshTyreAfterStop'].fillna(0).astype(int)\n",
    "        print(\"Transformada FreshTyreAfterStop a valores 0/1 (0=no fresco o sin parada)\")\n",
    "    \n",
    "    # 8. Transformar Compound actual\n",
    "    if 'Compound' in data.columns:\n",
    "        # Usar el mismo mapeo que para NextCompound\n",
    "        compound_mapping = {\n",
    "            'SOFT': 1,\n",
    "            'MEDIUM': 2,\n",
    "            'HARD': 3,\n",
    "            'INTERMEDIATE': 4,\n",
    "            'WET': 5\n",
    "        }\n",
    "        \n",
    "        # Guardar para referencia\n",
    "        compound_values = data['Compound'].value_counts(dropna=False)\n",
    "        print(f\"Valores originales de Compound:\\n{compound_values}\")\n",
    "        \n",
    "        # Mapear y verificar valores faltantes\n",
    "        data['CompoundID'] = data['Compound'].map(compound_mapping)\n",
    "        \n",
    "        # Manejar valores no mapeados\n",
    "        unmapped = data[data['CompoundID'].isna()]['Compound'].unique()\n",
    "        if len(unmapped) > 0:\n",
    "            print(f\"ADVERTENCIA: Compuestos sin mapear: {unmapped}\")\n",
    "            # Asignar valores para compuestos no mapeados\n",
    "            next_id = max(compound_mapping.values()) + 1\n",
    "            for compound in unmapped:\n",
    "                compound_mapping[compound] = next_id\n",
    "                data.loc[data['Compound'] == compound, 'CompoundID'] = next_id\n",
    "                next_id += 1\n",
    "        \n",
    "        # Eliminar columna original\n",
    "        data = data.drop(columns=['Compound'])\n",
    "        print(\"Transformada Compound a CompoundID (1=soft, 2=medium, 3=hard, etc.)\")\n",
    "    \n",
    "    # 9. Antes de eliminar outliers, guardar estos datos en un DataFrame separado\n",
    "    if 'LapTime' in data.columns:\n",
    "        # Identificar outliers (vueltas muy rápidas o muy lentas)\n",
    "        q1 = data['LapTime'].quantile(0.05)\n",
    "        q3 = data['LapTime'].quantile(0.95)\n",
    "        \n",
    "        # Datos que se considerarían outliers\n",
    "        outlier_data = data[(data['LapTime'] < q1) | (data['LapTime'] > q3)].copy()\n",
    "        \n",
    "        # Añadir columna para clasificar el tipo de outlier\n",
    "        outlier_data['OutlierType'] = 'Unknown'\n",
    "        outlier_data.loc[outlier_data['LapTime'] < q1, 'OutlierType'] = 'VeryFast'\n",
    "        outlier_data.loc[outlier_data['LapTime'] > q3, 'OutlierType'] = 'VerySlow'\n",
    "        \n",
    "        # Guardar para uso futuro en estrategias\n",
    "        outlier_data.to_csv('../f1-strategy/data/raw/processed/exceptional_laps.csv', index=False)\n",
    "        print(f\"Guardados {len(outlier_data)} registros de vueltas excepcionales para análisis estratégico.\")\n",
    "        \n",
    "        # Continuar con el filtrado para el modelo predictivo\n",
    "        data = data[(data['LapTime'] >= q1) & (data['LapTime'] <= q3)]\n",
    "        print(f\"Filtrados outliers para el modelo predictivo. Rango válido: {q1:.2f}s - {q3:.2f}s\")\n",
    "    \n",
    "    # 10. Guardar versión limpia para referencia\n",
    "    data.to_csv('../f1-strategy/data/raw/processed/cleaned_data.csv', index=False)\n",
    "    print(f\"Guardado dataset limpio con {data.shape[0]} filas y {data.shape[1]} columnas.\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la limpieza de datos\n",
    "cleaned_data = clean_time_data(merged_data)\n",
    "\n",
    "# Mostrar las dimensiones antes y después\n",
    "print(f\"Dimensiones antes de limpieza: {merged_data.shape}\")\n",
    "print(f\"Dimensiones después de limpieza: {cleaned_data.shape}\")\n",
    "\n",
    "display(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Transformation Strategy\n",
    "\n",
    "## Columns Removed\n",
    "\n",
    "We removed several columns from the dataset to improve model performance and reduce dimensionality:\n",
    "\n",
    "1. **Time-related columns**:\n",
    "   - `Time`: Redundant timestamp information that doesn't provide predictive value\n",
    "   - `Sector1SessionTime`, `Sector2SessionTime`, `Sector3SessionTime`: Absolute timing information that's not relevant for lap time prediction\n",
    "   - `LapStartTime`, `LapStartDate`: Absolute timing that doesn't impact lap performance\n",
    "   - `PitOutTime`: 100% missing values, no usable information\n",
    "\n",
    "2. **Quality/metadata columns**:\n",
    "   - `FastF1Generated`: Metadata about data source, not a race performance factor\n",
    "   - `IsAccurate`: Data quality flag that doesn't impact predictive modeling\n",
    "   - `DeletedReason`: Only contained \"track limits\" information which is already captured in the `Deleted` flag\n",
    "\n",
    "## Columns Transformed\n",
    "\n",
    "We transformed several columns to improve their utility for machine learning:\n",
    "\n",
    "1. **Time conversions**:\n",
    "   - `LapTime`, `Sector1Time`, `Sector2Time`, `Sector3Time`: Converted from timedelta objects to seconds (float) for direct mathematical operations\n",
    "\n",
    "2. **Pit stop information**:\n",
    "   - `PitInTime` → `LapToPit`: Converted from timestamp to binary indicator (0 = no pit, actual lap number = pit entry) to represent when a driver entered the pits\n",
    "   - `NextCompound` → `NextCompoundID`: Mapped compound names to integers (0 = no change, 1 = soft, 2 = medium, 3 = hard, etc.)\n",
    "   - `FreshTyreAfterStop`: Filled NaN values with 0 (no fresh tire) and converted to integer (0/1)\n",
    "\n",
    "3. **Categorical conversions**:\n",
    "   - `Team` → `TeamID`: Mapped team names to integers (1-10) following the team order in the championship\n",
    "   - `Compound` → `CompoundID`: Mapped tire compounds to integers (1 = soft, 2 = medium, 3 = hard, etc.)\n",
    "   - `Deleted`: Converted boolean to integer (0/1)\n",
    "\n",
    "## Outlier Handling\n",
    "\n",
    "We implemented a robust outlier detection and handling strategy:\n",
    "\n",
    "1. Identified outliers in lap times using the 5th and 95th percentiles\n",
    "2. Classified outliers as \"VeryFast\" or \"VerySlow\" laps\n",
    "3. Stored outliers separately for potential strategic analysis\n",
    "4. Removed outliers from the training dataset to improve model quality\n",
    "\n",
    "## Data Preservation\n",
    "\n",
    "Throughout the cleaning process, we maintained data integrity by:\n",
    "\n",
    "1. Working with copies of the original dataframe\n",
    "2. Saving the original data before modifications\n",
    "3. Documenting all transformations with clear logging\n",
    "4. Saving both the exceptional laps and cleaned datasets for future reference\n",
    "\n",
    "These transformations significantly improve the dataset's suitability for machine learning while preserving the essential racing performance information needed for accurate lap time prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Create features related with tyres and impact on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tyre_features(df):\n",
    "    \"\"\"\n",
    "    Crea características relacionadas con neumáticos y su impacto en el rendimiento\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con datos limpios\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con nuevas características de neumáticos\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # 1. Edad de los neumáticos\n",
    "    if 'TyreLife' in data.columns:\n",
    "        data['TyreAge'] = data['TyreLife']\n",
    "        print(\"Creada feature: TyreAge\")\n",
    "    \n",
    "    # 2. Cambio de posición (comparado con la vuelta anterior)\n",
    "    if 'Position' in data.columns and 'Driver' in data.columns:\n",
    "        data['PositionChange'] = data.groupby('Driver')['Position'].diff().fillna(0)\n",
    "        print(\"Creada feature: PositionChange\")\n",
    "    \n",
    "    # 3. Carga de combustible (aproximación basada en la vuelta)\n",
    "    if 'LapNumber' in data.columns:\n",
    "        max_lap = data['LapNumber'].max()\n",
    "        data['FuelLoad'] = 1 - (data['LapNumber'] / max_lap).round(4)  # Aproximación simple\n",
    "        print(\"Creada feature: FuelLoad (aproximación)\")\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creada feature: TyreAge\n",
      "Creada feature: PositionChange\n",
      "Creada feature: FuelLoad (aproximación)\n",
      "Nuevas columnas creadas: {'TyreAge', 'FuelLoad', 'PositionChange'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Driver",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DriverNumber",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "LapTime",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LapNumber",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Stint",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Sector1Time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Sector2Time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Sector3Time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SpeedI1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SpeedI2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SpeedFL",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SpeedST",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "IsPersonalBest",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "TyreLife",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "FreshTyre",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "TrackStatus",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Position",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Deleted",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AirTemp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Humidity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pressure",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Rainfall",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "TrackTemp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "WindDirection",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "WindSpeed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "FreshTyreAfterStop",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PitNextLap",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "LapsSincePitStop",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DRSUsed",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "UndercutWindow",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "IsLapped",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GapToLeader",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LapToPit",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TeamID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NextCompoundID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CompoundID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TyreAge",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PositionChange",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "FuelLoad",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "551e5186-fc7c-41c9-9417-c51e34f0495f",
       "rows": [
        [
         "0",
         "VER",
         "1",
         "83.935",
         "1.0",
         "1.0",
         null,
         "32.084",
         "23.926",
         "256.0",
         "261.0",
         "276.0",
         "275.0",
         "False",
         "1.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "1.0",
         "0.0",
         "0.9848"
        ],
        [
         "1",
         "VER",
         "1",
         "80.402",
         "2.0",
         "1.0",
         "24.186",
         "32.088",
         "24.128",
         "252.0",
         "257.0",
         "276.0",
         "295.0",
         "True",
         "2.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "2",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "2.0",
         "0.0",
         "0.9697"
        ],
        [
         "2",
         "VER",
         "1",
         "80.499",
         "3.0",
         "1.0",
         "24.167",
         "32.191",
         "24.141",
         "249.0",
         "256.0",
         "276.0",
         "297.0",
         "False",
         "3.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "3",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "3.0",
         "0.0",
         "0.9545"
        ],
        [
         "3",
         "VER",
         "1",
         "80.346",
         "4.0",
         "1.0",
         "24.022",
         "32.159",
         "24.165",
         "255.0",
         "256.0",
         "276.0",
         "300.0",
         "True",
         "4.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "4",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "4.0",
         "0.0",
         "0.9394"
        ],
        [
         "4",
         "VER",
         "1",
         "80.283",
         "5.0",
         "1.0",
         "24.034",
         "32.213",
         "24.036",
         "254.0",
         "256.0",
         "277.0",
         "301.0",
         "True",
         "5.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "5",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "5.0",
         "0.0",
         "0.9242"
        ],
        [
         "5",
         "VER",
         "1",
         "80.402",
         "6.0",
         "1.0",
         "24.009",
         "32.178",
         "24.215",
         "255.0",
         "253.0",
         "276.0",
         null,
         "False",
         "6.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "6",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "6.0",
         "0.0",
         "0.9091"
        ],
        [
         "6",
         "VER",
         "1",
         "80.476",
         "7.0",
         "1.0",
         "24.092",
         "32.224",
         "24.16",
         "258.0",
         "259.0",
         "278.0",
         null,
         "False",
         "7.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "7",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "7.0",
         "0.0",
         "0.8939"
        ],
        [
         "7",
         "VER",
         "1",
         "79.894",
         "8.0",
         "1.0",
         "23.827",
         "31.98",
         "24.087",
         "256.0",
         "264.0",
         "276.0",
         null,
         "True",
         "8.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "8",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "8.0",
         "0.0",
         "0.8788"
        ],
        [
         "8",
         "VER",
         "1",
         "80.282",
         "9.0",
         "1.0",
         "23.888",
         "32.18",
         "24.214",
         "259.0",
         "264.0",
         "276.0",
         null,
         "False",
         "9.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "9",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "9.0",
         "0.0",
         "0.8636"
        ],
        [
         "9",
         "VER",
         "1",
         "80.334",
         "10.0",
         "1.0",
         "23.937",
         "32.197",
         "24.2",
         "261.0",
         "265.0",
         "277.0",
         "302.0",
         "False",
         "10.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "10",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "10.0",
         "0.0",
         "0.8485"
        ],
        [
         "10",
         "VER",
         "1",
         "80.52",
         "11.0",
         "1.0",
         "24.003",
         "32.32",
         "24.197",
         "258.0",
         "263.0",
         "277.0",
         null,
         "False",
         "11.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "11",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "11.0",
         "0.0",
         "0.8333"
        ],
        [
         "11",
         "VER",
         "1",
         "80.442",
         "12.0",
         "1.0",
         "23.952",
         "32.215",
         "24.275",
         "261.0",
         "267.0",
         "278.0",
         null,
         "False",
         "12.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "12",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "12.0",
         "0.0",
         "0.8182"
        ],
        [
         "12",
         "VER",
         "1",
         "80.178",
         "13.0",
         "1.0",
         "23.794",
         "32.193",
         "24.191",
         "262.0",
         "266.0",
         "278.0",
         "303.0",
         "False",
         "13.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "13",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "13.0",
         "0.0",
         "0.8029999999999999"
        ],
        [
         "13",
         "VER",
         "1",
         "80.378",
         "14.0",
         "1.0",
         "24.004",
         "32.222",
         "24.152",
         "257.0",
         "264.0",
         "278.0",
         null,
         "False",
         "14.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "14",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "14.0",
         "0.0",
         "0.7879"
        ],
        [
         "14",
         "VER",
         "1",
         "80.282",
         "15.0",
         "1.0",
         "23.94",
         "32.176",
         "24.166",
         "260.0",
         "265.0",
         "278.0",
         null,
         "False",
         "15.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "15",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "15.0",
         "0.0",
         "0.7726999999999999"
        ],
        [
         "15",
         "VER",
         "1",
         "80.186",
         "16.0",
         "1.0",
         "23.889",
         "32.116",
         "24.181",
         "261.0",
         "270.0",
         "277.0",
         null,
         "False",
         "16.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "16",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "16.0",
         "0.0",
         "0.7576"
        ],
        [
         "16",
         "VER",
         "1",
         "80.344",
         "17.0",
         "1.0",
         "23.948",
         "32.182",
         "24.214",
         null,
         "268.0",
         "278.0",
         null,
         "False",
         "17.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "17",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "17.0",
         "0.0",
         "0.7424"
        ],
        [
         "17",
         "VER",
         "1",
         "80.258",
         "18.0",
         "1.0",
         "23.839",
         "32.188",
         "24.231",
         "262.0",
         "267.0",
         "277.0",
         null,
         "False",
         "18.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "18",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "18.0",
         "0.0",
         "0.7273000000000001"
        ],
        [
         "18",
         "VER",
         "1",
         "80.068",
         "19.0",
         "1.0",
         "23.835",
         "32.145",
         "24.088",
         "260.0",
         "268.0",
         "279.0",
         null,
         "False",
         "19.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "19",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "19.0",
         "0.0",
         "0.7121"
        ],
        [
         "19",
         "VER",
         "1",
         "79.988",
         "20.0",
         "1.0",
         "23.762",
         "32.13",
         "24.096",
         "269.0",
         "264.0",
         "279.0",
         "304.0",
         "False",
         "20.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "20",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "20.0",
         "0.0",
         "0.6970000000000001"
        ],
        [
         "20",
         "VER",
         "1",
         "80.087",
         "21.0",
         "1.0",
         "23.772",
         "32.151",
         "24.164",
         "268.0",
         "266.0",
         "278.0",
         "305.0",
         "False",
         "21.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "21",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "21.0",
         "0.0",
         "0.6818"
        ],
        [
         "21",
         "VER",
         "1",
         "79.869",
         "22.0",
         "1.0",
         "23.758",
         "32.035",
         "24.076",
         "265.0",
         "269.0",
         "279.0",
         null,
         "True",
         "22.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "22",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "22.0",
         "0.0",
         "0.6667000000000001"
        ],
        [
         "22",
         "VER",
         "1",
         "80.098",
         "23.0",
         "1.0",
         "23.821",
         "32.068",
         "24.209",
         null,
         "272.0",
         "279.0",
         "304.0",
         "False",
         "23.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "23",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "23.0",
         "0.0",
         "0.6515"
        ],
        [
         "23",
         "VER",
         "1",
         "79.946",
         "24.0",
         "1.0",
         "23.67",
         "32.188",
         "24.088",
         "263.0",
         "263.0",
         "279.0",
         "306.0",
         "False",
         "24.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "24",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "24.0",
         "0.0",
         "0.6364000000000001"
        ],
        [
         "24",
         "VER",
         "1",
         "79.973",
         "25.0",
         "1.0",
         "23.738",
         "32.151",
         "24.084",
         "262.0",
         "264.0",
         "279.0",
         "305.0",
         "False",
         "25.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "25",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "2",
         "25.0",
         "0.0",
         "0.6212"
        ],
        [
         "25",
         "VER",
         "1",
         "83.843",
         "26.0",
         "1.0",
         "23.774",
         "32.091",
         "27.978",
         "265.0",
         "267.0",
         null,
         null,
         "False",
         "26.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "1",
         "1",
         "26",
         "0",
         "0",
         "0",
         "0.0",
         "26",
         "9",
         "2",
         "2",
         "26.0",
         "0.0",
         "0.6061000000000001"
        ],
        [
         "27",
         "VER",
         "1",
         "78.707",
         "28.0",
         "2.0",
         "23.713",
         "31.496",
         "23.498",
         "260.0",
         "261.0",
         "279.0",
         "302.0",
         "True",
         "2.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "2",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "2.0",
         "0.0",
         "0.5758"
        ],
        [
         "28",
         "VER",
         "1",
         "78.86",
         "29.0",
         "2.0",
         "23.579",
         "31.646",
         "23.635",
         "259.0",
         "255.0",
         "279.0",
         "301.0",
         "False",
         "3.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "3",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "3.0",
         "0.0",
         "0.5606"
        ],
        [
         "29",
         "VER",
         "1",
         "78.976",
         "30.0",
         "2.0",
         "23.717",
         "31.619",
         "23.64",
         "258.0",
         "260.0",
         "279.0",
         "303.0",
         "False",
         "4.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "4",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "4.0",
         "0.0",
         "0.5455"
        ],
        [
         "30",
         "VER",
         "1",
         "79.165",
         "31.0",
         "2.0",
         "23.806",
         "31.687",
         "23.672",
         "257.0",
         "263.0",
         "281.0",
         null,
         "False",
         "5.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "5",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "5.0",
         "0.0",
         "0.5303"
        ],
        [
         "31",
         "VER",
         "1",
         "78.878",
         "32.0",
         "2.0",
         "23.52",
         "31.691",
         "23.667",
         "260.0",
         "262.0",
         "280.0",
         "304.0",
         "False",
         "6.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "6",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "6.0",
         "0.0",
         "0.5152"
        ],
        [
         "32",
         "VER",
         "1",
         "79.189",
         "33.0",
         "2.0",
         "23.678",
         "31.746",
         "23.765",
         null,
         "260.0",
         "280.0",
         null,
         "False",
         "7.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "7",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "7.0",
         "0.0",
         "0.5"
        ],
        [
         "33",
         "VER",
         "1",
         "78.806",
         "34.0",
         "2.0",
         "23.501",
         "31.599",
         "23.706",
         "261.0",
         "264.0",
         "281.0",
         "305.0",
         "False",
         "8.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "8",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "8.0",
         "0.0",
         "0.4848"
        ],
        [
         "34",
         "VER",
         "1",
         "78.89",
         "35.0",
         "2.0",
         "23.533",
         "31.641",
         "23.716",
         "265.0",
         "263.0",
         "280.0",
         null,
         "False",
         "9.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "9",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "9.0",
         "0.0",
         "0.4697"
        ],
        [
         "35",
         "VER",
         "1",
         "78.789",
         "36.0",
         "2.0",
         "23.49",
         "31.479",
         "23.82",
         "270.0",
         "268.0",
         "282.0",
         "306.0",
         "False",
         "10.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "10",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "10.0",
         "0.0",
         "0.4545"
        ],
        [
         "36",
         "VER",
         "1",
         "79.026",
         "37.0",
         "2.0",
         "23.422",
         "31.723",
         "23.881",
         "264.0",
         "266.0",
         "283.0",
         "307.0",
         "False",
         "11.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "11",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "11.0",
         "0.0",
         "0.4394"
        ],
        [
         "37",
         "VER",
         "1",
         "79.417",
         "38.0",
         "2.0",
         "23.483",
         "31.855",
         "24.079",
         "263.0",
         "270.0",
         "282.0",
         "309.0",
         "False",
         "12.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "12",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "12.0",
         "0.0",
         "0.4242"
        ],
        [
         "38",
         "VER",
         "1",
         "79.322",
         "39.0",
         "2.0",
         "23.636",
         "31.688",
         "23.998",
         "272.0",
         "273.0",
         "282.0",
         "310.0",
         "False",
         "13.0",
         "True",
         "1",
         "1.0",
         "1",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "13",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "13.0",
         "0.0",
         "0.4091"
        ],
        [
         "39",
         "VER",
         "1",
         "79.388",
         "40.0",
         "2.0",
         "23.53",
         "31.967",
         "23.891",
         "264.0",
         "266.0",
         "281.0",
         "309.0",
         "False",
         "14.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "14",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "14.0",
         "0.0",
         "0.39390000000000003"
        ],
        [
         "40",
         "VER",
         "1",
         "79.53",
         "41.0",
         "2.0",
         "23.603",
         "32.057",
         "23.87",
         "257.0",
         "267.0",
         "282.0",
         "308.0",
         "False",
         "15.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "15",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "15.0",
         "0.0",
         "0.3788"
        ],
        [
         "41",
         "VER",
         "1",
         "79.092",
         "42.0",
         "2.0",
         "23.452",
         "31.802",
         "23.838",
         "267.0",
         "265.0",
         "282.0",
         "310.0",
         "False",
         "16.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "16",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "16.0",
         "0.0",
         "0.36360000000000003"
        ],
        [
         "42",
         "VER",
         "1",
         "78.912",
         "43.0",
         "2.0",
         "23.399",
         "31.699",
         "23.814",
         "269.0",
         "272.0",
         "281.0",
         "309.0",
         "False",
         "17.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "17",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "17.0",
         "0.0",
         "0.34850000000000003"
        ],
        [
         "43",
         "VER",
         "1",
         "78.97",
         "44.0",
         "2.0",
         "23.397",
         "31.775",
         "23.798",
         "268.0",
         "269.0",
         "281.0",
         "308.0",
         "False",
         "18.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "18",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "18.0",
         "0.0",
         "0.33330000000000004"
        ],
        [
         "44",
         "VER",
         "1",
         "79.104",
         "45.0",
         "2.0",
         "23.437",
         "31.771",
         "23.896",
         "269.0",
         "268.0",
         "281.0",
         "307.0",
         "False",
         "19.0",
         "True",
         "1",
         "1.0",
         "1",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "19",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "19.0",
         "0.0",
         "0.31820000000000004"
        ],
        [
         "45",
         "VER",
         "1",
         "78.84",
         "46.0",
         "2.0",
         "23.373",
         "31.692",
         "23.775",
         "271.0",
         "273.0",
         "282.0",
         null,
         "False",
         "20.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "20",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "20.0",
         "0.0",
         "0.30300000000000005"
        ],
        [
         "46",
         "VER",
         "1",
         "79.109",
         "47.0",
         "2.0",
         "23.468",
         "31.738",
         "23.903",
         "268.0",
         "271.0",
         "282.0",
         "308.0",
         "False",
         "21.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "21",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "21.0",
         "0.0",
         "0.28790000000000004"
        ],
        [
         "47",
         "VER",
         "1",
         "78.953",
         "48.0",
         "2.0",
         "23.344",
         "31.884",
         "23.725",
         null,
         "270.0",
         "283.0",
         null,
         "False",
         "22.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "22",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "22.0",
         "0.0",
         "0.27270000000000005"
        ],
        [
         "48",
         "VER",
         "1",
         "79.093",
         "49.0",
         "2.0",
         "23.434",
         "31.793",
         "23.866",
         "269.0",
         "271.0",
         "283.0",
         "310.0",
         "False",
         "23.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "23",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "23.0",
         "0.0",
         "0.25760000000000005"
        ],
        [
         "49",
         "VER",
         "1",
         "79.221",
         "50.0",
         "2.0",
         "23.587",
         "31.739",
         "23.895",
         "274.0",
         "270.0",
         "282.0",
         null,
         "False",
         "24.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "24",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "24.0",
         "0.0",
         "0.24239999999999995"
        ],
        [
         "50",
         "VER",
         "1",
         "78.768",
         "51.0",
         "2.0",
         "23.256",
         "31.693",
         "23.819",
         null,
         "269.0",
         "284.0",
         "309.0",
         "False",
         "25.0",
         "True",
         "1",
         "1.0",
         "0",
         "23.5",
         "59.0",
         "1000.9",
         "False",
         "38.5",
         "238",
         "2.2",
         "0",
         "0",
         "25",
         "0",
         "0",
         "0",
         "0.0",
         "0",
         "9",
         "0",
         "3",
         "25.0",
         "0.0",
         "0.22729999999999995"
        ]
       ],
       "shape": {
        "columns": 39,
        "rows": 1180
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Driver</th>\n",
       "      <th>DriverNumber</th>\n",
       "      <th>LapTime</th>\n",
       "      <th>LapNumber</th>\n",
       "      <th>Stint</th>\n",
       "      <th>Sector1Time</th>\n",
       "      <th>Sector2Time</th>\n",
       "      <th>Sector3Time</th>\n",
       "      <th>SpeedI1</th>\n",
       "      <th>SpeedI2</th>\n",
       "      <th>...</th>\n",
       "      <th>UndercutWindow</th>\n",
       "      <th>IsLapped</th>\n",
       "      <th>GapToLeader</th>\n",
       "      <th>LapToPit</th>\n",
       "      <th>TeamID</th>\n",
       "      <th>NextCompoundID</th>\n",
       "      <th>CompoundID</th>\n",
       "      <th>TyreAge</th>\n",
       "      <th>PositionChange</th>\n",
       "      <th>FuelLoad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VER</td>\n",
       "      <td>1</td>\n",
       "      <td>83.935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.084</td>\n",
       "      <td>23.926</td>\n",
       "      <td>256.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VER</td>\n",
       "      <td>1</td>\n",
       "      <td>80.402</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.186</td>\n",
       "      <td>32.088</td>\n",
       "      <td>24.128</td>\n",
       "      <td>252.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VER</td>\n",
       "      <td>1</td>\n",
       "      <td>80.499</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.167</td>\n",
       "      <td>32.191</td>\n",
       "      <td>24.141</td>\n",
       "      <td>249.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VER</td>\n",
       "      <td>1</td>\n",
       "      <td>80.346</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.022</td>\n",
       "      <td>32.159</td>\n",
       "      <td>24.165</td>\n",
       "      <td>255.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VER</td>\n",
       "      <td>1</td>\n",
       "      <td>80.283</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.034</td>\n",
       "      <td>32.213</td>\n",
       "      <td>24.036</td>\n",
       "      <td>254.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>SAR</td>\n",
       "      <td>2</td>\n",
       "      <td>81.280</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.354</td>\n",
       "      <td>32.587</td>\n",
       "      <td>24.339</td>\n",
       "      <td>265.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47.568484</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>SAR</td>\n",
       "      <td>2</td>\n",
       "      <td>82.134</td>\n",
       "      <td>62.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.675</td>\n",
       "      <td>33.473</td>\n",
       "      <td>24.986</td>\n",
       "      <td>271.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47.208525</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>SAR</td>\n",
       "      <td>2</td>\n",
       "      <td>80.420</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.634</td>\n",
       "      <td>32.486</td>\n",
       "      <td>24.300</td>\n",
       "      <td>264.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46.646990</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>SAR</td>\n",
       "      <td>2</td>\n",
       "      <td>79.980</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.602</td>\n",
       "      <td>32.127</td>\n",
       "      <td>24.251</td>\n",
       "      <td>279.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48.122065</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>SAR</td>\n",
       "      <td>2</td>\n",
       "      <td>80.254</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.633</td>\n",
       "      <td>32.186</td>\n",
       "      <td>24.435</td>\n",
       "      <td>274.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47.829628</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1180 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Driver DriverNumber  LapTime  LapNumber  Stint  Sector1Time  Sector2Time  \\\n",
       "0       VER            1   83.935        1.0    1.0          NaN       32.084   \n",
       "1       VER            1   80.402        2.0    1.0       24.186       32.088   \n",
       "2       VER            1   80.499        3.0    1.0       24.167       32.191   \n",
       "3       VER            1   80.346        4.0    1.0       24.022       32.159   \n",
       "4       VER            1   80.283        5.0    1.0       24.034       32.213   \n",
       "...     ...          ...      ...        ...    ...          ...          ...   \n",
       "1307    SAR            2   81.280       61.0    3.0       24.354       32.587   \n",
       "1308    SAR            2   82.134       62.0    3.0       23.675       33.473   \n",
       "1309    SAR            2   80.420       63.0    3.0       23.634       32.486   \n",
       "1310    SAR            2   79.980       64.0    3.0       23.602       32.127   \n",
       "1311    SAR            2   80.254       65.0    3.0       23.633       32.186   \n",
       "\n",
       "      Sector3Time  SpeedI1  SpeedI2  ...  UndercutWindow  IsLapped  \\\n",
       "0          23.926    256.0    261.0  ...               0         0   \n",
       "1          24.128    252.0    257.0  ...               0         0   \n",
       "2          24.141    249.0    256.0  ...               0         0   \n",
       "3          24.165    255.0    256.0  ...               0         0   \n",
       "4          24.036    254.0    256.0  ...               0         0   \n",
       "...           ...      ...      ...  ...             ...       ...   \n",
       "1307       24.339    265.0    265.0  ...               0         1   \n",
       "1308       24.986    271.0    192.0  ...               0         1   \n",
       "1309       24.300    264.0    273.0  ...               0         1   \n",
       "1310       24.251    279.0    278.0  ...               0         1   \n",
       "1311       24.435    274.0    285.0  ...               0         1   \n",
       "\n",
       "      GapToLeader  LapToPit  TeamID NextCompoundID  CompoundID  TyreAge  \\\n",
       "0        0.000000         0       9              0           2      1.0   \n",
       "1        0.000000         0       9              0           2      2.0   \n",
       "2        0.000000         0       9              0           2      3.0   \n",
       "3        0.000000         0       9              0           2      4.0   \n",
       "4        0.000000         0       9              0           2      5.0   \n",
       "...           ...       ...     ...            ...         ...      ...   \n",
       "1307    47.568484         0      10              0           3     25.0   \n",
       "1308    47.208525         0      10              0           3     26.0   \n",
       "1309    46.646990         0      10              0           3     27.0   \n",
       "1310    48.122065         0      10              0           3     28.0   \n",
       "1311    47.829628         0      10              0           3     29.0   \n",
       "\n",
       "      PositionChange  FuelLoad  \n",
       "0                0.0    0.9848  \n",
       "1                0.0    0.9697  \n",
       "2                0.0    0.9545  \n",
       "3                0.0    0.9394  \n",
       "4                0.0    0.9242  \n",
       "...              ...       ...  \n",
       "1307             1.0    0.0758  \n",
       "1308             0.0    0.0606  \n",
       "1309             0.0    0.0455  \n",
       "1310             0.0    0.0303  \n",
       "1311             0.0    0.0152  \n",
       "\n",
       "[1180 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear características de neumáticos\n",
    "cleaned_tyre_features_data = create_tyre_features(cleaned_data)\n",
    "\n",
    "# Mostrar nuevas columnas\n",
    "new_columns = set(cleaned_tyre_features_data.columns) - set(cleaned_data.columns)\n",
    "print(f\"Nuevas columnas creadas: {new_columns}\")\n",
    "\n",
    "display(cleaned_tyre_features_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_colors = {\n",
    "    1: 'red',     # SOFT\n",
    "    2: 'yellow',  # MEDIUM\n",
    "    3: 'gray',    # HARD\n",
    "    4: 'green',   # INTERMEDIATE\n",
    "    5: 'blue'     # WET\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualizar la relación entre edad de neumáticos y tiempo por vuelta\n",
    "if 'TyreAge' in cleaned_tyre_features_data.columns and 'Compound' in cleaned_tyre_features_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Filtrar por compuestos principales\n",
    "    for compound in cleaned_tyre_features_data['Compound'].unique():\n",
    "        subset = cleaned_tyre_features_data[cleaned_tyre_features_data['Compound'] == compound]\n",
    "        # Agrupar por edad de neumático y calcular promedio\n",
    "        agg_data = subset.groupby('TyreAge')['LapTime'].mean().reset_index()\n",
    "        \n",
    "        # Usar el color correspondiente del diccionario\n",
    "        color = compound_colors.get(compound, 'black')  # 'black' como color por defecto\n",
    "        plt.plot(agg_data['TyreAge'], agg_data['LapTime'], 'o-', \n",
    "                 color=color, label=f'Compuesto {compound}')\n",
    "    \n",
    "    plt.xlabel('Edad del Neumático (vueltas)')\n",
    "    plt.ylabel('Tiempo por Vuelta (s)')\n",
    "    plt.title('Degradación de Neumáticos: Efecto en el Tiempo por Vuelta')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('../outputs/week3/tyre_degradation_colored.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tire Degradation Analysis\n",
    "\n",
    "This plot reveals how tire age affects lap times across different compounds. \n",
    "\n",
    "The SOFT compound (green) generally produces slower lap times but shows inconsistent degradation patterns with several performance spikes. \n",
    "\n",
    "MEDIUM tires (blue) deliver strong initial performance but gradually degrade. \n",
    "\n",
    "HARD tires (orange) demonstrate superior longevity, becoming the fastest option after approximately 35 laps. \n",
    "\n",
    "This visualization confirms the classic F1 tire performance trade-off: softer compounds offer initial speed but degrade faster, while harder compounds provide durability at the expense of initial performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe with features related to strategies and gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_strategy_features(df):\n",
    "    \"\"\"\n",
    "    Crea características relacionadas con estrategia y gaps entre coches\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con datos básicos y características de neumáticos\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con características estratégicas añadidas\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # 1. DRS usado (si está disponible)\n",
    "    if 'drs_window' in data.columns:\n",
    "        data['DRSUsed'] = data['drs_window'].astype(int)\n",
    "        print(\"Creada feature: DRSUsed\")\n",
    "    \n",
    "    # 2. Ventana de undercut (si está disponible)\n",
    "    if 'undercut_window' in data.columns:\n",
    "        data['UndercutWindow'] = data['undercut_window'].astype(int)\n",
    "        print(\"Creada feature: UndercutWindow\")\n",
    "    \n",
    "    # 3. Gap al líder (si está disponible)\n",
    "    if 'gap_to_leader_numeric' in data.columns:\n",
    "        # Convertir a float por si acaso\n",
    "        data['GapToLeader'] = pd.to_numeric(data['gap_to_leader_numeric'], errors='coerce')\n",
    "        print(\"Creada feature: GapToLeader\")\n",
    "    \n",
    "    # 4. Piloto en vuelta perdida (si está disponible)\n",
    "    if 'is_lapped' in data.columns:\n",
    "        data['IsLapped'] = data['is_lapped'].astype(int)\n",
    "        print(\"Creada feature: IsLapped\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear características estratégicas\n",
    "strategy_data = create_strategy_features(tyre_features_data)\n",
    "\n",
    "# Mostrar nuevas columnas\n",
    "new_columns = set(strategy_data.columns) - set(tyre_features_data.columns)\n",
    "print(f\"Nuevas columnas creadas: {new_columns}\")\n",
    "\n",
    "\n",
    "strategy_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos la característica DRSUsed, visualizar su impacto\n",
    "if 'DRSUsed' in strategy_data.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x='DRSUsed', y='LapTime', data=strategy_data)\n",
    "    plt.title('Impacto del DRS en Tiempos por Vuelta')\n",
    "    plt.xlabel('DRS Usado (0=No, 1=Sí)')\n",
    "    plt.ylabel('Tiempo por Vuelta (s)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('../outputs/week3/drs_impact.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRS Impact Analysis\n",
    "\n",
    "Surprisingly, this boxplot shows virtually no difference in lap times between when DRS is used (1) versus when it's not available (0). This counterintuitive result warrants further investigation, as DRS typically provides a 0.5-1.0 second advantage on suitable circuits. \n",
    "\n",
    "Possible explanations include: (1) the circuit has few DRS zones, (2) data collection issues, (3) the effect is masked by other variables, or (4) the DRS benefit appears primarily in overtaking scenarios rather than overall lap times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis del impacto del DRS en el GP de Barcelona\n",
    "\n",
    "El Circuito de Barcelona-Catalunya tiene características particulares que afectan significativamente el uso del DRS:\n",
    "\n",
    "![Circuito de Barcelona-Catalunya](ruta/a/tu/imagen.jpg)\n",
    "\n",
    "*Fuente: [Nombre de la fuente, como F1 oficial]*\n",
    "\n",
    "Las zonas de DRS están ubicadas en:\n",
    "1. La recta principal después de la última curva\n",
    "2. Entre las curvas 9 y 10\n",
    "\n",
    "El impacto del DRS en este circuito es especialmente relevante porque...\n",
    "[Aquí añadirías tu explicación sobre el impacto del DRS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los tiempos por vuelta para cada piloto\n",
    "lap_times = strategy_data.groupby(['LapNumber', 'Driver'])['LapTime'].mean().reset_index()\n",
    "\n",
    "# Convertir a segundos si es necesario (en caso de que sea un objeto timedelta)\n",
    "if pd.api.types.is_timedelta64_dtype(lap_times['LapTime']):\n",
    "    lap_times['LapTime'] = lap_times['LapTime'].dt.total_seconds()\n",
    "\n",
    "# Graficar los tiempos por vuelta para cada piloto\n",
    "plt.figure(figsize=(14, 7))\n",
    "for driver in ['VER', 'HAM', 'RUS']:\n",
    "    driver_data = lap_times[lap_times['Driver'] == driver]\n",
    "    if not driver_data.empty:\n",
    "        plt.plot(driver_data['LapNumber'], driver_data['LapTime'], \n",
    "                 marker='o', markersize=3, linewidth=2, label=driver)\n",
    "\n",
    "plt.title('Tiempo por Vuelta por Piloto')\n",
    "plt.xlabel('Número de Vuelta')\n",
    "plt.ylabel('Tiempo por Vuelta (s)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Añadir una línea horizontal en el tiempo de vuelta \"ideal\" para referencia visual\n",
    "min_laptime = lap_times['LapTime'].min()\n",
    "plt.axhline(y=min_laptime, color='gray', linestyle='--', alpha=0.5, \n",
    "            label=f'Mejor tiempo: {min_laptime:.2f}s')\n",
    "\n",
    "# Ajustar rango del eje Y para mejor visualización (excluyendo valores extremos)\n",
    "q1 = lap_times['LapTime'].quantile(0.05)\n",
    "q3 = lap_times['LapTime'].quantile(0.95)\n",
    "plt.ylim(q1 * 0.95, q3 * 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/week3/lap_times.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lap Time Analysis for F1 Race Strategy\n",
    "\n",
    "### Data Visualization Approach\n",
    "\n",
    "Our F1 strategy project collects two types of temporal data:\n",
    "\n",
    "1. **High-frequency gap measurements** (approximately every 4 seconds)\n",
    "2. **Lap time data** (completed lap times for each driver)\n",
    "\n",
    "While the high-frequency gap data is valuable input for our prediction models, we've chosen to visualize the lap times directly as they provide clearer strategic insights for human interpretation.\n",
    "\n",
    "### Benefits of Lap Time Visualization\n",
    "\n",
    "1. **Direct performance comparison**: Lap times are the fundamental unit of racing performance\n",
    "2. **Strategy identification**: Pit stops appear as clear spikes in the lap time graph\n",
    "3. **Tire degradation analysis**: Gradual increases in lap time reveal degradation patterns\n",
    "4. **Race pace assessment**: The baseline pace of each driver becomes immediately apparent\n",
    "\n",
    "### Behind the Scenes\n",
    "\n",
    "Although we prioritize lap time visualization for clarity, our machine learning models still utilize the granular gap-to-leader measurements for:\n",
    "\n",
    "- Predicting optimal pit stop windows\n",
    "- Simulating undercut/overcut opportunities\n",
    "- Calculating race position probabilities in different scenarios\n",
    "\n",
    "This dual approach allows us to leverage high-frequency data for model accuracy while providing intuitive visualizations for strategic decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Features of pitstops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pitstop_features(df):\n",
    "    \"\"\"\n",
    "    Procesa y crea características relacionadas con paradas en boxes\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con otras características ya creadas\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con características de paradas añadidas\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # 1. Indicador de parada en la siguiente vuelta (si está disponible)\n",
    "    if 'PitNextLap' in data.columns:\n",
    "        print(\"Creada feature: PitNextLap (ya existente)\")\n",
    "    \n",
    "    # 2. Vueltas desde última parada (si está disponible)\n",
    "    if 'LapsSincePitStop' in data.columns:\n",
    "        print(\"Creada feature: LapsSincePitStop (ya existente)\")\n",
    "    \n",
    "    # 3. Duración de parada (si está disponible)\n",
    "    if 'PitDuration' in data.columns:\n",
    "        # Crear una columna con el tiempo perdido en pit\n",
    "        # Para vueltas sin parada, el valor es 0\n",
    "        data['PitTimeLost'] = data['PitDuration'].fillna(0)\n",
    "        print(\"Creada feature: PitTimeLost\")\n",
    "    \n",
    "    # 4. Cambio de compuesto en la próxima parada (si está disponible)\n",
    "    if 'NextCompound' in data.columns and 'Compound' in data.columns:\n",
    "        # Marcar cuando hay un cambio de compuesto (soft->medium, etc.)\n",
    "        data['CompoundChange'] = (data['NextCompound'] != data['Compound']).astype(int)\n",
    "        print(\"Creada feature: CompoundChange\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear características de paradas\n",
    "pitstop_data = create_pitstop_features(strategy_data)\n",
    "# Mostrar nuevas columnas\n",
    "new_columns = set(pitstop_data.columns) - set(strategy_data.columns)\n",
    "print(f\"Nuevas columnas creadas: {new_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos PitNextLap, visualizar su impacto en el tiempo\n",
    "if 'PitNextLap' in pitstop_data.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x='PitNextLap', y='LapTime', data=pitstop_data)\n",
    "    plt.title('Tiempos por Vuelta Antes de una Parada')\n",
    "    plt.xlabel('Parada en Siguiente Vuelta (0=No, 1=Sí)')\n",
    "    plt.ylabel('Tiempo por Vuelta (s)')\n",
    "    plt.savefig('../outputs/week3/before_after_pit_times.png')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Pit Stop Lap Times\n",
    "\n",
    "This boxplot reveals a significant finding: laps immediately preceding pit stops (value 1) are substantially slower (approximately 4-5 seconds) than regular laps (value 0). This could indicate either strategic decisions (drivers conserving tires/fuel before pitting) or extreme tire degradation forcing a pit stop. \n",
    "\n",
    "This insight is valuable for predicting pit stop timing and understanding team strategies. The clear separation between categories suggests this is a powerful predictive feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos LapsSincePitStop, mostrar su relación con tiempos\n",
    "if 'LapsSincePitStop' in pitstop_data.columns:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Limitar a primeras 20 vueltas después de parada para claridad\n",
    "    subset = pitstop_data[pitstop_data['LapsSincePitStop'] <= 20]\n",
    "    \n",
    "    # Agrupar por vueltas desde parada\n",
    "    lap_groups = subset.groupby('LapsSincePitStop')['LapTime'].mean().reset_index()\n",
    "    \n",
    "    plt.plot(lap_groups['LapsSincePitStop'], lap_groups['LapTime'], 'o-', linewidth=2)\n",
    "    plt.title('Evolución del Tiempo por Vuelta Después de una Parada')\n",
    "    plt.xlabel('Vueltas Desde Última Parada')\n",
    "    plt.ylabel('Tiempo por Vuelta Promedio (s)')\n",
    "    plt.savefig('../outputs/week3/after_pit_time_evo.png')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Pit Stop Performance Recovery\n",
    "\n",
    "This chart tracks how lap times evolve after a pit stop. The first lap following a stop is significantly slower (approximately 5-6 seconds), likely due to pit exit procedures and getting new tires up to optimal temperature.\n",
    "\n",
    "Performance improves dramatically by lap 2.5 and then stabilizes, with minor fluctuations through lap 20. This pattern confirms the \"undercut\" strategy's effectiveness in F1, where fresh tires quickly outperform older tires despite the initial pit stop time loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Select and Prepare Final Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_modeling_features(df):\n",
    "    \"\"\"\n",
    "    Selecciona características para modelado y prepara el dataset final\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con todas las características creadas\n",
    "        \n",
    "    Returns:\n",
    "        Tuple con (dataset listo para modelado, lista de características disponibles)\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Asegurarse de tener el nombre correcto de la columna de compuesto\n",
    "    if 'Compound' not in data.columns and 'TyreCompound' in data.columns:\n",
    "        data['Compound'] = data['TyreCompound']\n",
    "    \n",
    "    # Seleccionar columnas relevantes para modelado\n",
    "    features = [\n",
    "        # Datos básicos de vuelta\n",
    "        'LapNumber', 'Compound', 'TyreAge', 'Position', 'PositionChange', 'FuelLoad',\n",
    "        # Velocidades (si están disponibles)\n",
    "        'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST',\n",
    "        # Clima\n",
    "        'TrackTemp', 'AirTemp', 'Humidity', 'Pressure', 'WindSpeed',\n",
    "        # Status\n",
    "        'TrackStatus', 'IsAccurate',\n",
    "        # Paradas\n",
    "        'PitNextLap', 'LapsSincePitStop', 'PitTimeLost', 'CompoundChange',\n",
    "        # Intervalos (si están disponibles)\n",
    "        'DRSUsed', 'UndercutWindow', 'GapToLeader', 'IsLapped',\n",
    "        # Driver\n",
    "        'Driver'\n",
    "    ]\n",
    "    \n",
    "    # Filtrar solo columnas que existen en el DataFrame\n",
    "    available_features = [f for f in features if f in data.columns]\n",
    "    print(f\"\\nCaracterísticas disponibles para modelado ({len(available_features)}):\")\n",
    "    for feature in available_features:\n",
    "        print(f\"- {feature}\")\n",
    "    \n",
    "    # Seleccionar solo filas con LapTime y features disponibles\n",
    "    model_data = data[available_features + ['LapTime']].dropna(subset=['LapTime'])\n",
    "    \n",
    "    # Eliminar filas con NaN en features críticas\n",
    "    critical_features = [f for f in ['Compound', 'LapNumber', 'TrackTemp'] if f in available_features]\n",
    "    if critical_features:\n",
    "        model_data = model_data.dropna(subset=critical_features)\n",
    "    \n",
    "    print(f\"\\nDimensiones finales del DataFrame para modelado: {model_data.shape}\")\n",
    "    \n",
    "    return model_data, available_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar características para modelado\n",
    "final_model_data, available_features = select_modeling_features(pitstop_data)\n",
    "# Mostrar distribución de valores faltantes por columna\n",
    "missing_data = final_model_data[available_features].isnull().sum().sort_values(ascending=False)\n",
    "missing_data = missing_data[missing_data > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not missing_data.empty:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    missing_data.plot(kind='bar')\n",
    "    plt.title('Valores Faltantes por Característica')\n",
    "    plt.xlabel('Característica')\n",
    "    plt.ylabel('Número de Valores Faltantes')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/week3/missing_values.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Analysis\n",
    "\n",
    "This bar chart identifies data completeness issues across key features. Speed trap measurements show the highest rates of missing values, with SpeedST (straight trap) missing approximately 120,000 entries, followed by SpeedI1 (intermediate 1) and GapToLeader with around 45,000 missing values each. SpeedFL (flying lap) has fewer gaps (~15,000). \n",
    "\n",
    "\n",
    "These patterns suggest systematic data collection issues at certain track points rather than random gaps, informing our imputation strategy in the data preparation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento para matriz de correlación (eliminación de valores faltantes)\n",
    "def prepare_correlation_features(df, max_features=10, target_col='LapTime'):\n",
    "    # Hacer una copia para no modificar el original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Seleccionar solo características numéricas\n",
    "    numeric_features = df_clean.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Eliminar filas con valores faltantes en cualquier característica numérica\n",
    "    df_clean = df_clean[numeric_features].dropna()\n",
    "    print(f\"Filas iniciales: {len(df)}, Filas después de eliminar NaN: {len(df_clean)}\")\n",
    "    \n",
    "    # Limitar a las características más correlacionadas con LapTime\n",
    "    if len(numeric_features) > max_features:\n",
    "        # Asegurarse que target_col esté en el conjunto de datos\n",
    "        if target_col in numeric_features:\n",
    "            # Calcular correlaciones y ordenar\n",
    "            corr = df_clean[numeric_features].corr()[target_col].abs().sort_values(ascending=False)\n",
    "            selected_features = corr.index[:max_features]  # Top N incluye target_col\n",
    "            print(f\"Seleccionadas {len(selected_features)} características de {len(numeric_features)} disponibles\")\n",
    "            return df_clean[selected_features], selected_features\n",
    "    \n",
    "    # Si no hay suficientes características, devolver todas\n",
    "    return df_clean[numeric_features], numeric_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para correlación\n",
    "clean_data, selected_features = prepare_correlation_features(final_model_data, max_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación de las principales características numéricas\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(clean_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Matriz de Correlación de Características Principales')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/week3/correlation_matrix_clean.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Matrix\n",
    "\n",
    "This correlation matrix reveals several important relationships:\n",
    "1. **Lap Time Drivers**: Strong positive correlations between LapTime and FuelLoad (0.63), PitNextLap (0.62), and PitTimeLost (0.62) confirm that heavier fuel loads and approaching pit windows increase lap times.\n",
    "2. **Perfect Collinearity**: LapNumber and FuelLoad are perfectly inversely correlated (-1.0), as expected since we calculated FuelLoad directly from LapNumber.\n",
    "3. **Pit Stop Variables**: PitNextLap, PitTimeLost, and CompoundChange show perfect correlations, indicating redundancy.\n",
    "4. **Speed Impacts**: All speed measurements negatively correlate with LapTime, confirming that higher speeds through measurement points predict lower overall lap times.\n",
    "\n",
    "This matrix helps identify feature redundancies to remove and important relationships to preserve in our predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la preparación de datos\n",
    "clean_data, selected_features = prepare_correlation_features(final_model_data, max_features=10)\n",
    "print(\"Características seleccionadas:\", selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Calcular y visualizar matriz de correlación de datos limpios\n",
    "def calculate_correlation_matrix(clean_df):\n",
    "    \"\"\"\n",
    "    Calcula la matriz de correlación para los datos limpios\n",
    "    \"\"\"\n",
    "    correlation_matrix = clean_df.corr()\n",
    "    \n",
    "    # Visualizar matriz\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Matriz de Correlación de Características Principales')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/week3/correlation_matrix_clean.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el cálculo de la matriz de correlación\n",
    "correlation_matrix = calculate_correlation_matrix(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3: Analizar variables perfectamente correlacionadas\n",
    "def analyze_perfect_correlations(clean_df, corr_matrix):\n",
    "    # Identificar pares de variables con correlación muy alta (>0.95)\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    # Obtener pares de alta correlación (excluyendo la diagonal)\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            col_i = corr_matrix.columns[i]\n",
    "            col_j = corr_matrix.columns[j]\n",
    "            corr_value = corr_matrix.iloc[i, j]\n",
    "            \n",
    "            if abs(corr_value) > 0.95:\n",
    "                high_corr_pairs.append((col_i, col_j, corr_value))\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    if high_corr_pairs:\n",
    "        print(\"Variables con correlación muy alta (>0.95):\")\n",
    "        for col_i, col_j, corr_value in high_corr_pairs:\n",
    "            print(f\"  - {col_i} y {col_j}: {corr_value:.2f}\")\n",
    "    else:\n",
    "        print(\"No se encontraron pares de variables con correlación extremadamente alta (>0.95).\")\n",
    "    \n",
    "    return high_corr_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar análisis de correlaciones perfectas\n",
    "high_correlation_pairs = analyze_perfect_correlations(clean_data, correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: Analizar correlaciones con el tiempo por vuelta\n",
    "def analyze_laptime_correlations(corr_matrix, target_col='LapTime'):\n",
    "    # Extraer correlaciones con LapTime\n",
    "    if target_col in corr_matrix.columns:\n",
    "        laptime_corr = corr_matrix[target_col].abs().sort_values(ascending=False)\n",
    "        \n",
    "        # Dividir en grupos según su correlación\n",
    "        high_corr = laptime_corr[laptime_corr >= 0.5]\n",
    "        medium_corr = laptime_corr[(laptime_corr >= 0.25) & (laptime_corr < 0.5)]\n",
    "        low_corr = laptime_corr[laptime_corr < 0.25]\n",
    "        \n",
    "        print(f\"Correlaciones con {target_col}:\")\n",
    "        print(\"\\nAlta correlación (>=0.5):\")\n",
    "        for feature, corr in high_corr.items():\n",
    "            if feature != target_col:  # Excluir correlación consigo mismo\n",
    "                print(f\"  - {feature}: {corr:.2f}\")\n",
    "        \n",
    "        print(\"\\nCorrelación media (0.25-0.5):\")\n",
    "        for feature, corr in medium_corr.items():\n",
    "            print(f\"  - {feature}: {corr:.2f}\")\n",
    "        \n",
    "        print(\"\\nBaja correlación (<0.25):\")\n",
    "        for feature, corr in low_corr.items():\n",
    "            print(f\"  - {feature}: {corr:.2f}\")\n",
    "        \n",
    "        return laptime_corr\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar análisis de correlaciones con LapTime\n",
    "laptime_correlations = analyze_laptime_correlations(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5: Recomendar conjunto final de características\n",
    "def recommend_final_features(corr_with_laptime, high_corr_pairs):\n",
    "    print(\"\\nRecomendaciones para el conjunto final de características:\")\n",
    "    \n",
    "    # 1. Identificar características altamente predictivas\n",
    "    if corr_with_laptime is not None:\n",
    "        high_pred_features = corr_with_laptime[corr_with_laptime >= 0.5].index.tolist()\n",
    "        if 'LapTime' in high_pred_features:\n",
    "            high_pred_features.remove('LapTime')  # Eliminar la variable objetivo\n",
    "        \n",
    "        print(\"1. Características altamente predictivas a mantener:\")\n",
    "        for feature in high_pred_features:\n",
    "            print(f\"   - {feature}\")\n",
    "    \n",
    "    # 2. Identificar redundancias a eliminar\n",
    "    if high_corr_pairs:\n",
    "        print(\"\\n2. Redundancias a considerar (mantener solo una de cada par):\")\n",
    "        for col_i, col_j, _ in high_corr_pairs:\n",
    "            # Decidir cuál mantener basado en correlación con LapTime\n",
    "            if corr_with_laptime is not None:\n",
    "                if corr_with_laptime.get(col_i, 0) >= corr_with_laptime.get(col_j, 0):\n",
    "                    print(f\"   - Mantener {col_i}, considerar eliminar {col_j}\")\n",
    "                else:\n",
    "                    print(f\"   - Mantener {col_j}, considerar eliminar {col_i}\")\n",
    "            else:\n",
    "                print(f\"   - Considerar mantener solo una de: {col_i} o {col_j}\")\n",
    "    \n",
    "    # 3. Características con valor estratégico pero baja correlación\n",
    "    if corr_with_laptime is not None:\n",
    "        strategic_features = ['Position', 'TrackStatus']\n",
    "        present_strategic = [f for f in strategic_features if f in corr_with_laptime.index]\n",
    "        \n",
    "        if present_strategic:\n",
    "            print(\"\\n3. Características con valor estratégico a mantener a pesar de baja correlación:\")\n",
    "            for feature in present_strategic:\n",
    "                print(f\"   - {feature}: {corr_with_laptime.get(feature, 0):.2f}\")\n",
    "    \n",
    "    print(\"\\nBalancear estos factores proporcionará un conjunto óptimo de características para el modelo final.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar recomendación final\n",
    "recommend_final_features(laptime_correlations, high_correlation_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Analysis Based on Correlation Matrix\n",
    "\n",
    "Based on the correlation matrix analysis of our cleaned dataset, we've identified several key insights for model optimization:\n",
    "\n",
    "1. **Highly Predictive Features**:\n",
    "   - Variables with the strongest correlation to lap time should form the foundation of our predictive model.\n",
    "   - This includes fuel load, speed measurements at key track sections, and tyre age.\n",
    "\n",
    "2. **Redundancy Management**:\n",
    "   - Several pairs of features show extremely high correlation (>0.95), indicating redundancy.\n",
    "   - For each redundant pair, we prioritize keeping the feature with stronger correlation to lap time.\n",
    "   - LapNumber and FuelLoad represent the same information in different forms - we retain both for their distinct interpretative value.\n",
    "\n",
    "3. **Strategic Variables**:\n",
    "   - Some variables like Position show lower statistical correlation but provide critical strategic insights.\n",
    "   - These are retained despite lower predictive power due to their importance for race strategy decisions.\n",
    "\n",
    "4. **Pitstop-related Features**:\n",
    "   - Pitstop timing and impact variables provide crucial information for strategy modeling.\n",
    "   - We created composite variables where appropriate to reduce dimensionality while preserving information.\n",
    "\n",
    "This optimization approach balances statistical considerations with domain knowledge to create a feature set that maximizes both predictive power and strategic insight for F1 strategy decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar estadísticas descriptivas del dataset final\n",
    "print(\"\\nEstadísticas descriptivas del dataset final:\")\n",
    "display(clean_data.describe())\n",
    "\n",
    "# Guardar datos procesados para modelos\n",
    "# final_model_data.to_csv('data/processed/model_ready_data.csv', index=False)\n",
    "print(\"\\nDatos listos para modelado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_csv = clean_data.to_csv(\"../outputs/week3/model_lap_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Data Preparation\n",
    "\n",
    "Our data preparation process follows a systematic approach broken down into specialized components:\n",
    "\n",
    "1. **Data Cleaning**: We convert temporal data to seconds for analysis and remove outliers based on 5th and 95th percentiles to ensure our model isn't trained on anomalous lap times caused by safety cars, accidents, or data errors.\n",
    "\n",
    "2. **Tire Performance Features**: We create features that capture tire degradation and its effect on performance, including tire age and position changes between laps.\n",
    "\n",
    "3. **Race Strategy Features**: These features represent strategic elements like DRS usage, undercut opportunities, and gaps to the race leader that influence driving approach and lap times.\n",
    "\n",
    "4. **Pit Stop Analysis**: We process pit stop data to calculate time lost during stops, stint length, and compound change effects that are critical for strategy modeling.\n",
    "\n",
    "5. **Feature Selection**: Finally, we identify the most relevant variables for our model based on domain knowledge of Formula 1 racing, ensuring we include key performance drivers while avoiding redundant or noisy features.\n",
    "\n",
    "This modular approach allows us to clearly understand each transformation and facilitates future refinements to specific aspects of data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico\n",
    "print(\"Resumen estadístico de variables numéricas:\")\n",
    "display(clean_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiempo por vuelta según tipo de neumático\n",
    "if 'Compound' in clean_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Compound', y='LapTime', data=clean_data)\n",
    "    plt.title('Tiempo por Vuelta según Tipo de Neumático')\n",
    "    plt.xlabel('Compuesto')\n",
    "    plt.ylabel('Tiempo (segundos)')\n",
    "    plt.savefig('../outputs/week3/laptime_by_tyre.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de dispersión: Variables importantes vs LapTime\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "if 'TyreAge' in pitstop_data.columns:\n",
    "    sns.scatterplot(x='TyreAge', y='LapTime', hue='Compound', data=pitstop_data, ax=axes[0])\n",
    "    axes[0].set_title('LapTime vs TyreAge')\n",
    "    \n",
    "if 'TrackTemp' in pitstop_data.columns:\n",
    "    sns.scatterplot(x='TrackTemp', y='LapTime', data=pitstop_data, ax=axes[1])\n",
    "    axes[1].set_title('LapTime vs TrackTemp')\n",
    "    \n",
    "if 'FuelLoad' in pitstop_data.columns:\n",
    "    sns.scatterplot(x='FuelLoad', y='LapTime', data=pitstop_data, ax=axes[2])\n",
    "    axes[2].set_title('LapTime vs FuelLoad')\n",
    "\n",
    "if 'Position' in pitstop_data.columns:\n",
    "    sns.scatterplot(x='Position', y='LapTime', data=pitstop_data, ax=axes[3])\n",
    "    axes[3].set_title('LapTime vs Position')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/week3/key_variables_scatter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Análisis Específico de Paradas en Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pitstop_impact(data):\n",
    "    \"\"\"\n",
    "    Visualiza el impacto de las paradas en los tiempos por vuelta\n",
    "    Versión simplificada que asegura generar todos los gráficos relevantes\n",
    "    \"\"\"\n",
    "    # Verificar disponibilidad de datos de paradas\n",
    "    has_pitstop_data = ('LapsSincePitStop' in data.columns and 'PitNextLap' in data.columns)\n",
    "    \n",
    "    if not has_pitstop_data:\n",
    "        print(\"No hay suficientes datos de paradas disponibles para visualización.\")\n",
    "        return\n",
    "    \n",
    "    # 1. Degradación de neumáticos por compuesto\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Filtrar para mostrar hasta 20 vueltas después de una parada\n",
    "    plot_data = data[data['LapsSincePitStop'] <= 20].copy()\n",
    "    \n",
    "    # Verificar si hay datos suficientes\n",
    "    if len(plot_data) < 10:\n",
    "        print(\"Datos insuficientes para analizar degradación por compuesto.\")\n",
    "    else:\n",
    "        # Agrupar por tipo de compuesto y vueltas desde parada\n",
    "        plot_data['LapsSincePitStop'] = plot_data['LapsSincePitStop'].astype(int)\n",
    "        grouped = plot_data.groupby(['Compound', 'LapsSincePitStop'])['LapTime'].mean().reset_index()\n",
    "        \n",
    "        # Dibujar una línea por cada compuesto\n",
    "        for compound in grouped['Compound'].unique():\n",
    "            compound_data = grouped[grouped['Compound'] == compound]\n",
    "            color = compound_colors.get(compound, 'black')\n",
    "            plt.plot(compound_data['LapsSincePitStop'], \n",
    "                     compound_data['LapTime'], \n",
    "                     'o-', \n",
    "                     color=color,\n",
    "                     label=f'Compuesto {compound}')\n",
    "        \n",
    "        plt.xlabel('Vueltas desde la parada')\n",
    "        plt.ylabel('Tiempo por vuelta promedio (s)')\n",
    "        plt.title('Degradación de neumáticos por compuesto')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig('../outputs/week3/tyre_degradation_by_compound.png')\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. Análisis de paradas por compuesto\n",
    "    if 'PitNextLap' in data.columns:\n",
    "        # Identificar vueltas previas a paradas\n",
    "        pitstop_rows = data['PitNextLap'] == 1\n",
    "        pitstop_count = pitstop_rows.sum()\n",
    "        \n",
    "        if pitstop_count > 0:\n",
    "            print(f\"Encontradas {pitstop_count} filas que indican paradas.\")\n",
    "            pitstop_data = data[pitstop_rows]\n",
    "            \n",
    "            # 2.1 Contar paradas por cada piloto para verificar\n",
    "            pit_by_driver = pitstop_data.groupby('Driver').size()\n",
    "            print(\"\\nNúmero de paradas por piloto:\")\n",
    "            print(pit_by_driver)\n",
    "            \n",
    "            # 2.2 Gráfico de compuestos usados antes de paradas\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            compound_counts = pitstop_data['Compound'].value_counts()\n",
    "            ax = sns.barplot(x=compound_counts.index, y=compound_counts.values, \n",
    "                           palette={comp: compound_colors.get(comp, 'gray') for comp in compound_counts.index})\n",
    "            \n",
    "            # Añadir etiquetas\n",
    "            for i, count in enumerate(compound_counts):\n",
    "                ax.text(i, count/2, str(count), ha='center', va='center', fontweight='bold')\n",
    "                \n",
    "            plt.title('Compuestos utilizados antes de paradas')\n",
    "            plt.xlabel('Compuesto')\n",
    "            plt.ylabel('Número de paradas')\n",
    "            plt.savefig('../outputs/week3/compounds_before_pitstop.png')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No se encontraron filas que indiquen paradas en boxes (PitNextLap=1).\")\n",
    "    else:\n",
    "        print(\"No se encuentra la columna 'PitNextLap' para analizar paradas.\")\n",
    "\n",
    "# Ejecutar con los datos limpios\n",
    "visualize_pitstop_impact(final_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preprocesamiento para Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sección previa a \"8. Preprocesamiento para Modelado\"\n",
    "# Finalizar la selección de variables basada en el análisis de correlación\n",
    "\n",
    "# 1. Definir qué variables mantener según el análisis previo\n",
    "variables_to_keep = [\n",
    "    # Variables con alta correlación con LapTime\n",
    "    'LapTime', 'FuelLoad', 'TyreAge', 'SpeedI1', 'SpeedI2',\n",
    "    # Variables estratégicas\n",
    "    'Position', 'Compound', 'LapsSincePitStop',\n",
    "    # Información del piloto\n",
    "    'Driver'\n",
    "]\n",
    "\n",
    "# 2. Verificar cuáles de estas variables están disponibles\n",
    "available_vars = [var for var in variables_to_keep if var in clean_data.columns]\n",
    "print(f\"Variables finales seleccionadas para modelado ({len(available_vars)}/{len(variables_to_keep)}):\")\n",
    "for var in available_vars:\n",
    "    print(f\"- {var}\")\n",
    "\n",
    "# 3. Crear DataFrame final para modelado\n",
    "model_ready_data = clean_data[available_vars].copy()\n",
    "\n",
    "# 4. Guardar el dataset listo para modelado\n",
    "model_ready_data.to_csv('../outputs/week3/model_ready_data.csv', index=False)\n",
    "print(f\"\\nDatos listos para modelado guardados con {model_ready_data.shape[0]} filas y {model_ready_data.shape[1]} columnas\")\n",
    "\n",
    "# 5. Verificar valores faltantes finales\n",
    "missing_values = model_ready_data.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\nValores faltantes en el dataset final:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"\\nNo hay valores faltantes en el dataset final.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Preprocesamiento para Modelado\n",
    "\n",
    "def preprocess_data_for_modeling(df):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos para el modelado, dividiendo en conjuntos de entrenamiento/prueba\n",
    "    y configurando los transformadores necesarios.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame listo para modelado\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test, preprocessor\n",
    "    \"\"\"\n",
    "    # Separar características y objetivo\n",
    "    X = df.drop('LapTime', axis=1)\n",
    "    y = df['LapTime']\n",
    "    \n",
    "    # Identificar columnas categóricas y numéricas\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    print(f\"Características categóricas: {cat_cols}\")\n",
    "    print(f\"Características numéricas: {num_cols}\")\n",
    "    \n",
    "    # Configurar transformadores para preprocesamiento\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), num_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "        ])\n",
    "    \n",
    "    # Dividir datos en entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Conjunto de entrenamiento: {X_train.shape[0]} muestras\")\n",
    "    print(f\"Conjunto de prueba: {X_test.shape[0]} muestras\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "# Preprocesar datos para modelado\n",
    "X_train, X_test, y_train, y_test, preprocessor = preprocess_data_for_modeling(model_ready_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entrenamiento de Modelo XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Entrena un modelo XGBoost\n",
    "    \"\"\"\n",
    "    # Crear pipeline con preprocesamiento y modelo\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', xgb.XGBRegressor(objective='reg:squarederror'))\n",
    "    ])\n",
    "    \n",
    "    # Parámetros para Grid Search\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__learning_rate': [0.01, 0.1],\n",
    "        'regressor__max_depth': [3, 5, 7],\n",
    "        'regressor__min_child_weight': [1, 3]\n",
    "    }\n",
    "    \n",
    "    # Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"Entrenando modelo XGBoost con GridSearchCV (esto puede tardar varios minutos)...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Mejores parámetros\n",
    "    print(\"\\nMejores parámetros XGBoost:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Predecir\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluar\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nMétricas de evaluación del modelo XGBoost:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f} segundos\")\n",
    "    print(f\"  MAE: {mae:.4f} segundos\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    \n",
    "    # Analizar características importantes\n",
    "    if hasattr(best_model.named_steps['regressor'], 'feature_importances_'):\n",
    "        # Obtener nombres de características después del preprocesamiento\n",
    "        try:\n",
    "            cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "            \n",
    "            # Obtener nombres de características después de one-hot encoding\n",
    "            encoder = best_model.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot']\n",
    "            cat_features = encoder.get_feature_names_out(cat_cols).tolist()\n",
    "            all_features = num_cols + cat_features\n",
    "            \n",
    "            # Crear DataFrame con importancias\n",
    "            importances = best_model.named_steps['regressor'].feature_importances_\n",
    "            feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importances})\n",
    "            feature_importance = feature_importance.sort_values('Importance', ascending=False).head(15)\n",
    "            \n",
    "            # Visualizar\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "            plt.title('Top 15 Características Más Importantes - XGBoost')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('outputs/week3/xgboost_feature_importance.png')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error al analizar importancia de características: {e}\")\n",
    "    \n",
    "    return best_model, y_pred\n",
    "\n",
    "# Entrenar modelo XGBoost\n",
    "xgb_model, y_pred_xgb = train_xgboost(X_train, X_test, y_train, y_test, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Entrenamiento de Red Neuronal (Opcional)\n",
    "\n",
    "Este paso es opcional y puede omitirse si prefieres usar solo XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch_nn(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de red neuronal con PyTorch\n",
    "    \"\"\"\n",
    "    # Aplicar preprocesamiento\n",
    "    print(\"Preprocesando datos para la red neuronal...\")\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    X_train_tensor = torch.FloatTensor(X_train_processed.toarray())\n",
    "    y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_processed.toarray())\n",
    "    y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1)\n",
    "    \n",
    "    # Crear conjuntos de datos y cargadores\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Inicializar modelo\n",
    "    print(f\"Inicializando red neuronal con {X_train_processed.shape[1]} entradas...\")\n",
    "    input_size = X_train_processed.shape[1]\n",
    "    model = LapTimeNN(input_size)\n",
    "    \n",
    "    # Definir criterio y optimizador\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Guardar historial de pérdidas para graficar\n",
    "    losses = []\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"\\nEntrenando red neuronal...\")\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Visualizar curva de aprendizaje\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs+1), losses)\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.title('Curva de Aprendizaje - Red Neuronal')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('outputs/week3/neural_network_learning_curve.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "        y_pred = y_pred_tensor.numpy().flatten()\n",
    "        \n",
    "    # Métricas\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nMétricas de evaluación de la Red Neuronal:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f} segundos\")\n",
    "    print(f\"  MAE: {mae:.4f} segundos\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    \n",
    "    return model, y_pred\n",
    "\n",
    "# Preguntar si se desea entrenar la red neuronal\n",
    "train_nn = True  # Cambiar a False para omitir este paso\n",
    "if train_nn:\n",
    "    nn_model, y_pred_nn = train_pytorch_nn(X_train, X_test, y_train, y_test, preprocessor)\n",
    "else:\n",
    "    nn_model, y_pred_nn = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualización y Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(y_test, y_pred_xgb, y_pred_nn=None, max_points=1000):\n",
    "    \"\"\"\n",
    "    Visualiza los resultados de los modelos\n",
    "    \"\"\"\n",
    "    # Limitar número de puntos para visualización clara\n",
    "    if len(y_test) > max_points:\n",
    "        # Muestrear aleatoriamente para mantener la distribución\n",
    "        indices = np.random.choice(len(y_test), max_points, replace=False)\n",
    "        y_test_sample = y_test.iloc[indices]\n",
    "        y_pred_xgb_sample = y_pred_xgb[indices]\n",
    "        if y_pred_nn is not None:\n",
    "            y_pred_nn_sample = y_pred_nn[indices]\n",
    "    else:\n",
    "        y_test_sample = y_test\n",
    "        y_pred_xgb_sample = y_pred_xgb\n",
    "        y_pred_nn_sample = y_pred_nn\n",
    "    \n",
    "    # Configurar tamaño de figura\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Determinar número de subplots\n",
    "    if y_pred_nn is not None:\n",
    "        n_plots = 3\n",
    "    else:\n",
    "        n_plots = 2\n",
    "    \n",
    "    # 1. Dispersión XGBoost\n",
    "    plt.subplot(1, n_plots, 1)\n",
    "    plt.scatter(y_test_sample, y_pred_xgb_sample, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Tiempo real (s)')\n",
    "    plt.ylabel('Tiempo predicho (s)')\n",
    "    plt.title('XGBoost: Predicciones vs Reales')\n",
    "    \n",
    "    # 2. Residuos XGBoost\n",
    "    plt.subplot(1, n_plots, 2)\n",
    "    residuals_xgb = y_test_sample - y_pred_xgb_sample\n",
    "    plt.scatter(y_pred_xgb_sample, residuals_xgb, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicción (s)')\n",
    "    plt.ylabel('Residuos (s)')\n",
    "    plt.title('XGBoost: Residuos vs Predicciones')\n",
    "    \n",
    "    # 3. Si hay predicciones de red neuronal\n",
    "    if y_pred_nn is not None:\n",
    "        plt.subplot(1, n_plots, 3)\n",
    "        plt.scatter(y_test_sample, y_pred_nn_sample, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.xlabel('Tiempo real (s)')\n",
    "        plt.ylabel('Tiempo predicho (s)')\n",
    "        plt.title('Red Neuronal: Predicciones vs Reales')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/week3/prediction_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Comparativa de errores\n",
    "    if y_pred_nn is not None:\n",
    "        errors_xgb = np.abs(y_test - y_pred_xgb)\n",
    "        errors_nn = np.abs(y_test - y_pred_nn)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(errors_xgb, alpha=0.5, bins=50, label='XGBoost')\n",
    "        plt.hist(errors_nn, alpha=0.5, bins=50, label='Red Neuronal')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Error Absoluto (s)')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.title('Distribución de Errores por Modelo')\n",
    "        plt.xlim(0, min(10, max(errors_xgb.max(), errors_nn.max())))\n",
    "        plt.savefig('outputs/week3/error_distribution.png')\n",
    "        plt.show()\n",
    "\n",
    "# Visualizar resultados\n",
    "visualize_predictions(y_test, y_pred_xgb, y_pred_nn if train_nn else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Guardar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(xgb_model, nn_model=None):\n",
    "    \"\"\"\n",
    "    Guarda los modelos entrenados\n",
    "    \"\"\"\n",
    "    # Guardar modelo XGBoost\n",
    "    joblib.dump(xgb_model, 'models/week3/xgboost_laptime.joblib')\n",
    "    print(\"Modelo XGBoost guardado en 'models/week3/xgboost_laptime.joblib'\")\n",
    "    \n",
    "    # Guardar modelo PyTorch si existe\n",
    "    if nn_model is not None:\n",
    "        torch.save(nn_model.state_dict(), 'models/week3/nn_laptime.pth')\n",
    "        print(\"Modelo PyTorch guardado en 'models/week3/nn_laptime.pth'\")\n",
    "\n",
    "# Guardar modelos\n",
    "save_models(xgb_model, nn_model if train_nn else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Prueba de Predicción con Nuevos Datos\n",
    "\n",
    "Probemos el modelo con una situación de carrera hipotética."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model, available_features):\n",
    "    \"\"\"\n",
    "    Prueba el modelo con datos hipotéticos\n",
    "    \"\"\"\n",
    "    # Crear un escenario de ejemplo\n",
    "    example_drivers = [\"VER\", \"HAM\", \"LEC\", \"PER\", \"SAI\"]\n",
    "    compounds = [\"SOFT\", \"MEDIUM\", \"HARD\"]\n",
    "    \n",
    "    # Crear DataFrame de ejemplo\n",
    "    example_data = []\n",
    "    \n",
    "    for driver in example_drivers:\n",
    "        for compound in compounds:\n",
    "            for tyre_age in [1, 10, 20]:\n",
    "                # Valores predeterminados\n",
    "                row = {\n",
    "                    'Driver': driver,\n",
    "                    'Compound': compound,\n",
    "                    'TyreAge': tyre_age,\n",
    "                    'LapNumber': 30,\n",
    "                    'TrackTemp': 35.0,\n",
    "                    'AirTemp': 25.0,\n",
    "                    'Position': 5,\n",
    "                    'FuelLoad': 0.5,\n",
    "                    'LapsSincePitStop': tyre_age,  # Si está disponible\n",
    "                    'PitNextLap': 0,  # No hay parada en la siguiente vuelta\n",
    "                }\n",
    "                \n",
    "                # Añadir solo características disponibles\n",
    "                example_row = {k: v for k, v in row.items() if k in available_features}\n",
    "                example_data.append(example_row)\n",
    "    \n",
    "    example_df = pd.DataFrame(example_data)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    predictions = model.predict(example_df)\n",
    "    \n",
    "    # Añadir predicciones al DataFrame\n",
    "    example_df['PredictedLapTime'] = predictions\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    print(\"\\nPredicciones para escenarios de ejemplo:\")\n",
    "    display(example_df)\n",
    "    \n",
    "    # Graficar por compuesto y edad de neumáticos\n",
    "    if 'Compound' in example_df.columns and 'TyreAge' in example_df.columns:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for driver in example_drivers[:3]:  # Limitar a 3 pilotos para claridad\n",
    "            driver_data = example_df[example_df['Driver'] == driver]\n",
    "            for compound in compounds:\n",
    "                tyre_data = driver_data[driver_data['Compound'] == compound]\n",
    "                plt.plot(tyre_data['TyreAge'], tyre_data['PredictedLapTime'], \n",
    "                         marker='o', linestyle='-', label=f\"{driver} - {compound}\")\n",
    "                \n",
    "        plt.xlabel('Edad de Neumáticos (vueltas)')\n",
    "        plt.ylabel('Tiempo Predicho (s)')\n",
    "        plt.title('Predicción de Tiempos por Tipo y Edad de Neumáticos')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('outputs/week3/tyre_age_predictions.png')\n",
    "        plt.show()\n",
    "    \n",
    "    return example_df\n",
    "\n",
    "# Probar predicciones con el modelo XGBoost\n",
    "try:\n",
    "    prediction_examples = test_prediction(xgb_model, available_features)\n",
    "except Exception as e:\n",
    "    print(f\"Error al realizar predicciones de prueba: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusiones y Próximos Pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "En este notebook, hemos:\n",
    "1. Cargado y preparado datos de FastF1 y OpenF1, incluyendo información de vueltas, paradas y condiciones meteorológicas\n",
    "2. Realizado feature engineering para crear características como edad de neumáticos, carga de combustible, y paradas en boxes\n",
    "3. Analizado el impacto de las paradas y la degradación de neumáticos en los tiempos por vuelta\n",
    "4. Entrenado un modelo XGBoost para predecir tiempos por vuelta\n",
    "5. Opcionalmente, entrenado una red neuronal para comparar su rendimiento\n",
    "6. Visualizado y evaluado los resultados\n",
    "\n",
    "### Resultados Principales\n",
    "- El modelo XGBoost puede predecir tiempos por vuelta con un error medio de X segundos\n",
    "- Las características más importantes para la predicción son [listar las top 3-5 características]\n",
    "- La degradación de neumáticos tiene un impacto significativo en los tiempos por vuelta\n",
    "- Las paradas en boxes muestran patrones claros en cuanto a su timing y elección de compuesto\n",
    "\n",
    "### Próximos Pasos (Semana 4)\n",
    "1. Integrar este modelo de predicción con un sistema de decisiones basado en reglas\n",
    "2. Añadir lógica para recomendar estrategias de paradas en boxes\n",
    "3. Desarrollar sistema para simular undercuts/overcuts basados en las predicciones\n",
    "4. Crear visualizaciones interactivas para analizar el impacto de diferentes estrategias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
