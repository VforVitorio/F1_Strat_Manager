{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de Tiempos por Vuelta en F1\n",
    "\n",
    "Este notebook implementa modelos predictivos para estimar el tiempo por vuelta de los coches de F1 en función de diversas variables como el tipo de neumático, condiciones meteorológicas, y estado de la pista.\n",
    "\n",
    "## Objetivos\n",
    "1. Cargar y preprocesar datos de FastF1 y OpenF1\n",
    "2. Realizar feature engineering para potenciar la capacidad predictiva\n",
    "3. Incluir análisis de degradación de neumáticos y paradas en boxes\n",
    "4. Entrenar modelos de predicción (XGBoost y opcionalmente una Red Neuronal)\n",
    "5. Evaluar el rendimiento y visualizar resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import fastf1\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Configuración de fastf1\n",
    "fastf1.Cache.enable_cache('../f1-strategy/f1_cache')  # Asegúrate de que esta carpeta exista\n",
    "\n",
    "# Crear directorios para outputs y models si no existen\n",
    "os.makedirs('../outputs/week3', exist_ok=True)\n",
    "os.makedirs('../models/week3', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definición de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para el modelo PyTorch\n",
    "class LapTimeNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LapTimeNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga y Preparación de Datos\n",
    "\n",
    "Vamos a cargar los datos desde los archivos Parquet que tenemos disponibles:\n",
    "- laps.parquet: Contiene información sobre vueltas individuales\n",
    "- weather.parquet: Contiene información meteorológica\n",
    "- intervals.parquet: Contiene información sobre gaps y estados de carrera\n",
    "- pitstops.parquet: Contiene información sobre paradas en boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos desde archivos parquet\n",
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Carga todos los datasets desde archivos parquet y los devuelve como DataFrames\n",
    "    \"\"\"\n",
    "    # Definir rutas a los archivos\n",
    "    laps_path = \"../f1-strategy/data/raw/Spain_2023_laps.parquet\"\n",
    "    weather_path = \"../f1-strategy/data/raw/Spain_2023_weather.parquet\"\n",
    "    intervals_path = \"../f1-strategy/data/raw/Spain_2023_openf1_intervals.parquet\"\n",
    "    pitstops_path = \"../f1-strategy/data/raw/Spain_2023_pitstops.parquet\"\n",
    "    \n",
    "    # Cargar DataFrames\n",
    "    laps_df = pd.read_parquet(laps_path)\n",
    "    weather_df = pd.read_parquet(weather_path)\n",
    "    \n",
    "\n",
    "    # Establish booleans of intervals and pitstops to True\n",
    "    # Before, error magaing to knwo if this dataframes could be correctly downloaded\n",
    "    \n",
    "    return laps_df, weather_df, intervals_df, pitstops_df, has_intervals, has_pitstops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la carga de datos\n",
    "laps_df, weather_df, intervals_df, pitstops_df, has_intervals, has_pitstops = load_all_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de laps_df: (1312, 31)\n",
      "Dimensiones de weather_df: (154, 8)\n",
      "Dimensiones de intervals_df: (8933, 10)\n",
      "Dimensiones de pitstops_df: (43, 31)\n"
     ]
    }
   ],
   "source": [
    "# Mostrar información sobre los DataFrames\n",
    "print(\"Dimensiones de laps_df:\", laps_df.shape)\n",
    "print(\"Dimensiones de weather_df:\", weather_df.shape)\n",
    "if has_intervals:\n",
    "    print(\"Dimensiones de intervals_df:\", intervals_df.shape)\n",
    "if has_pitstops:\n",
    "    print(\"Dimensiones de pitstops_df:\", pitstops_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploración Inicial de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar los primeros registros\n",
    "print(\"Primeros registros de laps_df:\")\n",
    "display(laps_df.head())\n",
    "\n",
    "print(\"\\nPrimeros registros de weather_df:\")\n",
    "display(weather_df.head())\n",
    "\n",
    "if has_pitstops:\n",
    "    print(\"\\nPrimeros registros de pitstops_df:\")\n",
    "    display(pitstops_df.head())\n",
    "\n",
    "# Verificar si tenemos las columnas esperadas para laps\n",
    "expected_laps_columns = [\n",
    "    'LapTime', 'LapNumber', 'Stint', 'PitOutTime', 'PitInTime', 'Sector1Time', \n",
    "    'Sector2Time', 'Sector3Time', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', \n",
    "    'Position', 'TyreLife', 'TrackStatus', 'IsAccurate', 'Compound', 'Driver'\n",
    "]\n",
    "\n",
    "# Verificar si tenemos las columnas esperadas para weather\n",
    "expected_weather_columns = [\n",
    "    'Time', 'AirTemp', 'Humidity', 'Pressure', 'Rainfall', \n",
    "    'TrackTemp', 'WindDirection', 'WindSpeed'\n",
    "]\n",
    "\n",
    "# Verificar columnas esperadas para pitstops\n",
    "expected_pitstop_columns = [\n",
    "    'Time', 'Driver', 'LapNumber', 'PitInTime', 'Compound', 'TyreLife', 'FreshTyre'\n",
    "]\n",
    "\n",
    "# Verificar columnas disponibles en laps_df\n",
    "print(\"\\nColumnas disponibles en laps_df:\")\n",
    "for col in expected_laps_columns:\n",
    "    if col in laps_df.columns:\n",
    "        print(f\"✓ {col}\")\n",
    "    else:\n",
    "        print(f\"✗ {col}\")\n",
    "\n",
    "# Verificar columnas disponibles en weather_df\n",
    "print(\"\\nColumnas disponibles en weather_df:\")\n",
    "for col in expected_weather_columns:\n",
    "    if col in weather_df.columns:\n",
    "        print(f\"✓ {col}\")\n",
    "    else:\n",
    "        print(f\"✗ {col}\")\n",
    "\n",
    "# Verificar columnas disponibles en pitstops_df si existe\n",
    "if has_pitstops:\n",
    "    print(\"\\nColumnas disponibles en pitstops_df:\")\n",
    "    for col in expected_pitstop_columns:\n",
    "        if col in pitstops_df.columns:\n",
    "            print(f\"✓ {col}\")\n",
    "        else:\n",
    "            print(f\"✗ {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocesamiento y Unión de Datos\n",
    "\n",
    "Unimos los datos de laps, weather, intervals y pitstops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_data(laps_df, weather_df, intervals_df=None, pitstops_df=None):\n",
    "    \"\"\"\n",
    "    Combina los DataFrames de laps, weather, intervals y pitstops\n",
    "    \"\"\"\n",
    "    # Primero unir laps y weather basados en el tiempo\n",
    "    if 'Time' in laps_df.columns and 'Time' in weather_df.columns:\n",
    "        # Convertir a datetime si aún no lo está\n",
    "        if not pd.api.types.is_datetime64_any_dtype(laps_df['Time']):\n",
    "            laps_df['Time'] = pd.to_datetime(laps_df['Time'])\n",
    "        if not pd.api.types.is_datetime64_any_dtype(weather_df['Time']):\n",
    "            weather_df['Time'] = pd.to_datetime(weather_df['Time'])\n",
    "            \n",
    "        # Combinar datos basados en el tiempo más cercano\n",
    "        merged_df = pd.merge_asof(\n",
    "            laps_df.sort_values('Time'), \n",
    "            weather_df.sort_values('Time'),\n",
    "            on='Time',\n",
    "            direction='nearest'\n",
    "        )\n",
    "    else:\n",
    "        print(\"Las columnas 'Time' no están disponibles en ambos DataFrames. Usando laps_df únicamente.\")\n",
    "        merged_df = laps_df.copy()\n",
    "    \n",
    "    # Si tenemos pitstops, agregar información de paradas\n",
    "    if pitstops_df is not None:\n",
    "        # Preparar datos de pitstops para unión\n",
    "        print(\"Enriqueciendo datos con información de pitstops...\")\n",
    "        \n",
    "        # 1. Calcular duración de pitstop para cada parada\n",
    "        if 'PitInTime' in pitstops_df.columns and 'Time' in pitstops_df.columns:\n",
    "            if not pd.api.types.is_datetime64_any_dtype(pitstops_df['PitInTime']):\n",
    "                pitstops_df['PitInTime'] = pd.to_datetime(pitstops_df['PitInTime'])\n",
    "            if not pd.api.types.is_datetime64_any_dtype(pitstops_df['Time']):\n",
    "                pitstops_df['Time'] = pd.to_datetime(pitstops_df['Time'])\n",
    "            \n",
    "            # Tiempo en pit = Time (salida) - PitInTime (entrada)\n",
    "            pitstops_df['PitDuration'] = (pitstops_df['Time'] - pitstops_df['PitInTime']).dt.total_seconds()\n",
    "        \n",
    "        # 2. Crear variables auxiliares para unir con laps\n",
    "        pitstops_features = pitstops_df[['Driver', 'LapNumber', 'Compound', 'FreshTyre', 'PitDuration']].copy()\n",
    "        pitstops_features.columns = ['Driver', 'PitStopLap', 'NextCompound', 'FreshTyreAfterStop', 'PitDuration']\n",
    "        \n",
    "        # 3. Unir pitstops con laps para indicar cuándo hay parada en la siguiente vuelta\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            pitstops_features,\n",
    "            left_on=['Driver', 'LapNumber'],\n",
    "            right_on=['Driver', 'PitStopLap'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # 4. Crear columna que indique si hay parada en la siguiente vuelta\n",
    "        merged_df['PitNextLap'] = merged_df['PitDuration'].notna().astype(int)\n",
    "        \n",
    "        # 5. Crear columna con vueltas desde última parada\n",
    "        def calculate_laps_since_pitstop(group):\n",
    "            # Inicializar columna con valores altos\n",
    "            group['LapsSincePitStop'] = 999\n",
    "            \n",
    "            # Obtener vueltas con parada\n",
    "            pit_laps = group[group['PitNextLap'] == 1]['LapNumber'].values\n",
    "            \n",
    "            # Para cada vuelta, calcular distancia a la última parada\n",
    "            for idx, row in group.iterrows():\n",
    "                lap_num = row['LapNumber']\n",
    "                # Encontrar la parada anterior más cercana\n",
    "                previous_pits = [p for p in pit_laps if p < lap_num]\n",
    "                if previous_pits:\n",
    "                    group.loc[idx, 'LapsSincePitStop'] = lap_num - max(previous_pits)\n",
    "                else:\n",
    "                    # Si no hay parada previa, mantener en 999 o asignar otro valor\n",
    "                    pass\n",
    "            \n",
    "            return group\n",
    "        \n",
    "        # Aplicar cálculo por piloto\n",
    "        try:\n",
    "            merged_df = merged_df.groupby('Driver').apply(calculate_laps_since_pitstop).reset_index(drop=True)\n",
    "            \n",
    "            # Vueltas desde la primera vuelta para el primer stint\n",
    "            merged_df.loc[merged_df['LapsSincePitStop'] == 999, 'LapsSincePitStop'] = merged_df.loc[merged_df['LapsSincePitStop'] == 999, 'LapNumber']\n",
    "        except Exception as e:\n",
    "            print(f\"Error al calcular LapsSincePitStop: {e}. Continuando sin esta característica.\")\n",
    "    \n",
    "    # Si tenemos intervals, intentar unirlos también\n",
    "    if intervals_df is not None:\n",
    "        # Verificar columnas disponibles\n",
    "        print(\"Columnas en intervals_df:\", intervals_df.columns.tolist())\n",
    "        \n",
    "        # Intentar unir por driver_number y LapNumber si están disponibles\n",
    "        if 'driver_number' in intervals_df.columns and 'DriverNumber' in merged_df.columns:\n",
    "            print(\"Uniendo con intervals_df usando driver_number...\")\n",
    "            # Convertir a mismo tipo\n",
    "            intervals_df['driver_number'] = intervals_df['driver_number'].astype(str)\n",
    "            merged_df['DriverNumber'] = merged_df['DriverNumber'].astype(str)\n",
    "            \n",
    "            # Unir en driver_number\n",
    "            merged_df = pd.merge(\n",
    "                merged_df,\n",
    "                intervals_df,\n",
    "                left_on=['DriverNumber'],\n",
    "                right_on=['driver_number'],\n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            # Intentar encontrar otras columnas comunes\n",
    "            common_cols = set(merged_df.columns).intersection(set(intervals_df.columns))\n",
    "            if len(common_cols) > 0:\n",
    "                print(f\"Uniendo con intervals_df usando columna común: {list(common_cols)[0]}\")\n",
    "                \n",
    "                join_col = list(common_cols)[0]\n",
    "                merged_df = pd.merge(\n",
    "                    merged_df,\n",
    "                    intervals_df,\n",
    "                    on=join_col,\n",
    "                    how='left'\n",
    "                )\n",
    "            else:\n",
    "                print(\"No se encontraron columnas comunes para unir con intervals_df.\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Unir todos los datos\n",
    "merged_data = merge_all_data(\n",
    "    laps_df, \n",
    "    weather_df, \n",
    "    intervals_df if has_intervals else None,\n",
    "    pitstops_df if has_pitstops else None\n",
    ")\n",
    "\n",
    "# Verificar el resultado\n",
    "print(\"Dimensiones del DataFrame combinado:\", merged_data.shape)\n",
    "display(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modeling(df):\n",
    "    \"\"\"\n",
    "    Realiza la limpieza y feature engineering en el DataFrame combinado\n",
    "    \"\"\"\n",
    "    # Trabajar con una copia para no modificar el original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Convertir LapTime a segundos (si es timedelta)\n",
    "    if 'LapTime' in data.columns and pd.api.types.is_timedelta64_dtype(data['LapTime']):\n",
    "        data['LapTime'] = data['LapTime'].dt.total_seconds()\n",
    "        print(\"Convertida LapTime a segundos.\")\n",
    "    \n",
    "    # Convertir tiempos de sector a segundos si están disponibles\n",
    "    for sector in ['Sector1Time', 'Sector2Time', 'Sector3Time']:\n",
    "        if sector in data.columns and pd.api.types.is_timedelta64_dtype(data[sector]):\n",
    "            data[sector] = data[sector].dt.total_seconds()\n",
    "            print(f\"Convertido {sector} a segundos.\")\n",
    "    \n",
    "    # Eliminar outliers en LapTime\n",
    "    if 'LapTime' in data.columns:\n",
    "        # Filtrar vueltas válidas (eliminar outliers)\n",
    "        q1 = data['LapTime'].quantile(0.05)\n",
    "        q3 = data['LapTime'].quantile(0.95)\n",
    "        data = data[(data['LapTime'] >= q1) & (data['LapTime'] <= q3)]\n",
    "        print(f\"Eliminados outliers en LapTime. Rango válido: {q1:.2f}s - {q3:.2f}s\")\n",
    "    \n",
    "    # Feature Engineering\n",
    "    \n",
    "    # 1. Edad de los neumáticos\n",
    "    if 'TyreLife' in data.columns:\n",
    "        data['TyreAge'] = data['TyreLife']\n",
    "        print(\"Creada feature: TyreAge\")\n",
    "    \n",
    "    # 2. Cambio de posición (comparado con la vuelta anterior)\n",
    "    if 'Position' in data.columns and 'Driver' in data.columns:\n",
    "        data['PositionChange'] = data.groupby('Driver')['Position'].diff().fillna(0)\n",
    "        print(\"Creada feature: PositionChange\")\n",
    "    \n",
    "    # 3. Carga de combustible (aproximación basada en la vuelta)\n",
    "    if 'LapNumber' in data.columns:\n",
    "        max_lap = data['LapNumber'].max()\n",
    "        data['FuelLoad'] = 1 - (data['LapNumber'] / max_lap)  # Aproximación simple\n",
    "        print(\"Creada feature: FuelLoad (aproximación)\")\n",
    "    \n",
    "    # 4. DRS usado (si está disponible)\n",
    "    if 'drs_window' in data.columns:\n",
    "        data['DRSUsed'] = data['drs_window'].astype(int)\n",
    "        print(\"Creada feature: DRSUsed\")\n",
    "    \n",
    "    # 5. Ventana de undercut (si está disponible)\n",
    "    if 'undercut_window' in data.columns:\n",
    "        data['UndercutWindow'] = data['undercut_window'].astype(int)\n",
    "        print(\"Creada feature: UndercutWindow\")\n",
    "    \n",
    "    # 6. Gap al líder (si está disponible)\n",
    "    if 'gap_to_leader_numeric' in data.columns:\n",
    "        # Convertir a float por si acaso\n",
    "        data['GapToLeader'] = pd.to_numeric(data['gap_to_leader_numeric'], errors='coerce')\n",
    "        print(\"Creada feature: GapToLeader\")\n",
    "    \n",
    "    # 7. Piloto en vuelta perdida (si está disponible)\n",
    "    if 'is_lapped' in data.columns:\n",
    "        data['IsLapped'] = data['is_lapped'].astype(int)\n",
    "        print(\"Creada feature: IsLapped\")\n",
    "    \n",
    "    # 8. Indicador de parada en la siguiente vuelta (si está disponible)\n",
    "    if 'PitNextLap' in data.columns:\n",
    "        print(\"Creada feature: PitNextLap (ya existente)\")\n",
    "    \n",
    "    # 9. Vueltas desde última parada (si está disponible)\n",
    "    if 'LapsSincePitStop' in data.columns:\n",
    "        print(\"Creada feature: LapsSincePitStop (ya existente)\")\n",
    "    \n",
    "    # 10. Duración de parada (si está disponible)\n",
    "    if 'PitDuration' in data.columns:\n",
    "        # Crear una columna con el tiempo perdido en pit\n",
    "        # Para vueltas sin parada, el valor es 0\n",
    "        data['PitTimeLost'] = data['PitDuration'].fillna(0)\n",
    "        print(\"Creada feature: PitTimeLost\")\n",
    "    \n",
    "    # 11. Cambio de compuesto en la próxima parada (si está disponible)\n",
    "    if 'NextCompound' in data.columns and 'Compound' in data.columns:\n",
    "        # Marcar cuando hay un cambio de compuesto (soft->medium, etc.)\n",
    "        data['CompoundChange'] = (data['NextCompound'] != data['Compound']).astype(int)\n",
    "        print(\"Creada feature: CompoundChange\")\n",
    "    \n",
    "    # Asegurarse de tener el nombre correcto de la columna de compuesto\n",
    "    if 'Compound' not in data.columns and 'TyreCompound' in data.columns:\n",
    "        data['Compound'] = data['TyreCompound']\n",
    "    \n",
    "    # Seleccionar columnas relevantes para modelado\n",
    "    features = [\n",
    "        # Datos básicos de vuelta\n",
    "        'LapNumber', 'Compound', 'TyreAge', 'Position', 'PositionChange', 'FuelLoad',\n",
    "        # Velocidades (si están disponibles)\n",
    "        'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST',\n",
    "        # Clima\n",
    "        'TrackTemp', 'AirTemp', 'Humidity', 'Pressure', 'WindSpeed',\n",
    "        # Status\n",
    "        'TrackStatus', 'IsAccurate',\n",
    "        # Paradas\n",
    "        'PitNextLap', 'LapsSincePitStop', 'PitTimeLost', 'CompoundChange',\n",
    "        # Intervalos (si están disponibles)\n",
    "        'DRSUsed', 'UndercutWindow', 'GapToLeader', 'IsLapped',\n",
    "        # Driver\n",
    "        'Driver'\n",
    "    ]\n",
    "    \n",
    "    # Filtrar solo columnas que existen en el DataFrame\n",
    "    available_features = [f for f in features if f in data.columns]\n",
    "    print(f\"\\nCaracterísticas disponibles para modelado ({len(available_features)}):\\n{available_features}\")\n",
    "    \n",
    "    # Seleccionar solo filas con LapTime y features disponibles\n",
    "    model_data = data[available_features + ['LapTime']].dropna(subset=['LapTime'])\n",
    "    \n",
    "    # Eliminar filas con NaN en features críticas\n",
    "    critical_features = [f for f in ['Compound', 'LapNumber', 'TrackTemp'] if f in available_features]\n",
    "    if critical_features:\n",
    "        model_data = model_data.dropna(subset=critical_features)\n",
    "    \n",
    "    print(f\"\\nDimensiones finales del DataFrame para modelado: {model_data.shape}\")\n",
    "    \n",
    "    return model_data, available_features\n",
    "\n",
    "# Preparar datos para modelado\n",
    "model_data, available_features = prepare_data_for_modeling(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico\n",
    "print(\"Resumen estadístico de variables numéricas:\")\n",
    "display(model_data.describe())\n",
    "\n",
    "# Distribución de LapTime\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(model_data['LapTime'], kde=True)\n",
    "plt.title('Distribución de Tiempos por Vuelta')\n",
    "plt.xlabel('Tiempo (segundos)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.savefig('outputs/week3/laptime_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# Tiempo por vuelta según tipo de neumático\n",
    "if 'Compound' in model_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Compound', y='LapTime', data=model_data)\n",
    "    plt.title('Tiempo por Vuelta según Tipo de Neumático')\n",
    "    plt.xlabel('Compuesto')\n",
    "    plt.ylabel('Tiempo (segundos)')\n",
    "    plt.savefig('outputs/week3/laptime_by_tyre.png')\n",
    "    plt.show()\n",
    "\n",
    "# Evolución del tiempo por vuelta a lo largo de la carrera\n",
    "if 'LapNumber' in model_data.columns and 'Driver' in model_data.columns:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    top_drivers = model_data['Driver'].value_counts().nlargest(5).index.tolist()\n",
    "    for driver in top_drivers:\n",
    "        driver_data = model_data[model_data['Driver'] == driver]\n",
    "        plt.plot(driver_data['LapNumber'], driver_data['LapTime'], marker='o', linestyle='-', label=driver)\n",
    "    plt.title('Evolución del Tiempo por Vuelta - Top 5 Pilotos')\n",
    "    plt.xlabel('Número de Vuelta')\n",
    "    plt.ylabel('Tiempo (segundos)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('outputs/week3/laptime_evolution.png')\n",
    "    plt.show()\n",
    "\n",
    "# Matriz de correlación\n",
    "# Seleccionar solo columnas numéricas\n",
    "numeric_data = model_data.select_dtypes(include=['int64', 'float64'])\n",
    "# Limitar a 15 columnas para mejor visualización\n",
    "if numeric_data.shape[1] > 15:\n",
    "    # Incluir LapTime y las columnas más correlacionadas con ella\n",
    "    if 'LapTime' in numeric_data.columns:\n",
    "        corr_with_laptime = numeric_data.corr()['LapTime'].abs().sort_values(ascending=False)\n",
    "        top_columns = corr_with_laptime.index[:14].tolist()  # 14 + LapTime = 15\n",
    "        if 'LapTime' not in top_columns:  # Por si acaso\n",
    "            top_columns = ['LapTime'] + top_columns[:13]\n",
    "        numeric_data = numeric_data[top_columns]\n",
    "    else:\n",
    "        numeric_data = numeric_data.iloc[:, :15]  # Primeras 15 columnas\n",
    "\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Filtrar correlaciones con LapTime\n",
    "if 'LapTime' in correlation_matrix.columns:\n",
    "    laptime_correlations = correlation_matrix['LapTime'].sort_values(ascending=False)\n",
    "    print(\"\\nCorrelaciones con LapTime:\")\n",
    "    print(laptime_correlations)\n",
    "\n",
    "# Visualizar matriz de correlación\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
    "plt.title('Matriz de Correlación')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/week3/correlation_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Diagrama de dispersión: Variables importantes vs LapTime\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "if 'TyreAge' in model_data.columns:\n",
    "    sns.scatterplot(x='TyreAge', y='LapTime', hue='Compound', data=model_data, ax=axes[0])\n",
    "    axes[0].set_title('LapTime vs TyreAge')\n",
    "    \n",
    "if 'TrackTemp' in model_data.columns:\n",
    "    sns.scatterplot(x='TrackTemp', y='LapTime', data=model_data, ax=axes[1])\n",
    "    axes[1].set_title('LapTime vs TrackTemp')\n",
    "    \n",
    "if 'FuelLoad' in model_data.columns:\n",
    "    sns.scatterplot(x='FuelLoad', y='LapTime', data=model_data, ax=axes[2])\n",
    "    axes[2].set_title('LapTime vs FuelLoad')\n",
    "\n",
    "if 'Position' in model_data.columns:\n",
    "    sns.scatterplot(x='Position', y='LapTime', data=model_data, ax=axes[3])\n",
    "    axes[3].set_title('LapTime vs Position')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/week3/key_variables_scatter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Análisis Específico de Paradas en Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pitstop_impact(model_data):\n",
    "    \"\"\"\n",
    "    Visualiza el impacto de las paradas en los tiempos por vuelta\n",
    "    \"\"\"\n",
    "    if 'LapsSincePitStop' not in model_data.columns:\n",
    "        print(\"No hay datos de paradas disponibles para visualización.\")\n",
    "        return\n",
    "    \n",
    "    # 1. Efecto de la edad de los neumáticos en el tiempo por vuelta\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Filtrar para mostrar hasta 20 vueltas después de una parada\n",
    "    plot_data = model_data[model_data['LapsSincePitStop'] <= 20].copy()\n",
    "    \n",
    "    # Agrupar por tipo de compuesto y vueltas desde parada\n",
    "    plot_data['LapsSincePitStop'] = plot_data['LapsSincePitStop'].astype(int)\n",
    "    grouped = plot_data.groupby(['Compound', 'LapsSincePitStop'])['LapTime'].mean().reset_index()\n",
    "    \n",
    "    # Dibujar una línea por cada compuesto\n",
    "    for compound in grouped['Compound'].unique():\n",
    "        compound_data = grouped[grouped['Compound'] == compound]\n",
    "        plt.plot(compound_data['LapsSincePitStop'], \n",
    "                 compound_data['LapTime'], \n",
    "                 'o-', \n",
    "                 label=f'Compuesto {compound}')\n",
    "    \n",
    "    plt.xlabel('Vueltas desde la parada')\n",
    "    plt.ylabel('Tiempo por vuelta promedio (s)')\n",
    "    plt.title('Degradación de neumáticos por compuesto')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('outputs/week3/tyre_degradation.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Proporción de paradas por compuesto\n",
    "    if 'PitNextLap' in model_data.columns and model_data['PitNextLap'].sum() > 0:\n",
    "        pitstop_data = model_data[model_data['PitNextLap'] == 1]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(x='Compound', data=pitstop_data)\n",
    "        plt.title('Compuestos utilizados antes de paradas')\n",
    "        plt.xlabel('Compuesto')\n",
    "        plt.ylabel('Número de paradas')\n",
    "        plt.savefig('outputs/week3/compounds_before_pitstop.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Distribución de duración de paradas\n",
    "        if 'PitTimeLost' in model_data.columns:\n",
    "            pitstop_times = model_data[model_data['PitTimeLost'] > 0]['PitTimeLost']\n",
    "            \n",
    "            if not pitstop_times.empty:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.histplot(pitstop_times, kde=True, bins=15)\n",
    "                plt.axvline(pitstop_times.mean(), color='r', linestyle='--', \n",
    "                           label=f'Media: {pitstop_times.mean():.2f}s')\n",
    "                plt.title('Distribución de tiempo perdido en paradas')\n",
    "                plt.xlabel('Tiempo (s)')\n",
    "                plt.ylabel('Frecuencia')\n",
    "                plt.legend()\n",
    "                plt.savefig('outputs/week3/pitstop_duration.png')\n",
    "                plt.show()\n",
    "    \n",
    "    # 4. Comparación de tiempos antes/después de paradas\n",
    "    if 'PitNextLap' in model_data.columns and 'LapsSincePitStop' in model_data.columns:\n",
    "        # Obtener vueltas justo antes de una parada\n",
    "        pre_stop_laps = model_data[model_data['PitNextLap'] == 1].copy()\n",
    "        if not pre_stop_laps.empty:\n",
    "            pre_stop_laps['LapType'] = 'Pre-Stop'\n",
    "            \n",
    "            # Obtener primera vuelta después de cada parada (LapsSincePitStop = 1)\n",
    "            post_stop_laps = model_data[model_data['LapsSincePitStop'] == 1].copy()\n",
    "            if not post_stop_laps.empty:\n",
    "                post_stop_laps['LapType'] = 'Post-Stop'\n",
    "                \n",
    "                # Combinar datos\n",
    "                comparison_df = pd.concat([pre_stop_laps, post_stop_laps])\n",
    "                \n",
    "                # Visualizar\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.boxplot(x='Compound', y='LapTime', hue='LapType', data=comparison_df)\n",
    "                plt.title('Comparación de tiempos antes y después de paradas')\n",
    "                plt.xlabel('Compuesto')\n",
    "                plt.ylabel('Tiempo por vuelta (s)')\n",
    "                plt.savefig('outputs/week3/pre_post_stop_comparison.png')\n",
    "                plt.show()\n",
    "\n",
    "# Llamar a la función de visualización\n",
    "if has_pitstops:\n",
    "    print(\"\\nAnalizando impacto de paradas en tiempos por vuelta...\")\n",
    "    visualize_pitstop_impact(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preprocesamiento para Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_for_modeling(df, features):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos para el modelado\n",
    "    \"\"\"\n",
    "    # Separar características y objetivo\n",
    "    X = df[features]\n",
    "    y = df['LapTime']\n",
    "    \n",
    "    # Identificar columnas categóricas y numéricas\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Mostrar información sobre las columnas\n",
    "    print(f\"Características categóricas ({len(cat_cols)}): {cat_cols}\")\n",
    "    print(f\"Características numéricas ({len(num_cols)}): {num_cols}\")\n",
    "    \n",
    "    # Crear preprocesadores\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Combinar preprocesadores\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, num_cols),\n",
    "            ('cat', categorical_transformer, cat_cols)\n",
    "        ])\n",
    "    \n",
    "    # Dividir datos en entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Tamaño de conjunto de entrenamiento: {X_train.shape}\")\n",
    "    print(f\"Tamaño de conjunto de prueba: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "# Preprocesar datos para modelado\n",
    "X_train, X_test, y_train, y_test, preprocessor = preprocess_data_for_modeling(model_data, available_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entrenamiento de Modelo XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Entrena un modelo XGBoost\n",
    "    \"\"\"\n",
    "    # Crear pipeline con preprocesamiento y modelo\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', xgb.XGBRegressor(objective='reg:squarederror'))\n",
    "    ])\n",
    "    \n",
    "    # Parámetros para Grid Search\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__learning_rate': [0.01, 0.1],\n",
    "        'regressor__max_depth': [3, 5, 7],\n",
    "        'regressor__min_child_weight': [1, 3]\n",
    "    }\n",
    "    \n",
    "    # Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"Entrenando modelo XGBoost con GridSearchCV (esto puede tardar varios minutos)...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Mejores parámetros\n",
    "    print(\"\\nMejores parámetros XGBoost:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Predecir\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluar\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nMétricas de evaluación del modelo XGBoost:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f} segundos\")\n",
    "    print(f\"  MAE: {mae:.4f} segundos\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    \n",
    "    # Analizar características importantes\n",
    "    if hasattr(best_model.named_steps['regressor'], 'feature_importances_'):\n",
    "        # Obtener nombres de características después del preprocesamiento\n",
    "        try:\n",
    "            cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "            \n",
    "            # Obtener nombres de características después de one-hot encoding\n",
    "            encoder = best_model.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot']\n",
    "            cat_features = encoder.get_feature_names_out(cat_cols).tolist()\n",
    "            all_features = num_cols + cat_features\n",
    "            \n",
    "            # Crear DataFrame con importancias\n",
    "            importances = best_model.named_steps['regressor'].feature_importances_\n",
    "            feature_importance = pd.DataFrame({'Feature': all_features, 'Importance': importances})\n",
    "            feature_importance = feature_importance.sort_values('Importance', ascending=False).head(15)\n",
    "            \n",
    "            # Visualizar\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "            plt.title('Top 15 Características Más Importantes - XGBoost')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('outputs/week3/xgboost_feature_importance.png')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error al analizar importancia de características: {e}\")\n",
    "    \n",
    "    return best_model, y_pred\n",
    "\n",
    "# Entrenar modelo XGBoost\n",
    "xgb_model, y_pred_xgb = train_xgboost(X_train, X_test, y_train, y_test, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Entrenamiento de Red Neuronal (Opcional)\n",
    "\n",
    "Este paso es opcional y puede omitirse si prefieres usar solo XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch_nn(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de red neuronal con PyTorch\n",
    "    \"\"\"\n",
    "    # Aplicar preprocesamiento\n",
    "    print(\"Preprocesando datos para la red neuronal...\")\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    X_train_tensor = torch.FloatTensor(X_train_processed.toarray())\n",
    "    y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_processed.toarray())\n",
    "    y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1)\n",
    "    \n",
    "    # Crear conjuntos de datos y cargadores\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Inicializar modelo\n",
    "    print(f\"Inicializando red neuronal con {X_train_processed.shape[1]} entradas...\")\n",
    "    input_size = X_train_processed.shape[1]\n",
    "    model = LapTimeNN(input_size)\n",
    "    \n",
    "    # Definir criterio y optimizador\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Guardar historial de pérdidas para graficar\n",
    "    losses = []\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"\\nEntrenando red neuronal...\")\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Visualizar curva de aprendizaje\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs+1), losses)\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.title('Curva de Aprendizaje - Red Neuronal')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('outputs/week3/neural_network_learning_curve.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "        y_pred = y_pred_tensor.numpy().flatten()\n",
    "        \n",
    "    # Métricas\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nMétricas de evaluación de la Red Neuronal:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f} segundos\")\n",
    "    print(f\"  MAE: {mae:.4f} segundos\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    \n",
    "    return model, y_pred\n",
    "\n",
    "# Preguntar si se desea entrenar la red neuronal\n",
    "train_nn = True  # Cambiar a False para omitir este paso\n",
    "if train_nn:\n",
    "    nn_model, y_pred_nn = train_pytorch_nn(X_train, X_test, y_train, y_test, preprocessor)\n",
    "else:\n",
    "    nn_model, y_pred_nn = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualización y Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(y_test, y_pred_xgb, y_pred_nn=None, max_points=1000):\n",
    "    \"\"\"\n",
    "    Visualiza los resultados de los modelos\n",
    "    \"\"\"\n",
    "    # Limitar número de puntos para visualización clara\n",
    "    if len(y_test) > max_points:\n",
    "        # Muestrear aleatoriamente para mantener la distribución\n",
    "        indices = np.random.choice(len(y_test), max_points, replace=False)\n",
    "        y_test_sample = y_test.iloc[indices]\n",
    "        y_pred_xgb_sample = y_pred_xgb[indices]\n",
    "        if y_pred_nn is not None:\n",
    "            y_pred_nn_sample = y_pred_nn[indices]\n",
    "    else:\n",
    "        y_test_sample = y_test\n",
    "        y_pred_xgb_sample = y_pred_xgb\n",
    "        y_pred_nn_sample = y_pred_nn\n",
    "    \n",
    "    # Configurar tamaño de figura\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Determinar número de subplots\n",
    "    if y_pred_nn is not None:\n",
    "        n_plots = 3\n",
    "    else:\n",
    "        n_plots = 2\n",
    "    \n",
    "    # 1. Dispersión XGBoost\n",
    "    plt.subplot(1, n_plots, 1)\n",
    "    plt.scatter(y_test_sample, y_pred_xgb_sample, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Tiempo real (s)')\n",
    "    plt.ylabel('Tiempo predicho (s)')\n",
    "    plt.title('XGBoost: Predicciones vs Reales')\n",
    "    \n",
    "    # 2. Residuos XGBoost\n",
    "    plt.subplot(1, n_plots, 2)\n",
    "    residuals_xgb = y_test_sample - y_pred_xgb_sample\n",
    "    plt.scatter(y_pred_xgb_sample, residuals_xgb, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicción (s)')\n",
    "    plt.ylabel('Residuos (s)')\n",
    "    plt.title('XGBoost: Residuos vs Predicciones')\n",
    "    \n",
    "    # 3. Si hay predicciones de red neuronal\n",
    "    if y_pred_nn is not None:\n",
    "        plt.subplot(1, n_plots, 3)\n",
    "        plt.scatter(y_test_sample, y_pred_nn_sample, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.xlabel('Tiempo real (s)')\n",
    "        plt.ylabel('Tiempo predicho (s)')\n",
    "        plt.title('Red Neuronal: Predicciones vs Reales')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/week3/prediction_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Comparativa de errores\n",
    "    if y_pred_nn is not None:\n",
    "        errors_xgb = np.abs(y_test - y_pred_xgb)\n",
    "        errors_nn = np.abs(y_test - y_pred_nn)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(errors_xgb, alpha=0.5, bins=50, label='XGBoost')\n",
    "        plt.hist(errors_nn, alpha=0.5, bins=50, label='Red Neuronal')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Error Absoluto (s)')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.title('Distribución de Errores por Modelo')\n",
    "        plt.xlim(0, min(10, max(errors_xgb.max(), errors_nn.max())))\n",
    "        plt.savefig('outputs/week3/error_distribution.png')\n",
    "        plt.show()\n",
    "\n",
    "# Visualizar resultados\n",
    "visualize_predictions(y_test, y_pred_xgb, y_pred_nn if train_nn else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Guardar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(xgb_model, nn_model=None):\n",
    "    \"\"\"\n",
    "    Guarda los modelos entrenados\n",
    "    \"\"\"\n",
    "    # Guardar modelo XGBoost\n",
    "    joblib.dump(xgb_model, 'models/week3/xgboost_laptime.joblib')\n",
    "    print(\"Modelo XGBoost guardado en 'models/week3/xgboost_laptime.joblib'\")\n",
    "    \n",
    "    # Guardar modelo PyTorch si existe\n",
    "    if nn_model is not None:\n",
    "        torch.save(nn_model.state_dict(), 'models/week3/nn_laptime.pth')\n",
    "        print(\"Modelo PyTorch guardado en 'models/week3/nn_laptime.pth'\")\n",
    "\n",
    "# Guardar modelos\n",
    "save_models(xgb_model, nn_model if train_nn else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Prueba de Predicción con Nuevos Datos\n",
    "\n",
    "Probemos el modelo con una situación de carrera hipotética."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model, available_features):\n",
    "    \"\"\"\n",
    "    Prueba el modelo con datos hipotéticos\n",
    "    \"\"\"\n",
    "    # Crear un escenario de ejemplo\n",
    "    example_drivers = [\"VER\", \"HAM\", \"LEC\", \"PER\", \"SAI\"]\n",
    "    compounds = [\"SOFT\", \"MEDIUM\", \"HARD\"]\n",
    "    \n",
    "    # Crear DataFrame de ejemplo\n",
    "    example_data = []\n",
    "    \n",
    "    for driver in example_drivers:\n",
    "        for compound in compounds:\n",
    "            for tyre_age in [1, 10, 20]:\n",
    "                # Valores predeterminados\n",
    "                row = {\n",
    "                    'Driver': driver,\n",
    "                    'Compound': compound,\n",
    "                    'TyreAge': tyre_age,\n",
    "                    'LapNumber': 30,\n",
    "                    'TrackTemp': 35.0,\n",
    "                    'AirTemp': 25.0,\n",
    "                    'Position': 5,\n",
    "                    'FuelLoad': 0.5,\n",
    "                    'LapsSincePitStop': tyre_age,  # Si está disponible\n",
    "                    'PitNextLap': 0,  # No hay parada en la siguiente vuelta\n",
    "                }\n",
    "                \n",
    "                # Añadir solo características disponibles\n",
    "                example_row = {k: v for k, v in row.items() if k in available_features}\n",
    "                example_data.append(example_row)\n",
    "    \n",
    "    example_df = pd.DataFrame(example_data)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    predictions = model.predict(example_df)\n",
    "    \n",
    "    # Añadir predicciones al DataFrame\n",
    "    example_df['PredictedLapTime'] = predictions\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    print(\"\\nPredicciones para escenarios de ejemplo:\")\n",
    "    display(example_df)\n",
    "    \n",
    "    # Graficar por compuesto y edad de neumáticos\n",
    "    if 'Compound' in example_df.columns and 'TyreAge' in example_df.columns:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for driver in example_drivers[:3]:  # Limitar a 3 pilotos para claridad\n",
    "            driver_data = example_df[example_df['Driver'] == driver]\n",
    "            for compound in compounds:\n",
    "                tyre_data = driver_data[driver_data['Compound'] == compound]\n",
    "                plt.plot(tyre_data['TyreAge'], tyre_data['PredictedLapTime'], \n",
    "                         marker='o', linestyle='-', label=f\"{driver} - {compound}\")\n",
    "                \n",
    "        plt.xlabel('Edad de Neumáticos (vueltas)')\n",
    "        plt.ylabel('Tiempo Predicho (s)')\n",
    "        plt.title('Predicción de Tiempos por Tipo y Edad de Neumáticos')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('outputs/week3/tyre_age_predictions.png')\n",
    "        plt.show()\n",
    "    \n",
    "    return example_df\n",
    "\n",
    "# Probar predicciones con el modelo XGBoost\n",
    "try:\n",
    "    prediction_examples = test_prediction(xgb_model, available_features)\n",
    "except Exception as e:\n",
    "    print(f\"Error al realizar predicciones de prueba: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusiones y Próximos Pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "En este notebook, hemos:\n",
    "1. Cargado y preparado datos de FastF1 y OpenF1, incluyendo información de vueltas, paradas y condiciones meteorológicas\n",
    "2. Realizado feature engineering para crear características como edad de neumáticos, carga de combustible, y paradas en boxes\n",
    "3. Analizado el impacto de las paradas y la degradación de neumáticos en los tiempos por vuelta\n",
    "4. Entrenado un modelo XGBoost para predecir tiempos por vuelta\n",
    "5. Opcionalmente, entrenado una red neuronal para comparar su rendimiento\n",
    "6. Visualizado y evaluado los resultados\n",
    "\n",
    "### Resultados Principales\n",
    "- El modelo XGBoost puede predecir tiempos por vuelta con un error medio de X segundos\n",
    "- Las características más importantes para la predicción son [listar las top 3-5 características]\n",
    "- La degradación de neumáticos tiene un impacto significativo en los tiempos por vuelta\n",
    "- Las paradas en boxes muestran patrones claros en cuanto a su timing y elección de compuesto\n",
    "\n",
    "### Próximos Pasos (Semana 4)\n",
    "1. Integrar este modelo de predicción con un sistema de decisiones basado en reglas\n",
    "2. Añadir lógica para recomendar estrategias de paradas en boxes\n",
    "3. Desarrollar sistema para simular undercuts/overcuts basados en las predicciones\n",
    "4. Crear visualizaciones interactivas para analizar el impacto de diferentes estrategias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
