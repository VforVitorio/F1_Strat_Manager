{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision: Gap Calculation using YOLO net and OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from collections import Counter\n",
    "import time as pytime\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): DetectionModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-3): 4 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-3): 4 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(384, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (11): Concat()\n",
       "      (12): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(960, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (14): Concat()\n",
       "      (15): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (17): Concat()\n",
       "      (18): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): Conv(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (20): Concat()\n",
       "      (21): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): Detect(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the Anti-Alpine optimized model\n",
    "model = YOLO(\"../f1-strategy/weights/model_anti_alpine.pt\")\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gap Calculation and Yolo video processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dedining constants and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size and scale configuration\n",
    "FRAME_WIDTH = 1280  # Higher resolution for better detail\n",
    "CAR_LENGTH_METERS = 5.63  # Real car length in meters\n",
    "\n",
    "# FOR GAPS: use a lower global threshold to maximize detections\n",
    "GAP_DETECTION_THRESHOLD = 0.25  # Low threshold to detect all possible cars\n",
    "\n",
    "# Specific colors for each F1 team (BGR format for OpenCV)\n",
    "class_colors = {\n",
    "    'Ferrari': (0, 0, 255),         # Red (BGR)\n",
    "    'Mercedes': (200, 200, 200),    # Silver (BGR)\n",
    "    'Red Bull': (139, 0, 0),        # Dark Blue (BGR)\n",
    "    'McLaren': (0, 165, 255),       # Orange (BGR)\n",
    "    'Aston Martin': (0, 128, 0),    # Green (BGR)\n",
    "    'Alpine': (128, 0, 0),          # Blue (BGR)\n",
    "    'Williams': (205, 0, 0),        # Light Blue (BGR)\n",
    "    'Haas': (255, 255, 255),        # White (BGR)\n",
    "    'Kick Sauber': (255, 255, 0),   # Cyan (BGR)\n",
    "    'Racing Bulls': (0, 0, 255),    # Red (BGR)\n",
    "    'background': (128, 128, 128),  # Gray (BGR)\n",
    "    'unknown': (0, 255, 255)        # Yellow for cars without secure classification\n",
    "}\n",
    "\n",
    "# Thresholds to show classification (not for filtering detections)\n",
    "class_thresholds = {\n",
    "    'Williams': 0.90,     # Very high for Williams\n",
    "    'Alpine': 0.90,       # Extremely high for Alpine\n",
    "    'McLaren': 0.30,      # Low for McLaren\n",
    "    'Red Bull': 0.85,     # High for Red Bull\n",
    "    'Ferrari': 0.40,      # Normal\n",
    "    'Mercedes': 0.50,     # Medium-high\n",
    "    'Haas': 0.40,         # Normal\n",
    "    'Kick Sauber': 0.40,  # Normal\n",
    "    'Racing Bulls': 0.40, # Normal\n",
    "    'Aston Martin': 0.40, # Normal\n",
    "    'background': 0.50    # High for background\n",
    "}\n",
    "\n",
    "# Detection history for stabilization\n",
    "last_detections = {}\n",
    "track_history = {}\n",
    "id_counter = 0\n",
    "class_history = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Calculating the gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gap(box1, box2, class1, class2):\n",
    "    \"\"\"Calculates the distance between centers using car width for scale\"\"\"\n",
    "    # Box centers\n",
    "    cx1, cy1 = (box1[0] + box1[2])/2, (box1[1] + box1[3])/2\n",
    "    cx2, cy2 = (box2[0] + box2[2])/2, (box2[1] + box2[3])/2\n",
    "    \n",
    "    # Distance in pixels\n",
    "    pixel_distance = np.hypot(cx2 - cx1, cy2 - cy1)\n",
    "    \n",
    "    # Scale based on average width of detected cars\n",
    "    avg_width = ((box1[2] - box1[0]) + (box2[2] - box2[0])) / 2\n",
    "    scale = CAR_LENGTH_METERS / avg_width if avg_width != 0 else 0\n",
    "    \n",
    "    # Calculate gap time at 300km/h (83.33 m/s)\n",
    "    speed_mps = 83.33  # Meters per second at 300km/h\n",
    "    gap_time = (pixel_distance * scale) / speed_mps\n",
    "    \n",
    "    return {\n",
    "        'distance': pixel_distance * scale,  # Distance in meters\n",
    "        'time': gap_time,                   # Time in seconds at 300km/h\n",
    "        'car1': class1,                     # Team of first car\n",
    "        'car2': class2                      # Team of second car\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Processing the video with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_with_yolo(video_path, output_path=None):\n",
    "    global last_detections, track_history, id_counter, class_history, GAP_DETECTION_THRESHOLD\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get original video dimensions\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    current_frame = 0\n",
    "    \n",
    "    # Calculate new height maintaining aspect ratio\n",
    "    target_height = int(FRAME_WIDTH * original_height / original_width)\n",
    "    \n",
    "    out = None\n",
    "    if output_path:\n",
    "        # Change codec from 'mp4v' to 'XVID' which is more reliable\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (FRAME_WIDTH, target_height))\n",
    "        if not out.isOpened():\n",
    "            print(f\"Error: Could not create output video file at {output_path}\")\n",
    "            print(\"Continuing without saving output...\")\n",
    "            output_path = None\n",
    "    \n",
    "    # Variables for calculating real FPS\n",
    "    frame_count = 0\n",
    "    start_time = pytime.time()\n",
    "    current_fps = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        current_frame += 1\n",
    "        \n",
    "        # Resize maintaining aspect ratio\n",
    "        frame_resized = cv2.resize(frame, (FRAME_WIDTH, target_height))\n",
    "        original_frame = frame_resized.copy()\n",
    "        \n",
    "        # Run YOLOv8 detection with low threshold to maximize detections\n",
    "        results = model.predict(\n",
    "            source=frame_resized, \n",
    "            conf=GAP_DETECTION_THRESHOLD,  # Low threshold to detect all possible cars\n",
    "            iou=0.45,   # Standard IoU\n",
    "            max_det=20, # Maximum detections\n",
    "            verbose=False\n",
    "        )[0]\n",
    "        \n",
    "        # Current detections\n",
    "        current_detections = {}\n",
    "        \n",
    "        # Process detection results\n",
    "        if results.boxes and len(results.boxes) > 0:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            confs = results.boxes.conf.cpu().numpy()\n",
    "            class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            # Create detection list with all information\n",
    "            detections = []\n",
    "            for i, box in enumerate(boxes):\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                conf = float(confs[i])\n",
    "                class_id = int(class_ids[i])\n",
    "                cls_name = model.names[class_id]\n",
    "                \n",
    "                # Determine whether to show team classification based on threshold\n",
    "                # Note: we still detect the car but might not show its team\n",
    "                classified = conf >= class_thresholds.get(cls_name, 0.40)\n",
    "                \n",
    "                # KEY: For gaps, we detect all cars even if we're not sure of the team\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                \n",
    "                # Assign unique ID or retrieve existing ID\n",
    "                object_id = None\n",
    "                for old_id, old_info in last_detections.items():\n",
    "                    old_cx, old_cy = old_info['center']\n",
    "                    old_cls = old_info['class']\n",
    "                    \n",
    "                    # Distance between centers\n",
    "                    dist = np.sqrt((center_x - old_cx)**2 + (center_y - old_cy)**2)\n",
    "                    \n",
    "                    # If it's close, it could be the same object\n",
    "                    if dist < 100:\n",
    "                        object_id = old_id\n",
    "                        \n",
    "                        # If previous class was valid and new one is uncertain, keep the previous one\n",
    "                        if old_info['classified'] and not classified:\n",
    "                            cls_name = old_cls\n",
    "                            classified = True\n",
    "                        \n",
    "                        # Stabilize classification with history for problematic classes\n",
    "                        if classified and old_cls != cls_name and (cls_name == 'Williams' or cls_name == 'Alpine'):\n",
    "                            if old_info['classified']:\n",
    "                                cls_name = old_cls\n",
    "                        break\n",
    "                \n",
    "                # If no match found, assign new ID\n",
    "                if object_id is None:\n",
    "                    object_id = id_counter\n",
    "                    id_counter += 1\n",
    "                    track_history[object_id] = []\n",
    "                    class_history[object_id] = []\n",
    "                \n",
    "                # Update history\n",
    "                if object_id in class_history:\n",
    "                    # Only add to history if we're sure of the class\n",
    "                    if classified:\n",
    "                        class_history[object_id].append(cls_name)\n",
    "                        # Limit history to 5 classes\n",
    "                        if len(class_history[object_id]) > 5:\n",
    "                            class_history[object_id].pop(0)\n",
    "                    \n",
    "                    # Use the most common class from history for stability\n",
    "                    if len(class_history[object_id]) >= 3:\n",
    "                        counts = Counter(class_history[object_id])\n",
    "                        if counts:  # Make sure it's not empty\n",
    "                            most_common = counts.most_common(1)[0][0]\n",
    "                            cls_name = most_common\n",
    "                            classified = True\n",
    "                \n",
    "                # Save current detection\n",
    "                current_detections[object_id] = {\n",
    "                    'box': (x1, y1, x2, y2),\n",
    "                    'conf': conf,\n",
    "                    'class': cls_name,\n",
    "                    'classified': classified,\n",
    "                    'center': (center_x, center_y),\n",
    "                    'area': area,\n",
    "                    'y_bottom': y2  # For sorting by vertical position\n",
    "                }\n",
    "                \n",
    "                # Add to detection list for gap calculation\n",
    "                detections.append({\n",
    "                    'id': object_id,\n",
    "                    'box': (x1, y1, x2, y2),\n",
    "                    'class': cls_name,\n",
    "                    'classified': classified,\n",
    "                    'conf': conf,\n",
    "                    'y_bottom': y2\n",
    "                })\n",
    "            \n",
    "            # Sort by vertical position (cars more below first - closer)\n",
    "            detections = sorted(detections, key=lambda x: x['y_bottom'], reverse=True)\n",
    "            \n",
    "            # Draw boxes and gaps\n",
    "            for i, det in enumerate(detections):\n",
    "                x1, y1, x2, y2 = det['box']\n",
    "                cls_name = det['class']\n",
    "                conf = det['conf']\n",
    "                classified = det['classified']\n",
    "                \n",
    "                # Get specific color for the team\n",
    "                if classified:\n",
    "                    color = class_colors.get(cls_name, (0, 255, 0))\n",
    "                else:\n",
    "                    color = class_colors['unknown']  # Yellow for cars without secure classification\n",
    "                \n",
    "                # Draw box with team color\n",
    "                cv2.rectangle(frame_resized, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # Label with class and confidence\n",
    "                if classified:\n",
    "                    label = f\"{cls_name}: {conf:.2f}\"\n",
    "                else:\n",
    "                    label = f\"F1 Car: {conf:.2f}\"\n",
    "                \n",
    "                t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "                cv2.rectangle(frame_resized, (x1, y1-t_size[1]-3), (x1+t_size[0], y1), color, -1)\n",
    "                cv2.putText(frame_resized, label, (x1, y1-3), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                \n",
    "                # Only if there's a next car\n",
    "                if i < len(detections)-1:\n",
    "                    next_det = detections[i+1]\n",
    "                    gap_info = calculate_gap(\n",
    "                        det['box'], next_det['box'], \n",
    "                        det['class'] if det['classified'] else \"F1 Car\", \n",
    "                        next_det['class'] if next_det['classified'] else \"F1 Car\"\n",
    "                    )\n",
    "                    \n",
    "                    # Connection points\n",
    "                    cx1, cy1 = int((x1+x2)/2), int(y1)  # Use top of the car\n",
    "                    nx1, ny1, nx2, ny2 = next_det['box']\n",
    "                    cx2, cy2 = int((nx1+nx2)/2), int(ny2)  # Use bottom of the next car\n",
    "                    \n",
    "                    # Diagonal line between cars\n",
    "                    cv2.line(frame_resized, (cx1, cy1), (cx2, cy2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Text at midpoint with more information\n",
    "                    mid_x, mid_y = (cx1+cx2)//2, (cy1+cy2)//2\n",
    "                    \n",
    "                    # Distance and gap time\n",
    "                    dist_text = f\"{gap_info['distance']:.1f}m\"\n",
    "                    time_text = f\"{gap_info['time']:.2f}s\"\n",
    "                    \n",
    "                    # Background for text\n",
    "                    dist_size = cv2.getTextSize(dist_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "                    time_size = cv2.getTextSize(time_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "                    \n",
    "                    # Draw semi-transparent background\n",
    "                    overlay = frame_resized.copy()\n",
    "                    cv2.rectangle(overlay, \n",
    "                                 (mid_x - 5, mid_y - 50), \n",
    "                                 (mid_x + max(dist_size[0], time_size[0]) + 10, mid_y + 10),\n",
    "                                 (0, 0, 0), -1)\n",
    "                    cv2.addWeighted(overlay, 0.6, frame_resized, 0.4, 0, frame_resized)\n",
    "                    \n",
    "                    # Draw texts\n",
    "                    cv2.putText(frame_resized, dist_text, (mid_x, mid_y - 25),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                    cv2.putText(frame_resized, time_text, (mid_x, mid_y),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Update last_detections for next iteration\n",
    "        last_detections = current_detections\n",
    "        \n",
    "        # Calculate FPS\n",
    "        if frame_count % 10 == 0:\n",
    "            current_time = pytime.time()\n",
    "            current_fps = 10.0 / (current_time - start_time)\n",
    "            start_time = current_time\n",
    "        \n",
    "        # Show FPS and model information\n",
    "        cv2.putText(frame_resized, f\"FPS: {current_fps:.1f}\", (20, 40),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.putText(frame_resized, \"F1 Gap Detection\", (FRAME_WIDTH - 300, 40),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Show detection mode and progress\n",
    "        detection_mode = f\"Detection Threshold: {GAP_DETECTION_THRESHOLD:.2f}\"\n",
    "        cv2.putText(frame_resized, detection_mode, (20, target_height - 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Show video progress\n",
    "        progress_text = f\"Frame: {current_frame}/{total_frames} ({current_frame/total_frames*100:.1f}%)\"\n",
    "        cv2.putText(frame_resized, progress_text, (FRAME_WIDTH - 400, target_height - 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Save processed frame if requested\n",
    "        if output_path:\n",
    "            out.write(frame_resized)\n",
    "        \n",
    "        # Show frame\n",
    "        cv2.imshow(\"F1 Gap Detection\", frame_resized)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('+'):  # Increase threshold\n",
    "            GAP_DETECTION_THRESHOLD = min(GAP_DETECTION_THRESHOLD + 0.05, 0.95)\n",
    "            print(f\"Detection threshold increased to {GAP_DETECTION_THRESHOLD:.2f}\")\n",
    "        elif key == ord('-'):  # Decrease threshold\n",
    "            GAP_DETECTION_THRESHOLD = max(GAP_DETECTION_THRESHOLD - 0.05, 0.05)\n",
    "            print(f\"Detection threshold decreased to {GAP_DETECTION_THRESHOLD:.2f}\")\n",
    "        elif key == ord('d'):  # Skip forward 10 seconds\n",
    "            skip_frames = int(fps * 10)  # 10 seconds * fps = number of frames to skip\n",
    "            new_frame_pos = min(current_frame + skip_frames, total_frames - 1)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, new_frame_pos)\n",
    "            current_frame = new_frame_pos - 1  # Will be incremented in the next cycle\n",
    "            # Temporarily reset tracking\n",
    "            last_detections = {}\n",
    "            print(f\"Skipped forward 10 seconds to frame {new_frame_pos}\")\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Running a demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with your video\n",
    "# def main():\n",
    "#     video_path = \"../f1-strategy/data/videos/best_overtakes_2023.mp4.f399.mp4\"\n",
    "#     video_path = \"../f1-strategy/data/videos/spain_2023_race.mp4.f399.mp4\"\n",
    "    \n",
    "#     video_path = \"../f1-strategy/data/videos/belgium_gp.f399.mp4\"\n",
    "#     output_path = \"../f1-strategy/data/videos/gap_detection_output.mp4\"\n",
    "#     process_video_with_yolo(video_path, output_path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controls\n",
    "\n",
    "'q': out\n",
    "'+': more detection threshold\n",
    "'-': less detection threshold\n",
    "'d': 10 seconds ahead "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gap Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Gap extraction function with visualization\n",
    "def extract_gaps_from_video(video_path, sample_interval_seconds=10, output_csv=None, show_video=True):\n",
    "    \"\"\"\n",
    "    Process a video and extract gap data at regular intervals with visualization\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to the F1 video\n",
    "        sample_interval_seconds: How often to sample gap data (in seconds)\n",
    "        output_csv: Path to save CSV data (if None, will generate a default path)\n",
    "        show_video: Whether to display the video during processing\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with extracted gap data\n",
    "    \"\"\"\n",
    "    global last_detections, track_history, id_counter, class_history, GAP_DETECTION_THRESHOLD\n",
    "    \n",
    "    # Initialize empty list to store all gap data\n",
    "    all_gaps = []\n",
    "    \n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Calculate frame interval based on desired time interval\n",
    "    frame_interval = int(fps * sample_interval_seconds)\n",
    "    print(f\"Video FPS: {fps}, sampling every {frame_interval} frames ({sample_interval_seconds} seconds)\")\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    frame_count = 0\n",
    "    current_frame = 0\n",
    "    last_sample_frame = -frame_interval  # Ensure we get a sample on first frame\n",
    "    start_time = pytime.time()\n",
    "    current_fps = 0\n",
    "    \n",
    "    # Reset tracking variables\n",
    "    last_detections = {}\n",
    "    track_history = {}\n",
    "    id_counter = 0\n",
    "    class_history = {}\n",
    "    \n",
    "    # Size configuration\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    target_height = int(FRAME_WIDTH * original_height / original_width)\n",
    "    \n",
    "    print(f\"Starting gap extraction from {video_path}...\")\n",
    "    print(f\"Will sample approximately every {sample_interval_seconds} seconds\")\n",
    "    \n",
    "    # Process video frames\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        current_frame += 1\n",
    "        \n",
    "        # Calculate video timestamp in seconds\n",
    "        timestamp = current_frame / fps\n",
    "        \n",
    "        # Determine if this is a sample frame\n",
    "        is_sample_frame = (current_frame - last_sample_frame >= frame_interval)\n",
    "        \n",
    "        # Skip processing if not showing video and not a sample frame\n",
    "        if not show_video and not is_sample_frame:\n",
    "            if current_frame % 100 == 0:  # Show progress occasionally\n",
    "                print(f\"Progress: Frame {current_frame}/{total_frames} ({current_frame/total_frames*100:.1f}%)\")\n",
    "            continue\n",
    "        \n",
    "        # Mark this as a sample frame if needed\n",
    "        if is_sample_frame:\n",
    "            last_sample_frame = current_frame\n",
    "            print(f\"Taking sample at frame {current_frame} (timestamp: {timestamp:.2f}s)\")\n",
    "        \n",
    "        # Resize frame for processing\n",
    "        frame_resized = cv2.resize(frame, (FRAME_WIDTH, target_height))\n",
    "        original_frame = frame_resized.copy()\n",
    "        \n",
    "        # Run YOLOv8 detection\n",
    "        results = model.predict(\n",
    "            source=frame_resized, \n",
    "            conf=GAP_DETECTION_THRESHOLD,\n",
    "            iou=0.45,\n",
    "            max_det=20,\n",
    "            verbose=False\n",
    "        )[0]\n",
    "        \n",
    "        # Current detections dictionary\n",
    "        current_detections = {}\n",
    "        \n",
    "        # Process detection results\n",
    "        if results.boxes and len(results.boxes) > 0:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            confs = results.boxes.conf.cpu().numpy()\n",
    "            class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            # Create detection list\n",
    "            detections = []\n",
    "            \n",
    "            # Process each detected object\n",
    "            for i, box in enumerate(boxes):\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                conf = float(confs[i])\n",
    "                class_id = int(class_ids[i])\n",
    "                cls_name = model.names[class_id]\n",
    "                \n",
    "                # Check if confidence meets threshold\n",
    "                classified = conf >= class_thresholds.get(cls_name, 0.40)\n",
    "                \n",
    "                # Calculate center point\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                \n",
    "                # Try to match with existing objects for tracking\n",
    "                object_id = None\n",
    "                for old_id, old_info in last_detections.items():\n",
    "                    old_cx, old_cy = old_info['center']\n",
    "                    old_cls = old_info['class']\n",
    "                    \n",
    "                    # Calculate distance between centers\n",
    "                    dist = np.sqrt((center_x - old_cx)**2 + (center_y - old_cy)**2)\n",
    "                    \n",
    "                    # If close enough, it's likely the same car\n",
    "                    if dist < 100:\n",
    "                        object_id = old_id\n",
    "                        \n",
    "                        # Use previous classification if it was better\n",
    "                        if old_info['classified'] and not classified:\n",
    "                            cls_name = old_cls\n",
    "                            classified = True\n",
    "                        \n",
    "                        # Special handling for problematic classes\n",
    "                        if classified and old_cls != cls_name and (cls_name == 'Williams' or cls_name == 'Alpine'):\n",
    "                            if old_info['classified']:\n",
    "                                cls_name = old_cls\n",
    "                        break\n",
    "                \n",
    "                # If no match found, create new ID\n",
    "                if object_id is None:\n",
    "                    object_id = id_counter\n",
    "                    id_counter += 1\n",
    "                    track_history[object_id] = []\n",
    "                    class_history[object_id] = []\n",
    "                \n",
    "                # Update classification history for stability\n",
    "                if object_id in class_history:\n",
    "                    if classified:\n",
    "                        class_history[object_id].append(cls_name)\n",
    "                        # Keep history limited\n",
    "                        if len(class_history[object_id]) > 5:\n",
    "                            class_history[object_id].pop(0)\n",
    "                    \n",
    "                    # Use most common class from history\n",
    "                    if len(class_history[object_id]) >= 3:\n",
    "                        counts = Counter(class_history[object_id])\n",
    "                        if counts:\n",
    "                            most_common = counts.most_common(1)[0][0]\n",
    "                            cls_name = most_common\n",
    "                            classified = True\n",
    "                \n",
    "                # Save current detection\n",
    "                current_detections[object_id] = {\n",
    "                    'box': (x1, y1, x2, y2),\n",
    "                    'conf': conf,\n",
    "                    'class': cls_name,\n",
    "                    'classified': classified,\n",
    "                    'center': (center_x, center_y),\n",
    "                    'area': area,\n",
    "                    'y_bottom': y2\n",
    "                }\n",
    "                \n",
    "                # Add to detection list\n",
    "                detections.append({\n",
    "                    'id': object_id,\n",
    "                    'box': (x1, y1, x2, y2),\n",
    "                    'class': cls_name,\n",
    "                    'classified': classified,\n",
    "                    'conf': conf,\n",
    "                    'y_bottom': y2\n",
    "                })\n",
    "            \n",
    "            # Sort by vertical position (cars more below first - closer to camera)\n",
    "            detections = sorted(detections, key=lambda x: x['y_bottom'], reverse=True)\n",
    "            \n",
    "            # Calculate gaps between consecutive cars\n",
    "            frame_gaps = []\n",
    "            \n",
    "            # Process detections for visualization\n",
    "            if show_video:\n",
    "                for i, det in enumerate(detections):\n",
    "                    x1, y1, x2, y2 = det['box']\n",
    "                    cls_name = det['class']\n",
    "                    conf = det['conf']\n",
    "                    classified = det['classified']\n",
    "                    \n",
    "                    # Get specific color for the team\n",
    "                    if classified:\n",
    "                        color = class_colors.get(cls_name, (0, 255, 0))\n",
    "                    else:\n",
    "                        color = class_colors['unknown']  # Yellow for cars without secure classification\n",
    "                    \n",
    "                    # Draw box with team color\n",
    "                    cv2.rectangle(frame_resized, (x1, y1), (x2, y2), color, 2)\n",
    "                    \n",
    "                    # Label with class and confidence\n",
    "                    if classified:\n",
    "                        label = f\"{cls_name}: {conf:.2f}\"\n",
    "                    else:\n",
    "                        label = f\"F1 Car: {conf:.2f}\"\n",
    "                    \n",
    "                    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "                    cv2.rectangle(frame_resized, (x1, y1-t_size[1]-3), (x1+t_size[0], y1), color, -1)\n",
    "                    cv2.putText(frame_resized, label, (x1, y1-3), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                    \n",
    "                    # Only if there's a next car\n",
    "                    if i < len(detections)-1:\n",
    "                        next_det = detections[i+1]\n",
    "                        gap_info = calculate_gap(\n",
    "                            det['box'], next_det['box'], \n",
    "                            det['class'] if det['classified'] else \"F1 Car\", \n",
    "                            next_det['class'] if next_det['classified'] else \"F1 Car\"\n",
    "                        )\n",
    "                        \n",
    "                        # Save gap info for data collection if this is a sample frame\n",
    "                        if is_sample_frame:\n",
    "                            frame_gaps.append({\n",
    "                                'frame': current_frame,\n",
    "                                'timestamp': timestamp,\n",
    "                                'car1_id': det['id'],\n",
    "                                'car2_id': next_det['id'],\n",
    "                                'car1_team': gap_info['car1'],\n",
    "                                'car2_team': gap_info['car2'],\n",
    "                                'distance_meters': gap_info['distance'],\n",
    "                                'gap_seconds': gap_info['time']\n",
    "                            })\n",
    "                        \n",
    "                        # Connection points\n",
    "                        cx1, cy1 = int((x1+x2)/2), int(y1)  # Use top of the car\n",
    "                        nx1, ny1, nx2, ny2 = next_det['box']\n",
    "                        cx2, cy2 = int((nx1+nx2)/2), int(ny2)  # Use bottom of the next car\n",
    "                        \n",
    "                        # Diagonal line between cars\n",
    "                        line_color = (0, 0, 255) if is_sample_frame else (0, 255, 0)  # Red if sampled, otherwise green\n",
    "                        line_thickness = 3 if is_sample_frame else 2  # Thicker if sampled\n",
    "                        cv2.line(frame_resized, (cx1, cy1), (cx2, cy2), line_color, line_thickness)\n",
    "                        \n",
    "                        # Text at midpoint with more information\n",
    "                        mid_x, mid_y = (cx1+cx2)//2, (cy1+cy2)//2\n",
    "                        \n",
    "                        # Distance and gap time\n",
    "                        dist_text = f\"{gap_info['distance']:.1f}m\"\n",
    "                        time_text = f\"{gap_info['time']:.2f}s\"\n",
    "                        \n",
    "                        # Background for text\n",
    "                        dist_size = cv2.getTextSize(dist_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "                        time_size = cv2.getTextSize(time_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "                        \n",
    "                        # Draw semi-transparent background\n",
    "                        overlay = frame_resized.copy()\n",
    "                        bg_color = (0, 0, 180) if is_sample_frame else (0, 0, 0)  # Dark red if sampled\n",
    "                        cv2.rectangle(overlay, \n",
    "                                    (mid_x - 5, mid_y - 50), \n",
    "                                    (mid_x + max(dist_size[0], time_size[0]) + 10, mid_y + 10),\n",
    "                                    bg_color, -1)\n",
    "                        cv2.addWeighted(overlay, 0.6, frame_resized, 0.4, 0, frame_resized)\n",
    "                        \n",
    "                        # Draw texts - add \"SAVED\" mark if sampled\n",
    "                        if is_sample_frame:\n",
    "                            saved_text = \"SAVED\"\n",
    "                            cv2.putText(frame_resized, saved_text, (mid_x, mid_y - 50),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                        \n",
    "                        # Draw gap measurements\n",
    "                        cv2.putText(frame_resized, dist_text, (mid_x, mid_y - 25),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                        cv2.putText(frame_resized, time_text, (mid_x, mid_y),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # If processing a sample frame, also collect gap data without visualization\n",
    "            elif is_sample_frame:\n",
    "                for i, det in enumerate(detections):\n",
    "                    if i < len(detections)-1:\n",
    "                        next_det = detections[i+1]\n",
    "                        gap_info = calculate_gap(\n",
    "                            det['box'], next_det['box'], \n",
    "                            det['class'] if det['classified'] else \"F1 Car\", \n",
    "                            next_det['class'] if next_det['classified'] else \"F1 Car\"\n",
    "                        )\n",
    "                        \n",
    "                        frame_gaps.append({\n",
    "                            'frame': current_frame,\n",
    "                            'timestamp': round(timestamp, 2),\n",
    "                            'car1_id': det['id'],\n",
    "                            'car2_id': next_det['id'],\n",
    "                            'car1_team': gap_info['car1'],\n",
    "                            'car2_team': gap_info['car2'],\n",
    "                            'distance_meters': round(gap_info['distance'], 2),\n",
    "                            'gap_seconds': round(gap_info['time'], 2)\n",
    "                        })\n",
    "            \n",
    "            # Add all gaps from this frame to our collection if it's a sample frame\n",
    "            if is_sample_frame:\n",
    "                all_gaps.extend(frame_gaps)\n",
    "                print(f\"Found {len(frame_gaps)} car gaps at timestamp {timestamp:.2f}s\")\n",
    "        \n",
    "        # Update last_detections for next iteration\n",
    "        last_detections = current_detections\n",
    "        \n",
    "        # Calculate FPS for display\n",
    "        if show_video and frame_count % 10 == 0:\n",
    "            current_time = pytime.time()\n",
    "            current_fps = 10.0 / (current_time - start_time)\n",
    "            start_time = current_time\n",
    "        \n",
    "        # Show visual elements if requested\n",
    "        if show_video:\n",
    "            # Show FPS and model information\n",
    "            cv2.putText(frame_resized, f\"FPS: {current_fps:.1f}\", (20, 40),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Add \"SAMPLE FRAME\" indicator if this is a frame being saved\n",
    "            if is_sample_frame:\n",
    "                cv2.putText(frame_resized, \" SAMPLE FRAME\", (FRAME_WIDTH - 300, 40),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(frame_resized, \"F1 Gap Extraction\", (FRAME_WIDTH - 300, 40),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show detection threshold\n",
    "            detection_mode = f\"Detection Threshold: {GAP_DETECTION_THRESHOLD:.2f}\"\n",
    "            cv2.putText(frame_resized, detection_mode, (20, target_height - 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show progress\n",
    "            progress_text = f\"Frame: {current_frame}/{total_frames} ({current_frame/total_frames*100:.1f}%)\"\n",
    "            cv2.putText(frame_resized, progress_text, (FRAME_WIDTH - 400, target_height - 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show the frame\n",
    "            cv2.imshow(\"F1 Gap Extraction\", frame_resized)\n",
    "            \n",
    "            # Handle key presses\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('+'):  # Increase threshold\n",
    "                GAP_DETECTION_THRESHOLD = min(GAP_DETECTION_THRESHOLD + 0.05, 0.95)\n",
    "                print(f\"Detection threshold increased to {GAP_DETECTION_THRESHOLD:.2f}\")\n",
    "            elif key == ord('-'):  # Decrease threshold\n",
    "                GAP_DETECTION_THRESHOLD = max(GAP_DETECTION_THRESHOLD - 0.05, 0.05)\n",
    "                print(f\"Detection threshold decreased to {GAP_DETECTION_THRESHOLD:.2f}\")\n",
    "            elif key == ord('d'):  # Skip forward 10 seconds\n",
    "                skip_frames = int(fps * 10)\n",
    "                new_frame_pos = min(current_frame + skip_frames, total_frames - 1)\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, new_frame_pos)\n",
    "                current_frame = new_frame_pos - 1\n",
    "                last_detections = {}\n",
    "                print(f\"Skipped forward 10 seconds to frame {new_frame_pos}\")\n",
    "    \n",
    "    # Release video resource\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if all_gaps:\n",
    "        gaps_df = pd.DataFrame(all_gaps)\n",
    "        \n",
    "        # Save to CSV if requested\n",
    "        if output_csv is None:\n",
    "            # Generate default filename based on video name\n",
    "            video_name = os.path.basename(video_path).split('.')[0]\n",
    "            timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_csv = f\"../f1-strategy/data/gaps/gap_data_{video_name}_{timestamp_str}.csv\"\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "        \n",
    "        # Save data\n",
    "        gaps_df.to_csv(output_csv, index=False, float_format='%.3f')\n",
    "        print(f\"Gap data saved to {output_csv}\")\n",
    "        print(f\"Total of {len(gaps_df)} gap measurements collected\")\n",
    "        \n",
    "        return gaps_df\n",
    "    else:\n",
    "        print(\"No gap data could be collected!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Run gap extraction with visualization\n",
    "def main_extract_gaps():\n",
    "    \"\"\"Extract gaps from the video at 10-second intervals and save to CSV\"\"\"\n",
    "    video_path = \"../f1-strategy/data/videos/best_overtakes_2023.mp4.f399.mp4\"\n",
    "    \n",
    "    # Run extraction with default 10-second interval, showing video\n",
    "    gap_data = extract_gaps_from_video(\n",
    "        video_path,\n",
    "        sample_interval_seconds=10,\n",
    "        show_video=True  # Enable video display\n",
    "    )\n",
    "    \n",
    "    # Display sample of extracted data\n",
    "    if gap_data is not None and not gap_data.empty:\n",
    "        print(\"\\nSample gap data:\")\n",
    "        print(gap_data.head())\n",
    "        \n",
    "        # Show some basic statistics\n",
    "        print(\"\\nGap statistics (seconds):\")\n",
    "        print(gap_data['gap_seconds'].describe())\n",
    "    \n",
    "    return gap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video FPS: 50, sampling every 500 frames (10 seconds)\n",
      "Starting gap extraction from ../f1-strategy/data/videos/best_overtakes_2023.mp4.f399.mp4...\n",
      "Will sample approximately every 10 seconds\n",
      "Taking sample at frame 1 (timestamp: 0.02s)\n",
      "Found 0 car gaps at timestamp 0.02s\n",
      "Skipped forward 10 seconds to frame 532\n",
      "Taking sample at frame 532 (timestamp: 10.64s)\n",
      "Skipped forward 10 seconds to frame 1032\n",
      "Taking sample at frame 1032 (timestamp: 20.64s)\n",
      "Skipped forward 10 seconds to frame 1532\n",
      "Taking sample at frame 1532 (timestamp: 30.64s)\n",
      "Found 1 car gaps at timestamp 30.64s\n",
      "Skipped forward 10 seconds to frame 2032\n",
      "Taking sample at frame 2032 (timestamp: 40.64s)\n",
      "Found 1 car gaps at timestamp 40.64s\n",
      "Skipped forward 10 seconds to frame 2532\n",
      "Taking sample at frame 2532 (timestamp: 50.64s)\n",
      "Found 0 car gaps at timestamp 50.64s\n",
      "Skipped forward 10 seconds to frame 3032\n",
      "Taking sample at frame 3032 (timestamp: 60.64s)\n",
      "Found 0 car gaps at timestamp 60.64s\n",
      "Skipped forward 10 seconds to frame 3532\n",
      "Taking sample at frame 3532 (timestamp: 70.64s)\n",
      "Found 2 car gaps at timestamp 70.64s\n",
      "Skipped forward 10 seconds to frame 4032\n",
      "Taking sample at frame 4032 (timestamp: 80.64s)\n",
      "Found 0 car gaps at timestamp 80.64s\n",
      "Skipped forward 10 seconds to frame 4532\n",
      "Taking sample at frame 4532 (timestamp: 90.64s)\n",
      "Found 1 car gaps at timestamp 90.64s\n",
      "Skipped forward 10 seconds to frame 5032\n",
      "Taking sample at frame 5032 (timestamp: 100.64s)\n",
      "Found 0 car gaps at timestamp 100.64s\n",
      "Skipped forward 10 seconds to frame 5532\n",
      "Taking sample at frame 5532 (timestamp: 110.64s)\n",
      "Skipped forward 10 seconds to frame 6032\n",
      "Taking sample at frame 6032 (timestamp: 120.64s)\n",
      "Found 2 car gaps at timestamp 120.64s\n",
      "Skipped forward 10 seconds to frame 6532\n",
      "Taking sample at frame 6532 (timestamp: 130.64s)\n",
      "Skipped forward 10 seconds to frame 7032\n",
      "Taking sample at frame 7032 (timestamp: 140.64s)\n",
      "Found 0 car gaps at timestamp 140.64s\n",
      "Skipped forward 10 seconds to frame 7532\n",
      "Taking sample at frame 7532 (timestamp: 150.64s)\n",
      "Found 1 car gaps at timestamp 150.64s\n",
      "Skipped forward 10 seconds to frame 8032\n",
      "Taking sample at frame 8032 (timestamp: 160.64s)\n",
      "Found 0 car gaps at timestamp 160.64s\n",
      "Skipped forward 10 seconds to frame 8532\n",
      "Taking sample at frame 8532 (timestamp: 170.64s)\n",
      "Found 0 car gaps at timestamp 170.64s\n",
      "Skipped forward 10 seconds to frame 9032\n",
      "Taking sample at frame 9032 (timestamp: 180.64s)\n",
      "Found 1 car gaps at timestamp 180.64s\n",
      "Skipped forward 10 seconds to frame 9532\n",
      "Taking sample at frame 9532 (timestamp: 190.64s)\n",
      "Found 0 car gaps at timestamp 190.64s\n",
      "Skipped forward 10 seconds to frame 10032\n",
      "Taking sample at frame 10032 (timestamp: 200.64s)\n",
      "Found 1 car gaps at timestamp 200.64s\n",
      "Skipped forward 10 seconds to frame 10532\n",
      "Taking sample at frame 10532 (timestamp: 210.64s)\n",
      "Found 0 car gaps at timestamp 210.64s\n",
      "Taking sample at frame 11032 (timestamp: 220.64s)\n",
      "Found 0 car gaps at timestamp 220.64s\n",
      "Taking sample at frame 11532 (timestamp: 230.64s)\n",
      "Found 0 car gaps at timestamp 230.64s\n",
      "Taking sample at frame 12032 (timestamp: 240.64s)\n",
      "Found 2 car gaps at timestamp 240.64s\n",
      "Skipped forward 10 seconds to frame 12643\n",
      "Taking sample at frame 12643 (timestamp: 252.86s)\n",
      "Found 0 car gaps at timestamp 252.86s\n",
      "Skipped forward 10 seconds to frame 13149\n",
      "Taking sample at frame 13149 (timestamp: 262.98s)\n",
      "Found 0 car gaps at timestamp 262.98s\n",
      "Skipped forward 10 seconds to frame 13655\n",
      "Taking sample at frame 13655 (timestamp: 273.10s)\n",
      "Gap data saved to ../f1-strategy/data/gaps/gap_data_best_overtakes_2023_20250410_121758.csv\n",
      "Total of 12 gap measurements collected\n",
      "\n",
      "Sample gap data:\n",
      "   frame  timestamp  car1_id  car2_id     car1_team car2_team  \\\n",
      "0   1532      30.64       10       11      Mercedes    F1 Car   \n",
      "1   2032      40.64       12       13  Aston Martin    F1 Car   \n",
      "2   3532      70.64       16       18      Williams    F1 Car   \n",
      "3   3532      70.64       18       17        F1 Car      Haas   \n",
      "4   4532      90.64       20       21        F1 Car   Mclaren   \n",
      "\n",
      "   distance_meters  gap_seconds  \n",
      "0         2.014841     0.024179  \n",
      "1         3.432174     0.041188  \n",
      "2         4.366518     0.052400  \n",
      "3        13.021583     0.156265  \n",
      "4         7.130490     0.085569  \n",
      "\n",
      "Gap statistics (seconds):\n",
      "count    12.000000\n",
      "mean      0.070155\n",
      "std       0.036108\n",
      "min       0.024179\n",
      "25%       0.042145\n",
      "50%       0.068407\n",
      "75%       0.087132\n",
      "max       0.156265\n",
      "Name: gap_seconds, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Execute the extraction\n",
    "if __name__ == \"__main__\":\n",
    "    gap_data = main_extract_gaps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclussions\n",
    "\n",
    "In this notebook, we've successfully implemented a computer vision system that:\n",
    "\n",
    "1. Detects F1 cars in video footage using our YOLOv8 model\n",
    "2. Calculates gaps between consecutive cars in both meters and seconds\n",
    "3. Extracts data at regular intervals (every 10 seconds) to feed into our strategy system\n",
    "4. Saves this information to structured CSV files with team identification\n",
    "\n",
    "\n",
    "### 3.1 CSV Structure\n",
    "\n",
    "Our gap extraction system produces CSV files with the following columns:\n",
    "\n",
    "- ``frame``: Frame number in the video\n",
    "- ``timestamp``: Time in seconds from the start of the video\n",
    "- ``car1_id and car2_id``: Unique identifiers for each detected car\n",
    "- ``car1_team and car2_team``: Team identifications based on our model\n",
    "- ``distance_meters``: Physical distance between cars in meters\n",
    "- ``gap_seconds``: Time gap between cars (in seconds at 300 km/h)\n",
    "\n",
    "This data provides the foundation for our gap-based strategy rules, enabling our expert system to make informed decisions about:\n",
    "\n",
    "- Undercut/overcut opportunities\n",
    "- Traffic management\n",
    "- Defensive positioning\n",
    "\n",
    "\n",
    "### 3.2 Next Steps\n",
    "With our gap data now available, the next phase is to integrate this information into our expert system by:\n",
    "\n",
    "- Creating gap-specific rules that detect strategic opportunities\n",
    "- Combining gap data with our existing tire degradation and lap time models\n",
    "- Implementing undercut/overcut detection logic based on real-world F1 strategy principles\n",
    "\n",
    "In the next notebook (``N05_gap_strategy_rules.ipynb``), we'll develop these rules and integrate all components of our F1 strategy system.\n",
    "\n",
    "However, due to that this data can be not enough or too much erratic, we will do the gap rules with fastf1 data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
