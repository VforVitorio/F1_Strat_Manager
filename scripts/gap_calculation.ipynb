{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision: Gap Calculation using YOLO net and OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from collections import Counter\n",
    "import time as pytime\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): DetectionModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-3): 4 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-3): 4 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(384, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (11): Concat()\n",
       "      (12): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(960, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (14): Concat()\n",
       "      (15): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv(\n",
       "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (17): Concat()\n",
       "      (18): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): Conv(\n",
       "        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (20): Concat()\n",
       "      (21): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): Detect(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the Anti-Alpine optimized model\n",
    "model = YOLO(\"../f1-strategy/weights/model_anti_alpine.pt\")\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gap Calculation and Yolo video processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dedining constants and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size and scale configuration\n",
    "FRAME_WIDTH = 1280  # Higher resolution for better detail\n",
    "CAR_LENGTH_METERS = 5.63  # Real car length in meters\n",
    "\n",
    "# FOR GAPS: use a lower global threshold to maximize detections\n",
    "GAP_DETECTION_THRESHOLD = 0.25  # Low threshold to detect all possible cars\n",
    "\n",
    "# Specific colors for each F1 team (BGR format for OpenCV)\n",
    "class_colors = {\n",
    "    'Ferrari': (0, 0, 255),         # Red (BGR)\n",
    "    'Mercedes': (200, 200, 200),    # Silver (BGR)\n",
    "    'Red Bull': (139, 0, 0),        # Dark Blue (BGR)\n",
    "    'McLaren': (0, 165, 255),       # Orange (BGR)\n",
    "    'Aston Martin': (0, 128, 0),    # Green (BGR)\n",
    "    'Alpine': (128, 0, 0),          # Blue (BGR)\n",
    "    'Williams': (205, 0, 0),        # Light Blue (BGR)\n",
    "    'Haas': (255, 255, 255),        # White (BGR)\n",
    "    'Kick Sauber': (255, 255, 0),   # Cyan (BGR)\n",
    "    'Racing Bulls': (0, 0, 255),    # Red (BGR)\n",
    "    'background': (128, 128, 128),  # Gray (BGR)\n",
    "    'unknown': (0, 255, 255)        # Yellow for cars without secure classification\n",
    "}\n",
    "\n",
    "# Thresholds to show classification (not for filtering detections)\n",
    "class_thresholds = {\n",
    "    'Williams': 0.90,     # Very high for Williams\n",
    "    'Alpine': 0.90,       # Extremely high for Alpine\n",
    "    'McLaren': 0.30,      # Low for McLaren\n",
    "    'Red Bull': 0.85,     # High for Red Bull\n",
    "    'Ferrari': 0.40,      # Normal\n",
    "    'Mercedes': 0.50,     # Medium-high\n",
    "    'Haas': 0.40,         # Normal\n",
    "    'Kick Sauber': 0.40,  # Normal\n",
    "    'Racing Bulls': 0.40, # Normal\n",
    "    'Aston Martin': 0.40, # Normal\n",
    "    'background': 0.50    # High for background\n",
    "}\n",
    "\n",
    "# Detection history for stabilization\n",
    "last_detections = {}\n",
    "track_history = {}\n",
    "id_counter = 0\n",
    "class_history = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Calculating the gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gap(box1, box2, class1, class2):\n",
    "    \"\"\"Calculates the distance between centers using car width for scale\"\"\"\n",
    "    # Box centers\n",
    "    cx1, cy1 = (box1[0] + box1[2])/2, (box1[1] + box1[3])/2\n",
    "    cx2, cy2 = (box2[0] + box2[2])/2, (box2[1] + box2[3])/2\n",
    "    \n",
    "    # Distance in pixels\n",
    "    pixel_distance = np.hypot(cx2 - cx1, cy2 - cy1)\n",
    "    \n",
    "    # Scale based on average width of detected cars\n",
    "    avg_width = ((box1[2] - box1[0]) + (box2[2] - box2[0])) / 2\n",
    "    scale = CAR_LENGTH_METERS / avg_width if avg_width != 0 else 0\n",
    "    \n",
    "    # Calculate gap time at 300km/h (83.33 m/s)\n",
    "    speed_mps = 83.33  # Meters per second at 300km/h\n",
    "    gap_time = (pixel_distance * scale) / speed_mps\n",
    "    \n",
    "    return {\n",
    "        'distance': pixel_distance * scale,  # Distance in meters\n",
    "        'time': gap_time,                   # Time in seconds at 300km/h\n",
    "        'car1': class1,                     # Team of first car\n",
    "        'car2': class2                      # Team of second car\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Processing the video with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_with_yolo(video_path, output_path=None):\n",
    "    global last_detections, track_history, id_counter, class_history, GAP_DETECTION_THRESHOLD\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get original video dimensions\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    current_frame = 0\n",
    "    \n",
    "    # Calculate new height maintaining aspect ratio\n",
    "    target_height = int(FRAME_WIDTH * original_height / original_width)\n",
    "    \n",
    "    out = None\n",
    "    if output_path:\n",
    "        # Change codec from 'mp4v' to 'XVID' which is more reliable\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (FRAME_WIDTH, target_height))\n",
    "        if not out.isOpened():\n",
    "            print(f\"Error: Could not create output video file at {output_path}\")\n",
    "            print(\"Continuing without saving output...\")\n",
    "            output_path = None\n",
    "    \n",
    "    # Variables for calculating real FPS\n",
    "    frame_count = 0\n",
    "    start_time = pytime.time()\n",
    "    current_fps = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        current_frame += 1\n",
    "        \n",
    "        # Resize maintaining aspect ratio\n",
    "        frame_resized = cv2.resize(frame, (FRAME_WIDTH, target_height))\n",
    "        original_frame = frame_resized.copy()\n",
    "        \n",
    "        # Run YOLOv8 detection with low threshold to maximize detections\n",
    "        results = model.predict(\n",
    "            source=frame_resized, \n",
    "            conf=GAP_DETECTION_THRESHOLD,  # Low threshold to detect all possible cars\n",
    "            iou=0.45,   # Standard IoU\n",
    "            max_det=20, # Maximum detections\n",
    "            verbose=False\n",
    "        )[0]\n",
    "        \n",
    "        # Current detections\n",
    "        current_detections = {}\n",
    "        \n",
    "        # Process detection results\n",
    "        if results.boxes and len(results.boxes) > 0:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            confs = results.boxes.conf.cpu().numpy()\n",
    "            class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            # Create detection list with all information\n",
    "            detections = []\n",
    "            for i, box in enumerate(boxes):\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                conf = float(confs[i])\n",
    "                class_id = int(class_ids[i])\n",
    "                cls_name = model.names[class_id]\n",
    "                \n",
    "                # Determine whether to show team classification based on threshold\n",
    "                # Note: we still detect the car but might not show its team\n",
    "                classified = conf >= class_thresholds.get(cls_name, 0.40)\n",
    "                \n",
    "                # KEY: For gaps, we detect all cars even if we're not sure of the team\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                \n",
    "                # Assign unique ID or retrieve existing ID\n",
    "                object_id = None\n",
    "                for old_id, old_info in last_detections.items():\n",
    "                    old_cx, old_cy = old_info['center']\n",
    "                    old_cls = old_info['class']\n",
    "                    \n",
    "                    # Distance between centers\n",
    "                    dist = np.sqrt((center_x - old_cx)**2 + (center_y - old_cy)**2)\n",
    "                    \n",
    "                    # If it's close, it could be the same object\n",
    "                    if dist < 100:\n",
    "                        object_id = old_id\n",
    "                        \n",
    "                        # If previous class was valid and new one is uncertain, keep the previous one\n",
    "                        if old_info['classified'] and not classified:\n",
    "                            cls_name = old_cls\n",
    "                            classified = True\n",
    "                        \n",
    "                        # Stabilize classification with history for problematic classes\n",
    "                        if classified and old_cls != cls_name and (cls_name == 'Williams' or cls_name == 'Alpine'):\n",
    "                            if old_info['classified']:\n",
    "                                cls_name = old_cls\n",
    "                        break\n",
    "                \n",
    "                # If no match found, assign new ID\n",
    "                if object_id is None:\n",
    "                    object_id = id_counter\n",
    "                    id_counter += 1\n",
    "                    track_history[object_id] = []\n",
    "                    class_history[object_id] = []\n",
    "                \n",
    "                # Update history\n",
    "                if object_id in class_history:\n",
    "                    # Only add to history if we're sure of the class\n",
    "                    if classified:\n",
    "                        class_history[object_id].append(cls_name)\n",
    "                        # Limit history to 5 classes\n",
    "                        if len(class_history[object_id]) > 5:\n",
    "                            class_history[object_id].pop(0)\n",
    "                    \n",
    "                    # Use the most common class from history for stability\n",
    "                    if len(class_history[object_id]) >= 3:\n",
    "                        counts = Counter(class_history[object_id])\n",
    "                        if counts:  # Make sure it's not empty\n",
    "                            most_common = counts.most_common(1)[0][0]\n",
    "                            cls_name = most_common\n",
    "                            classified = True\n",
    "                \n",
    "                # Save current detection\n",
    "                current_detections[object_id] = {\n",
    "                    'box': (x1, y1, x2, y2),\n",
    "                    'conf': conf,\n",
    "                    'class': cls_name,\n",
    "                    'classified': classified,\n",
    "                    'center': (center_x, center_y),\n",
    "                    'area': area,\n",
    "                    'y_bottom': y2  # For sorting by vertical position\n",
    "                }\n",
    "                \n",
    "                # Add to detection list for gap calculation\n",
    "                detections.append({\n",
    "                    'id': object_id,\n",
    "                    'box': (x1, y1, x2, y2),\n",
    "                    'class': cls_name,\n",
    "                    'classified': classified,\n",
    "                    'conf': conf,\n",
    "                    'y_bottom': y2\n",
    "                })\n",
    "            \n",
    "            # Sort by vertical position (cars more below first - closer)\n",
    "            detections = sorted(detections, key=lambda x: x['y_bottom'], reverse=True)\n",
    "            \n",
    "            # Draw boxes and gaps\n",
    "            for i, det in enumerate(detections):\n",
    "                x1, y1, x2, y2 = det['box']\n",
    "                cls_name = det['class']\n",
    "                conf = det['conf']\n",
    "                classified = det['classified']\n",
    "                \n",
    "                # Get specific color for the team\n",
    "                if classified:\n",
    "                    color = class_colors.get(cls_name, (0, 255, 0))\n",
    "                else:\n",
    "                    color = class_colors['unknown']  # Yellow for cars without secure classification\n",
    "                \n",
    "                # Draw box with team color\n",
    "                cv2.rectangle(frame_resized, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # Label with class and confidence\n",
    "                if classified:\n",
    "                    label = f\"{cls_name}: {conf:.2f}\"\n",
    "                else:\n",
    "                    label = f\"F1 Car: {conf:.2f}\"\n",
    "                \n",
    "                t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "                cv2.rectangle(frame_resized, (x1, y1-t_size[1]-3), (x1+t_size[0], y1), color, -1)\n",
    "                cv2.putText(frame_resized, label, (x1, y1-3), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                \n",
    "                # Only if there's a next car\n",
    "                if i < len(detections)-1:\n",
    "                    next_det = detections[i+1]\n",
    "                    gap_info = calculate_gap(\n",
    "                        det['box'], next_det['box'], \n",
    "                        det['class'] if det['classified'] else \"F1 Car\", \n",
    "                        next_det['class'] if next_det['classified'] else \"F1 Car\"\n",
    "                    )\n",
    "                    \n",
    "                    # Connection points\n",
    "                    cx1, cy1 = int((x1+x2)/2), int(y1)  # Use top of the car\n",
    "                    nx1, ny1, nx2, ny2 = next_det['box']\n",
    "                    cx2, cy2 = int((nx1+nx2)/2), int(ny2)  # Use bottom of the next car\n",
    "                    \n",
    "                    # Diagonal line between cars\n",
    "                    cv2.line(frame_resized, (cx1, cy1), (cx2, cy2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Text at midpoint with more information\n",
    "                    mid_x, mid_y = (cx1+cx2)//2, (cy1+cy2)//2\n",
    "                    \n",
    "                    # Distance and gap time\n",
    "                    dist_text = f\"{gap_info['distance']:.1f}m\"\n",
    "                    time_text = f\"{gap_info['time']:.2f}s\"\n",
    "                    \n",
    "                    # Background for text\n",
    "                    dist_size = cv2.getTextSize(dist_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "                    time_size = cv2.getTextSize(time_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "                    \n",
    "                    # Draw semi-transparent background\n",
    "                    overlay = frame_resized.copy()\n",
    "                    cv2.rectangle(overlay, \n",
    "                                 (mid_x - 5, mid_y - 50), \n",
    "                                 (mid_x + max(dist_size[0], time_size[0]) + 10, mid_y + 10),\n",
    "                                 (0, 0, 0), -1)\n",
    "                    cv2.addWeighted(overlay, 0.6, frame_resized, 0.4, 0, frame_resized)\n",
    "                    \n",
    "                    # Draw texts\n",
    "                    cv2.putText(frame_resized, dist_text, (mid_x, mid_y - 25),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                    cv2.putText(frame_resized, time_text, (mid_x, mid_y),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Update last_detections for next iteration\n",
    "        last_detections = current_detections\n",
    "        \n",
    "        # Calculate FPS\n",
    "        if frame_count % 10 == 0:\n",
    "            current_time = pytime.time()\n",
    "            current_fps = 10.0 / (current_time - start_time)\n",
    "            start_time = current_time\n",
    "        \n",
    "        # Show FPS and model information\n",
    "        cv2.putText(frame_resized, f\"FPS: {current_fps:.1f}\", (20, 40),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.putText(frame_resized, \"F1 Gap Detection\", (FRAME_WIDTH - 300, 40),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Show detection mode and progress\n",
    "        detection_mode = f\"Detection Threshold: {GAP_DETECTION_THRESHOLD:.2f}\"\n",
    "        cv2.putText(frame_resized, detection_mode, (20, target_height - 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Show video progress\n",
    "        progress_text = f\"Frame: {current_frame}/{total_frames} ({current_frame/total_frames*100:.1f}%)\"\n",
    "        cv2.putText(frame_resized, progress_text, (FRAME_WIDTH - 400, target_height - 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Save processed frame if requested\n",
    "        if output_path:\n",
    "            out.write(frame_resized)\n",
    "        \n",
    "        # Show frame\n",
    "        cv2.imshow(\"F1 Gap Detection\", frame_resized)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('+'):  # Increase threshold\n",
    "            GAP_DETECTION_THRESHOLD = min(GAP_DETECTION_THRESHOLD + 0.05, 0.95)\n",
    "            print(f\"Detection threshold increased to {GAP_DETECTION_THRESHOLD:.2f}\")\n",
    "        elif key == ord('-'):  # Decrease threshold\n",
    "            GAP_DETECTION_THRESHOLD = max(GAP_DETECTION_THRESHOLD - 0.05, 0.05)\n",
    "            print(f\"Detection threshold decreased to {GAP_DETECTION_THRESHOLD:.2f}\")\n",
    "        elif key == ord('d'):  # Skip forward 10 seconds\n",
    "            skip_frames = int(fps * 10)  # 10 seconds * fps = number of frames to skip\n",
    "            new_frame_pos = min(current_frame + skip_frames, total_frames - 1)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, new_frame_pos)\n",
    "            current_frame = new_frame_pos - 1  # Will be incremented in the next cycle\n",
    "            # Temporarily reset tracking\n",
    "            last_detections = {}\n",
    "            print(f\"Skipped forward 10 seconds to frame {new_frame_pos}\")\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Running a demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run with your video\n",
    "def main():\n",
    "    # video_path = \"../f1-strategy/data/videos/best_overtakes_2023.mp4.f399.mp4\"\n",
    "    # video_path = \"../f1-strategy/data/videos/spain_2023_race.mp4.f399.mp4\"\n",
    "    \n",
    "    video_path = \"../f1-strategy/data/videos/belgium_gp.f399.mp4\"\n",
    "    output_path = \"../f1-strategy/data/videos/gap_detection_output.mp4\"\n",
    "    process_video_with_yolo(video_path, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controls\n",
    "\n",
    "'q': out\n",
    "'+': more detection threshold\n",
    "'-': less detection threshold\n",
    "'d': 10 seconds ahead "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gap Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Gap extraction function with visualization\n",
    "def extract_gaps_from_video(video_path, sample_interval_seconds=10, output_csv=None, show_video=True):\n",
    "    \"\"\"\n",
    "    Process a video and extract gap data at regular intervals with visualization\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to the F1 video\n",
    "        sample_interval_seconds: How often to sample gap data (in seconds)\n",
    "        output_csv: Path to save CSV data (if None, will generate a default path)\n",
    "        show_video: Whether to display the video during processing\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with extracted gap data\n",
    "    \"\"\"\n",
    "    global last_detections, track_history, id_counter, class_history, GAP_DETECTION_THRESHOLD\n",
    "    \n",
    "    # Initialize empty list to store all gap data\n",
    "    all_gaps = []\n",
    "    \n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Calculate frame interval based on desired time interval\n",
    "    frame_interval = int(fps * sample_interval_seconds)\n",
    "    print(f\"Video FPS: {fps}, sampling every {frame_interval} frames ({sample_interval_seconds} seconds)\")\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    frame_count = 0\n",
    "    current_frame = 0\n",
    "    last_sample_frame = -frame_interval  # Ensure we get a sample on first frame\n",
    "    start_time = pytime.time()\n",
    "    current_fps = 0\n",
    "    \n",
    "    # Reset tracking variables\n",
    "    last_detections = {}\n",
    "    track_history = {}\n",
    "    id_counter = 0\n",
    "    class_history = {}\n",
    "    \n",
    "    # Size configuration\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    target_height = int(FRAME_WIDTH * original_height / original_width)\n",
    "    \n",
    "    print(f\"Starting gap extraction from {video_path}...\")\n",
    "    print(f\"Will sample approximately every {sample_interval_seconds} seconds\")\n",
    "    \n",
    "    # Process video frames\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        current_frame += 1\n",
    "        \n",
    "        # Calculate video timestamp in seconds\n",
    "        timestamp = current_frame / fps\n",
    "        \n",
    "        # Determine if this is a sample frame\n",
    "        is_sample_frame = (current_frame - last_sample_frame >= frame_interval)\n",
    "        \n",
    "        # Skip processing if not showing video and not a sample frame\n",
    "        if not show_video and not is_sample_frame:\n",
    "            if current_frame % 100 == 0:  # Show progress occasionally\n",
    "                print(f\"Progress: Frame {current_frame}/{total_frames} ({current_frame/total_frames*100:.1f}%)\")\n",
    "            continue\n",
    "        \n",
    "        # Mark this as a sample frame if needed\n",
    "        if is_sample_frame:\n",
    "            last_sample_frame = current_frame\n",
    "            print(f\"Taking sample at frame {current_frame} (timestamp: {timestamp:.2f}s)\")\n",
    "        \n",
    "        # Resize frame for processing\n",
    "        frame_resized = cv2.resize(frame, (FRAME_WIDTH, target_height))\n",
    "        original_frame = frame_resized.copy()\n",
    "        \n",
    "        # Run YOLOv8 detection\n",
    "        results = model.predict(\n",
    "            source=frame_resized, \n",
    "            conf=GAP_DETECTION_THRESHOLD,\n",
    "            iou=0.45,\n",
    "            max_det=20,\n",
    "            verbose=False\n",
    "        )[0]\n",
    "        \n",
    "        # Current detections dictionary\n",
    "        current_detections = {}\n",
    "        \n",
    "        # Process detection results\n",
    "        if results.boxes and len(results.boxes) > 0:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            confs = results.boxes.conf.cpu().numpy()\n",
    "            class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            # Create detection list\n",
    "            detections = []\n",
    "            \n",
    "            # Process each detected object\n",
    "            for i, box in enumerate(boxes):\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                conf = float(confs[i])\n",
    "                class_id = int(class_ids[i])\n",
    "                cls_name = model.names[class_id]\n",
    "                \n",
    "                # Check if confidence meets threshold\n",
    "                classified = conf >= class_thresholds.get(cls_name, 0.40)\n",
    "                \n",
    "                # Calculate center point\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                \n",
    "                # Try to match with existing objects for tracking\n",
    "                object_id = None\n",
    "                for old_id, old_info in last_detections.items():\n",
    "                    old_cx, old_cy = old_info['center']\n",
    "                    old_cls = old_info['class']\n",
    "                    \n",
    "                    # Calculate distance between centers\n",
    "                    dist = np.sqrt((center_x - old_cx)**2 + (center_y - old_cy)**2)\n",
    "                    \n",
    "                    # If close enough, it's likely the same car\n",
    "                    if dist < 100:\n",
    "                        object_id = old_id\n",
    "                        \n",
    "                        # Use previous classification if it was better\n",
    "                        if old_info['classified'] and not classified:\n",
    "                            cls_name = old_cls\n",
    "                            classified = True\n",
    "                        \n",
    "                        # Special handling for problematic classes\n",
    "                        if classified and old_cls != cls_name and (cls_name == 'Williams' or cls_name == 'Alpine'):\n",
    "                            if old_info['classified']:\n",
    "                                cls_name = old_cls\n",
    "                        break\n",
    "                \n",
    "                # If no match found, create new ID\n",
    "                if object_id is None:\n",
    "                    object_id = id_counter\n",
    "                    id_counter += 1\n",
    "                    track_history[object_id] = []\n",
    "                    class_history[object_id] = []\n",
    "                \n",
    "                # Update classification history for stability\n",
    "                if object_id in class_history:\n",
    "                    if classified:\n",
    "                        class_history[object_id].append(cls_name)\n",
    "                        # Keep history limited\n",
    "                        if len(class_history[object_id]) > 5:\n",
    "                            class_history[object_id].pop(0)\n",
    "                    \n",
    "                    # Use most common class from history\n",
    "                    if len(class_history[object_id]) >= 3:\n",
    "                        counts = Counter(class_history[object_id])\n",
    "                        if counts:\n",
    "                            most_common = counts.most_common(1)[0][0]\n",
    "                            cls_name = most_common\n",
    "                            classified = True\n",
    "                \n",
    "                # Save current detection\n",
    "                current_detections[object_id] = {\n",
    "                    'box': (x1, y1, x2, y2),\n",
    "                    'conf': conf,\n",
    "                    'class': cls_name,\n",
    "                    'classified': classified,\n",
    "                    'center': (center_x, center_y),\n",
    "                    'area': area,\n",
    "                    'y_bottom': y2\n",
    "                }\n",
    "                \n",
    "                # Add to detection list\n",
    "                detections.append({\n",
    "                    'id': object_id,\n",
    "                    'box': (x1, y1, x2, y2),\n",
    "                    'class': cls_name,\n",
    "                    'classified': classified,\n",
    "                    'conf': conf,\n",
    "                    'y_bottom': y2\n",
    "                })\n",
    "            \n",
    "            # Sort by vertical position (cars more below first - closer to camera)\n",
    "            detections = sorted(detections, key=lambda x: x['y_bottom'], reverse=True)\n",
    "            \n",
    "            # Calculate gaps between consecutive cars\n",
    "            frame_gaps = []\n",
    "            \n",
    "            # Process detections for visualization\n",
    "            if show_video:\n",
    "                for i, det in enumerate(detections):\n",
    "                    x1, y1, x2, y2 = det['box']\n",
    "                    cls_name = det['class']\n",
    "                    conf = det['conf']\n",
    "                    classified = det['classified']\n",
    "                    \n",
    "                    # Get specific color for the team\n",
    "                    if classified:\n",
    "                        color = class_colors.get(cls_name, (0, 255, 0))\n",
    "                    else:\n",
    "                        color = class_colors['unknown']  # Yellow for cars without secure classification\n",
    "                    \n",
    "                    # Draw box with team color\n",
    "                    cv2.rectangle(frame_resized, (x1, y1), (x2, y2), color, 2)\n",
    "                    \n",
    "                    # Label with class and confidence\n",
    "                    if classified:\n",
    "                        label = f\"{cls_name}: {conf:.2f}\"\n",
    "                    else:\n",
    "                        label = f\"F1 Car: {conf:.2f}\"\n",
    "                    \n",
    "                    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "                    cv2.rectangle(frame_resized, (x1, y1-t_size[1]-3), (x1+t_size[0], y1), color, -1)\n",
    "                    cv2.putText(frame_resized, label, (x1, y1-3), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                    \n",
    "                    # Only if there's a next car\n",
    "                    if i < len(detections)-1:\n",
    "                        next_det = detections[i+1]\n",
    "                        gap_info = calculate_gap(\n",
    "                            det['box'], next_det['box'], \n",
    "                            det['class'] if det['classified'] else \"F1 Car\", \n",
    "                            next_det['class'] if next_det['classified'] else \"F1 Car\"\n",
    "                        )\n",
    "                        \n",
    "                        # Save gap info for data collection if this is a sample frame\n",
    "                        if is_sample_frame:\n",
    "                            frame_gaps.append({\n",
    "                                'frame': current_frame,\n",
    "                                'timestamp': timestamp,\n",
    "                                'car1_id': det['id'],\n",
    "                                'car2_id': next_det['id'],\n",
    "                                'car1_team': gap_info['car1'],\n",
    "                                'car2_team': gap_info['car2'],\n",
    "                                'distance_meters': gap_info['distance'],\n",
    "                                'gap_seconds': gap_info['time']\n",
    "                            })\n",
    "                        \n",
    "                        # Connection points\n",
    "                        cx1, cy1 = int((x1+x2)/2), int(y1)  # Use top of the car\n",
    "                        nx1, ny1, nx2, ny2 = next_det['box']\n",
    "                        cx2, cy2 = int((nx1+nx2)/2), int(ny2)  # Use bottom of the next car\n",
    "                        \n",
    "                        # Diagonal line between cars\n",
    "                        line_color = (0, 0, 255) if is_sample_frame else (0, 255, 0)  # Red if sampled, otherwise green\n",
    "                        line_thickness = 3 if is_sample_frame else 2  # Thicker if sampled\n",
    "                        cv2.line(frame_resized, (cx1, cy1), (cx2, cy2), line_color, line_thickness)\n",
    "                        \n",
    "                        # Text at midpoint with more information\n",
    "                        mid_x, mid_y = (cx1+cx2)//2, (cy1+cy2)//2\n",
    "                        \n",
    "                        # Distance and gap time\n",
    "                        dist_text = f\"{gap_info['distance']:.1f}m\"\n",
    "                        time_text = f\"{gap_info['time']:.2f}s\"\n",
    "                        \n",
    "                        # Background for text\n",
    "                        dist_size = cv2.getTextSize(dist_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "                        time_size = cv2.getTextSize(time_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "                        \n",
    "                        # Draw semi-transparent background\n",
    "                        overlay = frame_resized.copy()\n",
    "                        bg_color = (0, 0, 180) if is_sample_frame else (0, 0, 0)  # Dark red if sampled\n",
    "                        cv2.rectangle(overlay, \n",
    "                                    (mid_x - 5, mid_y - 50), \n",
    "                                    (mid_x + max(dist_size[0], time_size[0]) + 10, mid_y + 10),\n",
    "                                    bg_color, -1)\n",
    "                        cv2.addWeighted(overlay, 0.6, frame_resized, 0.4, 0, frame_resized)\n",
    "                        \n",
    "                        # Draw texts - add \"SAVED\" mark if sampled\n",
    "                        if is_sample_frame:\n",
    "                            saved_text = \"SAVED\"\n",
    "                            cv2.putText(frame_resized, saved_text, (mid_x, mid_y - 50),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                        \n",
    "                        # Draw gap measurements\n",
    "                        cv2.putText(frame_resized, dist_text, (mid_x, mid_y - 25),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                        cv2.putText(frame_resized, time_text, (mid_x, mid_y),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # If processing a sample frame, also collect gap data without visualization\n",
    "            elif is_sample_frame:\n",
    "                for i, det in enumerate(detections):\n",
    "                    if i < len(detections)-1:\n",
    "                        next_det = detections[i+1]\n",
    "                        gap_info = calculate_gap(\n",
    "                            det['box'], next_det['box'], \n",
    "                            det['class'] if det['classified'] else \"F1 Car\", \n",
    "                            next_det['class'] if next_det['classified'] else \"F1 Car\"\n",
    "                        )\n",
    "                        \n",
    "                        frame_gaps.append({\n",
    "                            'frame': current_frame,\n",
    "                            'timestamp': round(timestamp, 2),\n",
    "                            'car1_id': det['id'],\n",
    "                            'car2_id': next_det['id'],\n",
    "                            'car1_team': gap_info['car1'],\n",
    "                            'car2_team': gap_info['car2'],\n",
    "                            'distance_meters': round(gap_info['distance'], 2),\n",
    "                            'gap_seconds': round(gap_info['time'], 2)\n",
    "                        })\n",
    "            \n",
    "            # Add all gaps from this frame to our collection if it's a sample frame\n",
    "            if is_sample_frame:\n",
    "                all_gaps.extend(frame_gaps)\n",
    "                print(f\"Found {len(frame_gaps)} car gaps at timestamp {timestamp:.2f}s\")\n",
    "        \n",
    "        # Update last_detections for next iteration\n",
    "        last_detections = current_detections\n",
    "        \n",
    "        # Calculate FPS for display\n",
    "        if show_video and frame_count % 10 == 0:\n",
    "            current_time = pytime.time()\n",
    "            current_fps = 10.0 / (current_time - start_time)\n",
    "            start_time = current_time\n",
    "        \n",
    "        # Show visual elements if requested\n",
    "        if show_video:\n",
    "            # Show FPS and model information\n",
    "            cv2.putText(frame_resized, f\"FPS: {current_fps:.1f}\", (20, 40),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Add \"SAMPLE FRAME\" indicator if this is a frame being saved\n",
    "            if is_sample_frame:\n",
    "                cv2.putText(frame_resized, \"▶ SAMPLE FRAME\", (FRAME_WIDTH - 300, 40),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(frame_resized, \"F1 Gap Extraction\", (FRAME_WIDTH - 300, 40),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show detection threshold\n",
    "            detection_mode = f\"Detection Threshold: {GAP_DETECTION_THRESHOLD:.2f}\"\n",
    "            cv2.putText(frame_resized, detection_mode, (20, target_height - 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show progress\n",
    "            progress_text = f\"Frame: {current_frame}/{total_frames} ({current_frame/total_frames*100:.1f}%)\"\n",
    "            cv2.putText(frame_resized, progress_text, (FRAME_WIDTH - 400, target_height - 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show the frame\n",
    "            cv2.imshow(\"F1 Gap Extraction\", frame_resized)\n",
    "            \n",
    "            # Handle key presses\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('+'):  # Increase threshold\n",
    "                GAP_DETECTION_THRESHOLD = min(GAP_DETECTION_THRESHOLD + 0.05, 0.95)\n",
    "                print(f\"Detection threshold increased to {GAP_DETECTION_THRESHOLD:.2f}\")\n",
    "            elif key == ord('-'):  # Decrease threshold\n",
    "                GAP_DETECTION_THRESHOLD = max(GAP_DETECTION_THRESHOLD - 0.05, 0.05)\n",
    "                print(f\"Detection threshold decreased to {GAP_DETECTION_THRESHOLD:.2f}\")\n",
    "            elif key == ord('d'):  # Skip forward 10 seconds\n",
    "                skip_frames = int(fps * 10)\n",
    "                new_frame_pos = min(current_frame + skip_frames, total_frames - 1)\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, new_frame_pos)\n",
    "                current_frame = new_frame_pos - 1\n",
    "                last_detections = {}\n",
    "                print(f\"Skipped forward 10 seconds to frame {new_frame_pos}\")\n",
    "    \n",
    "    # Release video resource\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if all_gaps:\n",
    "        gaps_df = pd.DataFrame(all_gaps)\n",
    "        \n",
    "        # Save to CSV if requested\n",
    "        if output_csv is None:\n",
    "            # Generate default filename based on video name\n",
    "            video_name = os.path.basename(video_path).split('.')[0]\n",
    "            timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_csv = f\"../f1-strategy/data/gaps/gap_data_{video_name}_{timestamp_str}.csv\"\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "        \n",
    "        # Save data\n",
    "        gaps_df.to_csv(output_csv, index=False, float_format='%.3f')\n",
    "        print(f\"Gap data saved to {output_csv}\")\n",
    "        print(f\"Total of {len(gaps_df)} gap measurements collected\")\n",
    "        \n",
    "        return gaps_df\n",
    "    else:\n",
    "        print(\"No gap data could be collected!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Run gap extraction with visualization\n",
    "def main_extract_gaps():\n",
    "    \"\"\"Extract gaps from the video at 10-second intervals and save to CSV\"\"\"\n",
    "    video_path = \"../f1-strategy/data/videos/belgium_gp.f399.mp4\"\n",
    "    \n",
    "    # Run extraction with default 10-second interval, showing video\n",
    "    gap_data = extract_gaps_from_video(\n",
    "        video_path,\n",
    "        sample_interval_seconds=10,\n",
    "        show_video=True  # Enable video display\n",
    "    )\n",
    "    \n",
    "    # Display sample of extracted data\n",
    "    if gap_data is not None and not gap_data.empty:\n",
    "        print(\"\\nSample gap data:\")\n",
    "        print(gap_data.head())\n",
    "        \n",
    "        # Show some basic statistics\n",
    "        print(\"\\nGap statistics (seconds):\")\n",
    "        print(gap_data['gap_seconds'].describe())\n",
    "    \n",
    "    return gap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video FPS: 50, sampling every 500 frames (10 seconds)\n",
      "Starting gap extraction from ../f1-strategy/data/videos/belgium_gp.f399.mp4...\n",
      "Will sample approximately every 10 seconds\n",
      "Taking sample at frame 1 (timestamp: 0.02s)\n",
      "Found 2 car gaps at timestamp 0.02s\n",
      "Detection threshold increased to 0.30\n",
      "Detection threshold increased to 0.35\n",
      "Detection threshold increased to 0.40\n",
      "Detection threshold increased to 0.45\n",
      "Detection threshold increased to 0.50\n",
      "Detection threshold increased to 0.55\n",
      "Skipped forward 10 seconds to frame 686\n",
      "Taking sample at frame 686 (timestamp: 13.72s)\n",
      "Found 0 car gaps at timestamp 13.72s\n",
      "Skipped forward 10 seconds to frame 1186\n",
      "Taking sample at frame 1186 (timestamp: 23.72s)\n",
      "Found 0 car gaps at timestamp 23.72s\n",
      "Skipped forward 10 seconds to frame 1686\n",
      "Taking sample at frame 1686 (timestamp: 33.72s)\n",
      "Skipped forward 10 seconds to frame 2186\n",
      "Taking sample at frame 2186 (timestamp: 43.72s)\n",
      "Found 1 car gaps at timestamp 43.72s\n",
      "Skipped forward 10 seconds to frame 2686\n",
      "Taking sample at frame 2686 (timestamp: 53.72s)\n",
      "Found 1 car gaps at timestamp 53.72s\n",
      "Skipped forward 10 seconds to frame 3186\n",
      "Taking sample at frame 3186 (timestamp: 63.72s)\n",
      "Skipped forward 10 seconds to frame 3686\n",
      "Taking sample at frame 3686 (timestamp: 73.72s)\n",
      "Found 3 car gaps at timestamp 73.72s\n",
      "Skipped forward 10 seconds to frame 4186\n",
      "Taking sample at frame 4186 (timestamp: 83.72s)\n",
      "Found 1 car gaps at timestamp 83.72s\n",
      "Skipped forward 10 seconds to frame 4686\n",
      "Taking sample at frame 4686 (timestamp: 93.72s)\n",
      "Found 1 car gaps at timestamp 93.72s\n",
      "Skipped forward 10 seconds to frame 5233\n",
      "Taking sample at frame 5233 (timestamp: 104.66s)\n",
      "Found 2 car gaps at timestamp 104.66s\n",
      "Skipped forward 10 seconds to frame 5733\n",
      "Taking sample at frame 5733 (timestamp: 114.66s)\n",
      "Skipped forward 10 seconds to frame 6233\n",
      "Taking sample at frame 6233 (timestamp: 124.66s)\n",
      "Found 0 car gaps at timestamp 124.66s\n",
      "Skipped forward 10 seconds to frame 6733\n",
      "Taking sample at frame 6733 (timestamp: 134.66s)\n",
      "Found 1 car gaps at timestamp 134.66s\n",
      "Taking sample at frame 7233 (timestamp: 144.66s)\n",
      "Found 0 car gaps at timestamp 144.66s\n",
      "Skipped forward 10 seconds to frame 7821\n",
      "Taking sample at frame 7821 (timestamp: 156.42s)\n",
      "Skipped forward 10 seconds to frame 8321\n",
      "Taking sample at frame 8321 (timestamp: 166.42s)\n",
      "Found 1 car gaps at timestamp 166.42s\n",
      "Skipped forward 10 seconds to frame 8821\n",
      "Taking sample at frame 8821 (timestamp: 176.42s)\n",
      "Detection threshold increased to 0.60\n",
      "Detection threshold increased to 0.65\n",
      "Detection threshold increased to 0.70\n",
      "Taking sample at frame 9321 (timestamp: 186.42s)\n",
      "Found 1 car gaps at timestamp 186.42s\n",
      "Gap data saved to ../f1-strategy/data/gaps/gap_data_belgium_gp_20250411_114225.csv\n",
      "Total of 14 gap measurements collected\n",
      "\n",
      "Sample gap data:\n",
      "   frame  timestamp  car1_id  car2_id car1_team car2_team  distance_meters  \\\n",
      "0      1       0.02        2        0    F1 Car    F1 Car        17.364673   \n",
      "1      1       0.02        0        1    F1 Car    F1 Car        13.423841   \n",
      "2   2186      43.72       36       37    F1 Car    F1 Car        40.675699   \n",
      "3   2686      53.72       38       39   Ferarri    F1 Car         5.556660   \n",
      "4   3686      73.72       42       41    F1 Car    F1 Car        34.803207   \n",
      "\n",
      "   gap_seconds  \n",
      "0     0.208384  \n",
      "1     0.161093  \n",
      "2     0.488128  \n",
      "3     0.066683  \n",
      "4     0.417655  \n",
      "\n",
      "Gap statistics (seconds):\n",
      "count    14.000000\n",
      "mean      0.158485\n",
      "std       0.133139\n",
      "min       0.036265\n",
      "25%       0.084572\n",
      "50%       0.110610\n",
      "75%       0.158266\n",
      "max       0.488128\n",
      "Name: gap_seconds, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# # Cell 4: Execute the extraction\n",
    "# if __name__ == \"__main__\":\n",
    "#     gap_data = main_extract_gaps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclussions\n",
    "\n",
    "In this notebook, we've successfully implemented a computer vision system that:\n",
    "\n",
    "1. Detects F1 cars in video footage using our YOLOv8 model\n",
    "2. Calculates gaps between consecutive cars in both meters and seconds\n",
    "3. Extracts data at regular intervals (every 10 seconds) to feed into our strategy system\n",
    "4. Saves this information to structured CSV files with team identification\n",
    "\n",
    "\n",
    "### 3.1 CSV Structure\n",
    "\n",
    "Our gap extraction system produces CSV files with the following columns:\n",
    "\n",
    "- ``frame``: Frame number in the video\n",
    "- ``timestamp``: Time in seconds from the start of the video\n",
    "- ``car1_id and car2_id``: Unique identifiers for each detected car\n",
    "- ``car1_team and car2_team``: Team identifications based on our model\n",
    "- ``distance_meters``: Physical distance between cars in meters\n",
    "- ``gap_seconds``: Time gap between cars (in seconds at 300 km/h)\n",
    "\n",
    "This data provides the foundation for our gap-based strategy rules, enabling our expert system to make informed decisions about:\n",
    "\n",
    "- Undercut/overcut opportunities\n",
    "- Traffic management\n",
    "- Defensive positioning\n",
    "\n",
    "\n",
    "### 3.2 Next Steps\n",
    "With our gap data now available, the next phase is to integrate this information into our expert system by:\n",
    "\n",
    "- Creating gap-specific rules that detect strategic opportunities\n",
    "- Combining gap data with our existing tire degradation and lap time models\n",
    "- Implementing undercut/overcut detection logic based on real-world F1 strategy principles\n",
    "\n",
    "In the next notebook (``N05_gap_strategy_rules.ipynb``), we'll develop these rules and integrate all components of our F1 strategy system.\n",
    "\n",
    "However, due to that this data can be not enough or too much erratic, we will do the gap rules with fastf1 data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
