{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language: python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar el modelo fine tuned (de YOLO_fine_tune.ipynb)\n",
    "model = YOLO(\"../f1-strategy/weights/f1_detection_fine_tuned_best.pt\")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Parámetros para el cálculo de gaps\n",
    "CAR_LENGTH_METERS = 5.5  # Longitud del coche en metros\n",
    "FRAME_WIDTH = 640        # Ancho del frame que se usará (redimensionar)\n",
    "\n",
    "def calculate_gap(box1, box2, frame_width):\n",
    "    \"\"\"\n",
    "    Calcula el gap entre dos coches basándose en sus bounding boxes.\n",
    "    \"\"\"\n",
    "    x1, _, x2, _ = box1\n",
    "    x3, _, x4, _ = box2\n",
    "    center1 = (x1 + x2) / 2\n",
    "    center2 = (x3 + x4) / 2\n",
    "    pixel_distance = abs(center2 - center1)\n",
    "    meters_per_pixel = CAR_LENGTH_METERS / (frame_width / 3)  # Conversión aproximada\n",
    "    return pixel_distance * meters_per_pixel\n",
    "\n",
    "def process_video_with_yolo(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Redimensionar el frame para que coincida con el tamaño de entrada del modelo\n",
    "        frame_resized = cv2.resize(frame, (FRAME_WIDTH, FRAME_WIDTH))\n",
    "        \n",
    "        # Obtener detecciones usando el método predict del modelo\n",
    "        results = model.predict(frame_resized, conf=0.4, verbose=False)[0]\n",
    "        \n",
    "        # Verificar que se hayan detectado objetos\n",
    "        if results.boxes is not None and len(results.boxes) > 0:\n",
    "            # Extrae las bounding boxes y conviértelas a numpy array\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()  # Cada box: [x1, y1, x2, y2]\n",
    "            \n",
    "            # Ordenar las boxes según el centro en X\n",
    "            boxes_sorted = sorted(boxes, key=lambda b: (b[0] + b[2]) / 2)\n",
    "            \n",
    "            # Dibujar cada bounding box\n",
    "            for box in boxes_sorted:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame_resized, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                \n",
    "            # Calcular y dibujar el gap entre cada par de cajas consecutivas\n",
    "            for i in range(len(boxes_sorted) - 1):\n",
    "                gap = calculate_gap(boxes_sorted[i], boxes_sorted[i + 1], FRAME_WIDTH)\n",
    "                \n",
    "                # Calcular los centros horizontales de cada box\n",
    "                b1 = boxes_sorted[i]\n",
    "                b2 = boxes_sorted[i + 1]\n",
    "                cx1 = int((b1[0] + b1[2]) // 2)\n",
    "                cx2 = int((b2[0] + b2[2]) // 2)\n",
    "                # Usar el valor de y inferior (de la caja superior) para anotar el gap\n",
    "                y1_bottom = int(b1[3])\n",
    "                \n",
    "                # Dibujar la línea que une los centros\n",
    "                cv2.line(frame_resized, (cx1, y1_bottom), (cx2, y1_bottom), (0, 255, 0), 2)\n",
    "                \n",
    "                # Escribir el gap (en metros) cerca de la línea\n",
    "                cv2.putText(frame_resized, f\"{gap:.2f}m\", (cx1, y1_bottom - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.imshow(\"F1 Gap Detection\", frame_resized)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Ejecutar detección de gaps en un video real\n",
    "video_path = \"../f1-strategy/data/videos/abu_dhabi_2024_race.mp4.f399.mp4\"\n",
    "process_video_with_yolo(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
