{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula 1 NER Recognition\n",
    "\n",
    "As a continuation of `N04_radio_info.ipynb`, this notebook demonstrates the development of a custom Named Entity Recognition (NER) system for Formula 1 team radio communications. \n",
    "\n",
    "The system extracts structured information from unstructured radio messages exchanged between drivers and race engineers during F1 races."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology \n",
    "\n",
    "The notebook implements:\n",
    "\n",
    "1. **Data preparation** - Loading annotated F1 radio messages with entity labels\n",
    "\n",
    "2. **BIO tagging** - Converting character-level entity spans to token-level Beginning-Inside-Outside format\n",
    "\n",
    "3. **Model architecture** - Training transformer-based models including DeBERTa v3 and BERT. making a fine-tuning of this last transformer.\n",
    "\n",
    "4. **Fine-tuning strategies** - Testing various approaches including class weighting and focused training\n",
    "\n",
    "5. **Evaluation** - Detailed entity-level performance analysis and model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2ForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constant Definition \n",
    "\n",
    "In this cell, IÂ´ll document the type of entities and their correspondent colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity types defined:\n",
      "  - ACTION: Direct commands or actions mentioned in the message\n",
      "  - SITUATION: Racing context or circumstance descriptions\n",
      "  - INCIDENT: Accidents or on-track events\n",
      "  - STRATEGY_INSTRUCTION: Strategic directives\n",
      "  - POSITION_CHANGE: References to overtakes or positions\n",
      "  - PIT_CALL: Specific calls for pit stops\n",
      "  - TRACK_CONDITION: Mentions of the track's state\n",
      "  - TECHNICAL_ISSUE: Mechanical or car-related problems\n",
      "  - WEATHER: References to weather conditions\n"
     ]
    }
   ],
   "source": [
    "# Define entity types and their descriptions\n",
    "ENTITY_TYPES = {\n",
    "    \"ACTION\": \"Direct commands or actions mentioned in the message\",\n",
    "    \"SITUATION\": \"Racing context or circumstance descriptions\",\n",
    "    \"INCIDENT\": \"Accidents or on-track events\",\n",
    "    \"STRATEGY_INSTRUCTION\": \"Strategic directives\",\n",
    "    \"POSITION_CHANGE\": \"References to overtakes or positions\",\n",
    "    \"PIT_CALL\": \"Specific calls for pit stops\",\n",
    "    \"TRACK_CONDITION\": \"Mentions of the track's state\",\n",
    "    \"TECHNICAL_ISSUE\": \"Mechanical or car-related problems\",\n",
    "    \"WEATHER\": \"References to weather conditions\"\n",
    "}\n",
    "\n",
    "# Color scheme for entity visualization\n",
    "ENTITY_COLORS = {\n",
    "    \"ACTION\": \"#4e79a7\",           # Blue\n",
    "    \"SITUATION\": \"#f28e2c\",         # Orange\n",
    "    \"INCIDENT\": \"#e15759\",          # Red\n",
    "    \"STRATEGY_INSTRUCTION\": \"#76b7b2\", # Teal\n",
    "    \"POSITION_CHANGE\": \"#59a14f\",   # Green\n",
    "    \"PIT_CALL\": \"#edc949\",          # Yellow\n",
    "    \"TRACK_CONDITION\": \"#af7aa1\",   # Purple\n",
    "    \"TECHNICAL_ISSUE\": \"#ff9da7\",   # Pink\n",
    "    \"WEATHER\": \"#9c755f\"            # Brown\n",
    "}\n",
    "\n",
    "print(\"Entity types defined:\")\n",
    "for entity, description in ENTITY_TYPES.items():\n",
    "    print(f\"  - {entity}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load F1 radio data from JSON file\n",
    "def load_f1_radio_data(json_file):\n",
    "    \"\"\"Load and explore F1 radio data from JSON file\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} messages from {json_file}\")\n",
    "    \n",
    "    # Show sample structure\n",
    "    if len(data) > 0:\n",
    "        print(\"\\nSample record structure:\")\n",
    "        sample = data[0]\n",
    "        print(f\"  Driver: {sample.get('driver', 'N/A')}\")\n",
    "        print(f\"  Radio message: {sample.get('radio_message', 'N/A')[:100]}...\")\n",
    "        \n",
    "        if 'annotations' in sample and len(sample['annotations']) > 1:\n",
    "            if isinstance(sample['annotations'][1], dict) and 'entities' in sample['annotations'][1]:\n",
    "                entities = sample['annotations'][1]['entities']\n",
    "                print(f\"  Number of entities: {len(entities)}\")\n",
    "                if len(entities) > 0:\n",
    "                    entity = entities[0]\n",
    "                    entity_text = sample['radio_message'][entity[0]:entity[1]]\n",
    "                    print(f\"  Sample entity: [{entity[0]}, {entity[1]}, '{entity_text}', '{entity[2]}']\")\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 529 messages from ../../outputs/week4/NER/f1_radio_entity_annotations.json\n",
      "\n",
      "Sample record structure:\n",
      "  Driver: 1\n",
      "  Radio message: So don't forget Max, use your head please. Are we both doing it or what? You just follow my instruct...\n",
      "  Number of entities: 3\n",
      "  Sample entity: [82, 103, 'follow my instruction', 'ACTION']\n",
      "\n",
      "Entity type distribution in dataset:\n",
      "  - SITUATION: 255\n",
      "  - ACTION: 165\n",
      "  - STRATEGY_INSTRUCTION: 137\n",
      "  - TECHNICAL_ISSUE: 137\n",
      "  - WEATHER: 112\n",
      "  - POSITION_CHANGE: 83\n",
      "  - INCIDENT: 78\n",
      "  - TRACK_CONDITION: 62\n",
      "  - PIT_CALL: 42\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data\n",
    "json_file_path = \"../../outputs/week4/NER/f1_radio_entity_annotations.json\"\n",
    "f1_data = load_f1_radio_data(json_file_path)\n",
    "\n",
    "# Count entity types in the dataset\n",
    "entity_counts = {}\n",
    "for item in f1_data:\n",
    "    if 'annotations' in item and len(item['annotations']) > 1:\n",
    "        if isinstance(item['annotations'][1], dict) and 'entities' in item['annotations'][1]:\n",
    "            for _, _, entity_type in item['annotations'][1]['entities']:\n",
    "                entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1\n",
    "\n",
    "print(\"\\nEntity type distribution in dataset:\")\n",
    "for entity_type, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  - {entity_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing F1 Radio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_f1_data(data):\n",
    "    \"\"\"Extract and preprocess F1 radio data with valid annotations\"\"\"\n",
    "    processed_data = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for item in data:\n",
    "        if 'radio_message' not in item or 'annotations' not in item:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        text = item['radio_message']\n",
    "        \n",
    "        # Skip items with empty or null text\n",
    "        if not text or text.strip() == \"\":\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # Extract entities if they exist in expected format\n",
    "        if len(item['annotations']) > 1 and isinstance(item['annotations'][1], dict):\n",
    "            annotations = item['annotations'][1]\n",
    "            if 'entities' in annotations and annotations['entities']:\n",
    "                entities = annotations['entities']\n",
    "                \n",
    "                # Add to processed data\n",
    "                processed_data.append({\n",
    "                    'text': text,\n",
    "                    'entities': entities,\n",
    "                    'driver': item.get('driver', None)\n",
    "                })\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"Processed {len(processed_data)} messages with valid annotations\")\n",
    "    print(f\"Skipped {skipped_count} messages with missing or invalid annotations\")\n",
    "    \n",
    "    # Show a sample of processed data\n",
    "    if processed_data:\n",
    "        sample = processed_data[10]\n",
    "        print(\"\\nSample processed message:\")\n",
    "        print(f\"Text: {sample['text']}\")\n",
    "        print(\"Entities:\")\n",
    "        for start, end, entity_type in sample['entities']:\n",
    "            entity_text = sample['text'][start:end]\n",
    "            print(f\"  - [{start}, {end}] '{entity_text}' ({entity_type})\")\n",
    "    \n",
    "    return processed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 399 messages with valid annotations\n",
      "Skipped 130 messages with missing or invalid annotations\n",
      "\n",
      "Sample processed message:\n",
      "Text: Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\n",
      "Entities:\n",
      "  - [159, 194] 'keep to your protocol at the moment' (ACTION)\n",
      "  - [5, 42] 'we've currently got yellows in turn 7' (SITUATION)\n",
      "  - [98, 148] 'We are expecting the potential of an aborted start' (SITUATION)\n",
      "  - [44, 63] 'Ferrari in the wall' (INCIDENT)\n",
      "  - [74, 96] 'that's Charles stopped' (INCIDENT)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the loaded data\n",
    "processed_f1_data = preprocess_f1_data(f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Covert to BIO tagging format\n",
    "\n",
    "Deeper BIO tagging format information can be searched [here](https://en.wikipedia.org/wiki/Insideâoutsideâbeginning_(tagging)).\n",
    "\n",
    "### BIO Format Explanation\n",
    "\n",
    "The **BIO format** is a way to label words in a sentence to indicate if they are part of a named entity, and if so, where in the entity they belong. It uses three types of labels:\n",
    "\n",
    "- **B- (Beginning)**: The first word in an entity.\n",
    "- **I- (Inside)**: Any word inside the entity that isn't the first one.\n",
    "- **O (Outside)**: Words that are not part of any entity.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Radio\n",
    "\n",
    "Here is an example of a radio message from Max VerstappenÂ´s track engineer: \n",
    "\n",
    "**Text:**  \n",
    "*\"Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\"*\n",
    "\n",
    "Here are the entities mentioned in the message:\n",
    "\n",
    "1. **'keep to your protocol at the moment'** (ACTION)\n",
    "2. **'we've currently got yellows in turn 7'** (SITUATION)\n",
    "3. **'We are expecting the potential of an aborted start'** (SITUATION)\n",
    "4. **'Ferrari in the wall'** (INCIDENT)\n",
    "5. **'that's Charles stopped'** (INCIDENT)\n",
    "\n",
    "---\n",
    "\n",
    "### Breaking the Sentence\n",
    "\n",
    "We break the sentence into words and then tag them as follows:\n",
    "\n",
    "| Word            | BIO Tag          |\n",
    "|-----------------|------------------|\n",
    "| Max,            | O                |\n",
    "| we've           | O                |\n",
    "| currently       | O                |\n",
    "| got             | O                |\n",
    "| yellows         | O                |\n",
    "| in              | O                |\n",
    "| turn            | O                |\n",
    "| 7.              | O                |\n",
    "| Ferrari         | B-INCIDENT       |\n",
    "| in              | I-INCIDENT       |\n",
    "| the             | I-INCIDENT       |\n",
    "| wall,           | I-INCIDENT       |\n",
    "| no?             | O                |\n",
    "| Yes,            | O                |\n",
    "| that's          | B-INCIDENT       |\n",
    "| Charles         | I-INCIDENT       |\n",
    "| stopped.        | I-INCIDENT       |\n",
    "| We              | B-SITUATION      |\n",
    "| are             | I-SITUATION      |\n",
    "| expecting       | I-SITUATION      |\n",
    "| the             | I-SITUATION      |\n",
    "| potential       | I-SITUATION      |\n",
    "| of              | I-SITUATION      |\n",
    "| an              | I-SITUATION      |\n",
    "| aborted         | I-SITUATION      |\n",
    "| start,          | I-SITUATION      |\n",
    "| but             | O                |\n",
    "| just            | O                |\n",
    "| keep            | B-ACTION         |\n",
    "| to              | I-ACTION         |\n",
    "| your            | I-ACTION         |\n",
    "| protocol        | I-ACTION         |\n",
    "| at              | I-ACTION         |\n",
    "| the             | I-ACTION         |\n",
    "| moment.         | I-ACTION         |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_tags(text, entities):\n",
    "    \"\"\"Convert character-based entity spans to token-based BIO tags\"\"\"\n",
    "    words = text.split()\n",
    "    tags = [\"O\"] * len(words)\n",
    "    char_to_word = {}\n",
    "    \n",
    "    # Create mapping from character positions to word indices\n",
    "    char_idx = 0\n",
    "    for word_idx, word in enumerate(words):\n",
    "        # Account for spaces\n",
    "        if char_idx > 0:\n",
    "            char_idx += 1  # Space\n",
    "        \n",
    "        # Map each character position to its word index\n",
    "        for char_pos in range(char_idx, char_idx + len(word)):\n",
    "            char_to_word[char_pos] = word_idx\n",
    "        \n",
    "        char_idx += len(word)\n",
    "    \n",
    "    # Apply entity tags\n",
    "    for start_char, end_char, entity_type in entities:\n",
    "        # Skip invalid spans\n",
    "        if start_char >= len(text) or end_char > len(text) or start_char >= end_char:\n",
    "            continue\n",
    "            \n",
    "        # Find word indices for start and end characters\n",
    "        if start_char in char_to_word:\n",
    "            start_word = char_to_word[start_char]\n",
    "            # Find the last word of the entity\n",
    "            end_word = char_to_word.get(end_char - 1, start_word)\n",
    "            \n",
    "            # Tag the first word as B-entity\n",
    "            tags[start_word] = f\"B-{entity_type}\"\n",
    "            \n",
    "            # Tag subsequent words as I-entity\n",
    "            for word_idx in range(start_word + 1, end_word + 1):\n",
    "                tags[word_idx] = f\"I-{entity_type}\"\n",
    "    \n",
    "    return words, tags\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio_format(processed_data):\n",
    "    \"\"\"Convert processed data to BIO tagging format\"\"\"\n",
    "    bio_data = []\n",
    "    mapping_errors = 0\n",
    "    \n",
    "    for item in processed_data:\n",
    "        text = item['text']\n",
    "        entities = item['entities']\n",
    "        \n",
    "        # Convert to BIO tags\n",
    "        words, tags = create_ner_tags(text, entities)\n",
    "        \n",
    "        # Check if we mapped any entities\n",
    "        if all(tag == \"O\" for tag in tags) and len(entities) > 0:\n",
    "            mapping_errors += 1\n",
    "        \n",
    "        bio_data.append({\n",
    "            \"tokens\": words,\n",
    "            \"ner_tags\": tags,\n",
    "            \"driver\": item.get('driver', None)\n",
    "        })\n",
    "    \n",
    "    print(f\"Converted {len(bio_data)} messages to BIO format\")\n",
    "    print(f\"Mapping errors: {mapping_errors} (messages where no entities were mapped)\")\n",
    "    \n",
    "    # Show an example\n",
    "    if bio_data:\n",
    "        sample = bio_data[10]\n",
    "        print(\"\\nSample BIO tagging:\")\n",
    "        print(f\"Original text: {' '.join(sample['tokens'])}\")\n",
    "        for token, tag in zip(sample['tokens'], sample['ner_tags']):\n",
    "            print(f\"  {token} -> {tag}\")\n",
    "    \n",
    "    return bio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 399 messages to BIO format\n",
      "Mapping errors: 0 (messages where no entities were mapped)\n",
      "\n",
      "Sample BIO tagging:\n",
      "Original text: Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\n",
      "  Max, -> O\n",
      "  we've -> B-SITUATION\n",
      "  currently -> I-SITUATION\n",
      "  got -> I-SITUATION\n",
      "  yellows -> I-SITUATION\n",
      "  in -> I-SITUATION\n",
      "  turn -> I-SITUATION\n",
      "  7. -> I-SITUATION\n",
      "  Ferrari -> B-INCIDENT\n",
      "  in -> I-INCIDENT\n",
      "  the -> I-INCIDENT\n",
      "  wall, -> I-INCIDENT\n",
      "  no? -> O\n",
      "  Yes, -> O\n",
      "  that's -> B-INCIDENT\n",
      "  Charles -> I-INCIDENT\n",
      "  stopped. -> I-INCIDENT\n",
      "  We -> B-SITUATION\n",
      "  are -> I-SITUATION\n",
      "  expecting -> I-SITUATION\n",
      "  the -> I-SITUATION\n",
      "  potential -> I-SITUATION\n",
      "  of -> I-SITUATION\n",
      "  an -> I-SITUATION\n",
      "  aborted -> I-SITUATION\n",
      "  start, -> I-SITUATION\n",
      "  but -> O\n",
      "  just -> O\n",
      "  keep -> B-ACTION\n",
      "  to -> I-ACTION\n",
      "  your -> I-ACTION\n",
      "  protocol -> I-ACTION\n",
      "  at -> I-ACTION\n",
      "  the -> I-ACTION\n",
      "  moment. -> I-ACTION\n"
     ]
    }
   ],
   "source": [
    "# Convert processed data to BIO format\n",
    "bio_data = convert_to_bio_format(processed_f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the Function Does\n",
    "\n",
    "The function `create_ner_tags` takes the text and entities and converts them into BIO format. It starts by splitting the text into words. \n",
    "\n",
    "Then, it maps each word to a tag: \"O\" for words that are not part of an entity, \"B-\" for the first word of an entity, and \"I-\" for subsequent words inside the entity. \n",
    "\n",
    "The function also uses the character positions of the entities to determine which words they correspond to. Once the tags are assigned, the function returns the words and their BIO tags, ready for use in training a Named Entity Recognition (NER) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create tag mappings and prepare datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 `create_tag_mappings`\n",
    "\n",
    "This function creates mappings between NER (Named Entity Recognition) tags and unique IDs. It does this by:\n",
    "\n",
    "1. Collecting all unique NER tags from the `bio_data`.\n",
    "2. Sorting and assigning each unique tag an ID.\n",
    "3. Creating two mappings:\n",
    "   - `tag2id`: Maps each tag to its corresponding ID.\n",
    "   - `id2tag`: Maps each ID back to its corresponding tag.\n",
    "\n",
    "It then prints out the mappings and returns the two dictionaries: `tag2id` and `id2tag`.\n",
    "\n",
    "**What it does:**\n",
    "- Converts NER tags into unique IDs for easier processing in machine learning models.\n",
    "- Helps with transforming the tags when working with model inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_mappings(bio_data):\n",
    "    \"\"\"Create mappings between NER tags and IDs\"\"\"\n",
    "    unique_tags = set()\n",
    "    for item in bio_data:\n",
    "        unique_tags.update(item[\"ner_tags\"])\n",
    "    \n",
    "    tag2id = {tag: id for id, tag in enumerate(sorted(list(unique_tags)))}\n",
    "    id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "    \n",
    "    print(f\"Created mappings for {len(tag2id)} unique tags:\")\n",
    "    for tag, idx in tag2id.items():\n",
    "        print(f\"  {tag}: {idx}\")\n",
    "    \n",
    "    return tag2id, id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mappings for 19 unique tags:\n",
      "  B-ACTION: 0\n",
      "  B-INCIDENT: 1\n",
      "  B-PIT_CALL: 2\n",
      "  B-POSITION_CHANGE: 3\n",
      "  B-SITUATION: 4\n",
      "  B-STRATEGY_INSTRUCTION: 5\n",
      "  B-TECHNICAL_ISSUE: 6\n",
      "  B-TRACK_CONDITION: 7\n",
      "  B-WEATHER: 8\n",
      "  I-ACTION: 9\n",
      "  I-INCIDENT: 10\n",
      "  I-PIT_CALL: 11\n",
      "  I-POSITION_CHANGE: 12\n",
      "  I-SITUATION: 13\n",
      "  I-STRATEGY_INSTRUCTION: 14\n",
      "  I-TECHNICAL_ISSUE: 15\n",
      "  I-TRACK_CONDITION: 16\n",
      "  I-WEATHER: 17\n",
      "  O: 18\n"
     ]
    }
   ],
   "source": [
    "# Create tag mappings\n",
    "tag2id, id2tag = create_tag_mappings(bio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 `prepare_datasets`\n",
    "\n",
    "This function prepares the dataset for training a model by splitting it into training, validation, and test sets using the Hugging Face library. Here's what it does:\n",
    "\n",
    "1. Converts the input `bio_data` into a Hugging Face `Dataset`.\n",
    "2. Splits the data into two parts: training + validation, and test.\n",
    "3. Further splits the training data into training and validation sets based on the specified sizes (`test_size` and `val_size`).\n",
    "4. Returns a `DatasetDict` containing the `train`, `validation`, and `test` sets.\n",
    "\n",
    "**What it does:**\n",
    "- Converts the data into a format suitable for machine learning.\n",
    "- Splits the data into three parts: training, validation, and test sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(bio_data, test_size=0.1, val_size=0.1, seed=42):\n",
    "    \"\"\"Convert to Hugging Face Dataset and split into train/val/test\"\"\"\n",
    "    # Convert to Hugging Face dataset\n",
    "    hf_dataset = HFDataset.from_list(bio_data)\n",
    "    \n",
    "    # First split: train + validation vs test\n",
    "    train_val_test = hf_dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "    \n",
    "    # Second split: train vs validation (validation is val_size/(1-test_size) of the train set)\n",
    "    val_fraction = val_size / (1 - test_size)\n",
    "    train_val = train_val_test[\"train\"].train_test_split(test_size=val_fraction, seed=seed)\n",
    "    \n",
    "    # Combine into DatasetDict\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_val[\"train\"],\n",
    "        \"validation\": train_val[\"test\"],\n",
    "        \"test\": train_val_test[\"test\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"Prepared datasets with:\")\n",
    "    print(f\"  - Train: {len(datasets['train'])} examples\")\n",
    "    print(f\"  - Validation: {len(datasets['validation'])} examples\")\n",
    "    print(f\"  - Test: {len(datasets['test'])} examples\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared datasets with:\n",
      "  - Train: 319 examples\n",
      "  - Validation: 40 examples\n",
      "  - Test: 40 examples\n"
     ]
    }
   ],
   "source": [
    "datasets = prepare_datasets(bio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Calling Up the Model \n",
    "\n",
    "In the first run, I tried with *Microsoft Deberta-v3-large*, a bigger model than BERT or RoBERTa. I believe more that the robustness of this architecture can provide good results.\n",
    "\n",
    "As it will be seen, the f1-score of this model is not too bad and could be enhanced with further development (in 2 or 3 runs I tried different loss functions, focal loss, epochs, learning rates, etc). However, some lighter models like specific architectures derived from BERT have good metrics and are more easily customizable.\n",
    "\n",
    "Therefore, I guided the development to these models. However, much of the code made in the following cells are used by BERT models, so it is important to keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: DebertaV2Tokenizer\n",
      "Vocabulary size: 128001\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Cell 2: Initialize the tokenizer for DeBERTa v3 large\n",
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Check if it loaded correctly\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Custom Dataset for Deberta-v3 Tokenization\n",
    "\n",
    "The ``F1RadioNERDataset`` class is a custom dataset for Named Entity Recognition (NER) tasks using PyTorch. It is designed to work with a Hugging Face dataset (or our dataset ), a tokenizer, and a tag-to-id mapping.\n",
    "\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. Initialization (``__init__``):\n",
    "\n",
    "    - Accepts a dataset, tokenizer, tag-to-id mapping, and maximum sequence length.\n",
    "\n",
    "    - Stores these for later use in processing data.\n",
    "\n",
    "2. Length Method (``__len__``):\n",
    "\n",
    "    - Returns the total number of examples in the dataset.\n",
    "\n",
    "3. Item Retrieval (``__getitem__``):\n",
    "\n",
    "    - Retrieves a single data example by index.\n",
    "\n",
    "    - Tokenizes each word in the example while maintaining a mapping of tokens to their original word indices.\n",
    "\n",
    "    - Truncates the tokenized sequence if it exceeds the maximum length (keeping space for special tokens like [CLS] and [SEP]).\n",
    "\n",
    "    - Uses the tokenizer to add special tokens and convert tokens to their corresponding IDs.\n",
    "\n",
    "    - Creates a labels tensor where the NER tags are aligned with the tokens. Special tokens and padding tokens are marked with -100 so that they are ignored during loss computation.\n",
    "\n",
    "    - Returns a dictionary with input_ids, attention_mask, and labels ready for model input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1RadioNERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, tag2id, max_len=128):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with a Hugging Face dataset, tokenizer, tag-to-id mapping, and maximum sequence length.\n",
    "        \n",
    "        Parameters:\n",
    "        - hf_dataset: The dataset containing tokenized text and NER tags.\n",
    "        - tokenizer: A tokenizer to process and convert text to token IDs.\n",
    "        - tag2id: A dictionary mapping NER tag strings to numerical IDs.\n",
    "        - max_len: The maximum length for token sequences (default is 128).\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id  # Mapping of NER tags to their numeric IDs\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of examples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the data example at the given index, processes tokens and tags,\n",
    "        and prepares input tensors for a NER model.\n",
    "        \n",
    "        Parameters:\n",
    "        - idx: Index of the data example to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "        A dictionary containing:\n",
    "        - input_ids: Tensor of token IDs for the input sequence.\n",
    "        - attention_mask: Tensor indicating which tokens are real (1) and which are padding (0).\n",
    "        - labels: Tensor of label IDs for the tokens, with -100 for tokens to be ignored.\n",
    "        \"\"\"\n",
    "        # Get the raw data example\n",
    "        item = self.dataset[idx]\n",
    "        tokens = item[\"tokens\"]         # List of word tokens\n",
    "        tags = item[\"ner_tags\"]         # Corresponding NER tags\n",
    "        \n",
    "        # Initialize lists to store word indices and all sub-tokens\n",
    "        word_ids = []  # Maps each sub-token to its original word index\n",
    "        all_tokens = []  # Stores all sub-tokens after tokenization\n",
    "        \n",
    "        # Tokenize each word separately\n",
    "        for word_idx, word in enumerate(tokens):\n",
    "            # Tokenize the word using the provided tokenizer\n",
    "            word_tokens = self.tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                # Handle cases where tokenization results in an empty list\n",
    "                word_tokens = [self.tokenizer.unk_token]\n",
    "            \n",
    "            # For each sub-token produced from the word, record the original word index\n",
    "            for _ in word_tokens:\n",
    "                word_ids.append(word_idx)\n",
    "                \n",
    "            # Add the sub-tokens to the overall list of tokens\n",
    "            all_tokens.extend(word_tokens)\n",
    "        \n",
    "        # Check if the tokenized sequence needs truncation\n",
    "        if len(all_tokens) > self.max_len - 2:  # -2 accounts for special tokens ([CLS] and [SEP])\n",
    "            all_tokens = all_tokens[:self.max_len - 2]\n",
    "            word_ids = word_ids[:self.max_len - 2]\n",
    "        \n",
    "        # Encode the tokenized input, adding special tokens and padding as needed\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            all_tokens,\n",
    "            is_split_into_words=False,  # Already tokenized input; no further splitting required\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        # Initialize the labels tensor with -100 to ignore special tokens and padding during loss computation\n",
    "        labels = torch.ones(self.max_len, dtype=torch.long) * -100\n",
    "        \n",
    "        # Align NER labels with the tokenized sequence\n",
    "        # The first token ([CLS]) is already set to -100 by initialization\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            # Ensure we do not overwrite the [CLS] token and reserve space for [SEP]\n",
    "            if i + 1 < self.max_len - 1:\n",
    "                # Check if the tag is a string and convert it to its numeric ID if necessary\n",
    "                if isinstance(tags[word_idx], str):\n",
    "                    tag_id = self.tag2id.get(tags[word_idx], 0)  # Defaults to 0, often representing 'O'\n",
    "                else:\n",
    "                    tag_id = tags[word_idx]  # Already a numeric ID\n",
    "                    \n",
    "                # Set the label at the corresponding position (offset by 1 for [CLS])\n",
    "                labels[i + 1] = tag_id\n",
    "        \n",
    "        # Return the processed inputs as a dictionary\n",
    "        return {\n",
    "            \"input_ids\": encoded_input[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoded_input[\"attention_mask\"].flatten(),\n",
    "            \"labels\": labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Pytorch Setup\n",
    "\n",
    "In PyTorch, creating custom datasets and corresponding DataLoaders is essential for efficiently feeding data into our model during training and evaluation. \n",
    "\n",
    "I found that the following steps are crucial:\n",
    "\n",
    "### A) Creating Pytorch Datasets\n",
    "\n",
    "By using the ``F1RadioNERDataset`` class, we convert our raw data (tokens, NER tags, etc.) into a format that is compatible with PyTorch. \n",
    "\n",
    "This allows us to perform operations such as tokenization and label alignment on the fly. Passing the ``tag2id`` mapping ensures that the **NER tags are correctly converted into numeric IDs**, which is **necessary for training** the model.\n",
    "\n",
    "### B) Creating the DataLoaders\n",
    "\n",
    "The DataLoader is a PyTorch utility that **provides an iterable over our dataset**. It handles **batching, shuffling (for training), and even parallel data loading** with multiple workers if needed. \n",
    "\n",
    "This *makes the training process more efficient* and helps in managing *memory usage*, parts that are crucial knwoging that I am training these models in my own graphics card, with a limited VRAM. \n",
    "\n",
    "By specifying a batch size and shuffling the training data, we ensure that each mini-batch is representative and that the **model doesnât overfit to the order** of the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Creating Pytorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets using the custom F1RadioNERDataset class.\n",
    "# Passing the tag2id mapping is crucial as it converts NER tag strings to numeric IDs,\n",
    "# which are needed for model training.\n",
    "train_dataset = F1RadioNERDataset(datasets[\"train\"], tokenizer, tag2id)\n",
    "val_dataset = F1RadioNERDataset(datasets[\"validation\"], tokenizer, tag2id)\n",
    "test_dataset = F1RadioNERDataset(datasets[\"test\"], tokenizer, tag2id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Creating Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for the datasets.\n",
    "# DataLoaders are used in PyTorch to efficiently batch and shuffle data during training and evaluation.\n",
    "# They help in managing memory and speeding up the training process by allowing parallel data loading.\n",
    "batch_size = 8  # Reduced batch size due to model size constraints and resource availability.\n",
    "\n",
    "# For training, shuffling is enabled to ensure the model does not learn the order of the data.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# For validation and testing, shuffling is typically not required.\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Validating Samples\n",
    "\n",
    "In this step, I will only print some of the training samples and also the shapes present in the dataset.\n",
    "\n",
    "This way, I know if the split was made correctly, and also if all the sizes are the same. If they were not, error would occur during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 319\n",
      "Validation samples: 40\n",
      "Test samples: 40\n",
      "Sample input shape: torch.Size([128])\n",
      "Sample attention mask shape: torch.Size([128])\n",
      "Sample labels shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Optional: Check a sample to verify everything is working\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample input shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Sample attention mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Sample labels shape: {sample['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Initializing Deberta\n",
    "\n",
    "In the following cell, next things will be implemented:\n",
    "\n",
    "#### I) Device Setup\n",
    "PyTorch models and tensors need to be moved to the appropriate hardware (CPU or GPU) for computation. \n",
    "\n",
    "Checking if a GPU (CUDA) is available and setting the device accordingly ensures that the model leverages the faster processing power of a GPU when available. I will use it, as I have a Nvidia GPU with the drivers installed.\n",
    "\n",
    "This is crucial for efficient training and inference, especially when dealing with large models.\n",
    "\n",
    "#### II) Model Initialization with Label Information\n",
    "\n",
    "The ``DebertaV2ForTokenClassification`` model is loaded from a pretrained checkpoint, and it needs to know the number of labels for token classification. \n",
    "\n",
    "By using ``num_labels = len(tag2id)``, the model is configured to produce outputs that align with the NER task. This ensures that the final classification layer has the correct dimensions to predict the right classes.\n",
    "\n",
    "#### III) Moving the model to the device and Feedback print Statements\n",
    "\n",
    "Once the model is loaded and configured, moving it to the chosen device (CPU or GPU) is necessary. This step transfers all model parameters to the selected device, ensuring that subsequent computations (like forward passes and gradients during training) occur on the correct hardware.\n",
    "\n",
    "Moreover, it is a common practice to print the device, model name and number of labels, as it provides useful information for confirmation and debugging. Therefore, we can easily check if the model is correctly configured and that it will run in on the intended hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: microsoft/deberta-v3-large\n",
      "Number of labels: 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_labels = len(tag2id)  # Use our existing tag2id mapping\n",
    "model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Set Up the Training Configuration\n",
    "\n",
    "#### A) Extracting Training Labels\n",
    "\n",
    "In token classification tasks, not every token is relevant for computing the loss (for instance, special tokens or padding tokens are ignored by marking them with ``-100``). \n",
    "\n",
    "This code iterates over the training DataLoader and collects only the relevant labels (i.e., those not marked with ``-100``). This step is necessary to accurately analyze the distribution of actual labels used in training.\n",
    "\n",
    "#### B) Computing Class Weights\n",
    "\n",
    "**Class imbalance** is a common issue in many classification tasks, and my Dataset has it also. When some classes are under-represented, the model might become biased towards the majority classes (and it became biased on the first runs).\n",
    "\n",
    "Using scikit-learn's ``compute_class_weight`` function helps to compute weights for each class inversely proportional to their frequency in the dataset. These weights can later be used during training (e.g., in the loss function) to give more importance to minority classes and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "# Set the number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "# Initialize an empty list to store all training labels (ignoring special tokens)\n",
    "train_labels = []\n",
    "\n",
    "# Loop over each batch in the training DataLoader\n",
    "for batch in train_loader:\n",
    "    # Get the labels tensor from the current batch\n",
    "    labels = batch['labels']\n",
    "    \n",
    "    # Create a mask to filter out tokens with the ignore index (-100)\n",
    "    mask = labels != -100\n",
    "    \n",
    "    # Extend the train_labels list with the valid labels (convert tensor to numpy array)\n",
    "    train_labels.extend(labels[mask].numpy())\n",
    "\n",
    "# Calculate class weights using scikit-learn's compute_class_weight\n",
    "# 'balanced' mode adjusts weights inversely proportional to class frequencies in the dataset.\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(train_labels),  # Unique classes present in the training labels\n",
    "    y=train_labels                    # List of training labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) Class Weights Conversion\n",
    "\n",
    "The ``computed class weights`` (from scikit-learn) are converted to a PyTorch tensor and moved to the appropriate device (GPU or CPU). \n",
    "\n",
    "This conversion is necessary because the **loss function will use these weights during training**, and they must be in the **same format and on the same device as the model's parameters**.\n",
    "\n",
    "#### D) Custom Loss Function\n",
    "\n",
    "A **weighted CrossEntropyLoss** is defined, using the computed class weights to counteract class imbalance. The ``ignore_index=-100`` parameter ensures that tokens marked with -100 (such as special tokens or padding) do not contribute to the loss, **preventing them from skewing the training**.\n",
    "\n",
    "#### E) Learning Rate Setup\n",
    "\n",
    "A small learning rate (1e-5) is set to allow for fine-tuning of the model. **Lower learning rates help in stabilizing the training process**, especially when **fine-tuning large pre-trained models**.\n",
    "\n",
    "#### F) Warmup Steps\n",
    "\n",
    "They are included to **gradually increase the learning rate** form a small value to the target learning rate. This is implemented for **stabilizing training** in the initial phase and prevents **sudden jumps in gradient updates**.\n",
    "\n",
    "#### G) Optimizer Configuration\n",
    "The AdamW optimizer is used with weight decay to **help regularize the model and prevent overfitting**. AdamW is commonly used in transformer-based models.\n",
    "\n",
    "#### H) Learning Rate Scheduler\n",
    "A linear scheduler with warmup is set up to adjust the learning rate throughout training. This scheduler **gradually increases the learning rate during the warmup phase and then linearly decreases it during the remainder of the training process.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the class weights computed by scikit-learn into a PyTorch FloatTensor\n",
    "# and move it to the same device as the model (GPU or CPU).\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# Define a weighted CrossEntropyLoss function.\n",
    "# The weight parameter applies the class weights to handle class imbalance.\n",
    "# ignore_index=-100 ensures that tokens marked as -100 (like padding) are ignored in loss computation.\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "\n",
    "# Set a small learning rate for better fine tuning of the pre-trained model.\n",
    "learning_rate = 1e-5  # Reduced from 2e-5 to 1e-5 for more gradual learning updates.\n",
    "\n",
    "# Calculate the number of warmup steps.\n",
    "# Here, warmup steps are set to 10% of the total training steps to stabilize the initial training.\n",
    "warmup_steps = int(0.1 * len(train_loader) * epochs)  # 10% of the total steps\n",
    "\n",
    "# Calculate the total number of training steps.\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Initialize the AdamW optimizer with model parameters, a low learning rate, and weight decay.\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Set up a linear learning rate scheduler with warmup.\n",
    "# This scheduler increases the learning rate for the warmup_steps, then linearly decays it.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps  # Total training steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I) Computing the metrics\n",
    "\n",
    "The ``compute_metrics`` function is crucial for evaluating model performance during training and validation.\n",
    "\n",
    "1. **Prediction Processing**\n",
    "\n",
    "    - The function starts by converting raw model outputs (logits) into predicted labels by using np.argmax along the classification dimension. \n",
    "    \n",
    "    - This produces the most likely class for each token.\n",
    "\n",
    "2. **Flattening and Filtering**\n",
    "    - Both predictions and true labels are flattened into 1D arrays.\n",
    "     \n",
    "    - Special tokens and padding tokens are marked with -100 in the labels; filtering these out ensures that only valid tokens are considered when calculating the metrics. \n",
    "     \n",
    "    - This step prevents skewing the evaluation metrics by including irrelevant tokens.\n",
    "\n",
    "3. **Metric Calculation**\n",
    "\n",
    "    - **Accuracy**: Measures the overall proportion of correctly predicted labels.\n",
    "\n",
    "    - **Precision, Recall, and F1 Score**: These metrics provide deeper insights into the performance, especially in imbalanced datasets. The weighted average ensures that the contribution of each class is proportional to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function for evaluating model performance\n",
    "def compute_metrics(preds, labels):\n",
    "    # Convert model outputs (logits) to predicted labels by selecting the class with the highest probability\n",
    "    preds = np.argmax(preds, axis=2).flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    # Create a mask to filter out tokens with the ignore index (-100), which are not used in training (e.g., padding)\n",
    "    mask = labels != -100\n",
    "    \n",
    "    # Apply the mask to both predictions and labels to keep only the valid tokens\n",
    "    preds = preds[mask]\n",
    "    labels = labels[mask]\n",
    "    \n",
    "    # Calculate accuracy: the fraction of correctly predicted labels\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score using weighted averages to account for class imbalances.\n",
    "    # The weighted average ensures that each class contributes to the overall metric proportionally to its frequency.\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Return a dictionary containing all computed metrics for easy access and logging.\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Training and Evaluation Functions\n",
    "\n",
    "The next function will implement the training for one epoch, performing the following steps:\n",
    "\n",
    "1. *Model training*: sets the model in training mode and iterates over training batches.\n",
    "\n",
    "2. *Forward Pass and Loss calculation*: processes input data, computes logits (these are the raw outputs of the last layer in a neural network, see [this link explanation](https://www.geeksforgeeks.org/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entropy-with-logits/), and applies a mask to exclude ignored tokens from loss calculation.\n",
    "\n",
    "3. *Backward pass and Optimization*: finally, computes the gradients, clips them for more stability, updates the model parameters with the defined optimizer and adjusts the learning rate using the scheduler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using personalized loss function for training one epoch\n",
    "def train_epoch():\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0  # Initialize the total loss accumulator\n",
    "    \n",
    "    # Iterate over training batches with a progress bar\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        \n",
    "        # Move inputs and labels to the designated device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass: obtain model outputs (logits)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Create a mask for valid tokens (ignore tokens with label -100)\n",
    "        active_loss = labels != -100\n",
    "        \n",
    "        # Reshape logits to match the expected loss function input\n",
    "        active_logits = logits.view(-1, num_labels)\n",
    "        \n",
    "        # Prepare active labels: use the ignore index where needed\n",
    "        active_labels = torch.where(\n",
    "            active_loss.view(-1), \n",
    "            labels.view(-1), \n",
    "            torch.tensor(loss_fn.ignore_index).type_as(labels)\n",
    "        )\n",
    "        \n",
    "        # Calculate the loss for the current batch\n",
    "        loss = loss_fn(active_logits, active_labels)\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Return the average loss for the epoch\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation \n",
    "\n",
    "The `evaluate` function switches the model to evaluation mode, processing the given data loader without updating the gradients. It also calculates the loss and collects predictions and true labels. Finally, it computes the predefined metrics with our `compute_metrics` function (accuracy, f1-score,etc) and then returns them along with the average loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0  # Initialize loss accumulator\n",
    "    all_preds = []  # To store predictions\n",
    "    all_labels = []  # To store true labels\n",
    "    \n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the data loader with a progress bar\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move inputs and labels to the device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Get model outputs including loss and logits\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            # Accumulate the loss from the current batch\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Detach logits and move them to CPU for metric calculation\n",
    "            logits = outputs.logits\n",
    "            all_preds.append(logits.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batch predictions and labels into single arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    # Compute evaluation metrics (accuracy, precision, recall, F1)\n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    # Add the average loss to the metrics dictionary\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training loop is designed to iterate over multiple epochs to train the model and evaluate it on a validation set. The key steps are:\n",
    "\n",
    "#### I) Epoch Iteration\n",
    "The loop runs for a specified number of epochs. For each epoch, it prints the progress, trains the model with ``train_epoch()``, and evaluates it on the validation set with ``evaluate(val_loader)``.\n",
    "\n",
    "#### II) Metric Logging\n",
    "After each epoch, the training loss and validation metrics (including loss, accuracy, precision, recall, and F1 score) are printed for monitoring performance.\n",
    "\n",
    "#### III) Best Model Saving\n",
    "If the F1 score improves over the best observed so far, the modelâs state is saved. This helps to keep the best performing model checkpoint.\n",
    "\n",
    "#### IV) Commenting Out: Disclaimer.\n",
    "The entire block is commented out to prevent accidental execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following training loop is commented out to prevent accidental execution.\n",
    "# Uncomment it when you are ready to run the training process.\n",
    "\n",
    "# best_f1 = 0  # Initialize the best F1 score for saving the best model\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "    \n",
    "#     # Train the model for one epoch and print the training loss\n",
    "#     train_loss = train_epoch()\n",
    "#     print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "#     # Evaluate the model on the validation set and print the validation metrics\n",
    "#     val_metrics = evaluate(val_loader)\n",
    "#     print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "#     print(f\"Validation metrics: accuracy={val_metrics['accuracy']:.4f}, precision={val_metrics['precision']:.4f}, \"\n",
    "#           f\"recall={val_metrics['recall']:.4f}, f1={val_metrics['f1']:.4f}\")\n",
    "    \n",
    "#     # Save the model if it has the best F1 score so far\n",
    "#     if val_metrics['f1'] > best_f1:\n",
    "#         best_f1 = val_metrics['f1']\n",
    "#         torch.save(model.state_dict(), 'best_deberta_ner_model.pt')\n",
    "#         print(f\"New best model saved with F1: {best_f1:.4f}\")\n",
    "\n",
    "# print(\"\\nTraining complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code evaluates the model on the validation set using scikit-learn's classification report. It performs the following steps:\n",
    "\n",
    "#### I) General Metrics Calculation:\n",
    "\n",
    "The ``evaluate ``function is used to calculate overall metrics such as loss, accuracy, precision, recall, and F1 score on the validation set. This centralizes metric calculations, reducing code redundancy.\n",
    "\n",
    "#### II) Detailed Classification Report:\n",
    "Even though general metrics are computed, a detailed classification report is also generated. This involves iterating through the validation DataLoader, gathering predictions and true labels, filtering out tokens marked as ``-100``, converting numerical labels to their corresponding tag names using the ``id2tag mapping``, and then printing the report with ``scikit-learnâs classification_report``.\n",
    "\n",
    "#### III) Commented Out Code:\n",
    "The entire cell is commented out to prevent accidental execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "              B-ACTION       0.00      0.00      0.00        21\n",
      "            B-INCIDENT       0.00      0.50      0.01         2\n",
      "            B-PIT_CALL       0.00      0.00      0.00         1\n",
      "     B-POSITION_CHANGE       0.03      0.03      0.03        29\n",
      "           B-SITUATION       0.00      0.00      0.00        41\n",
      "B-STRATEGY_INSTRUCTION       0.18      0.05      0.07        43\n",
      "     B-TECHNICAL_ISSUE       0.00      0.00      0.00        18\n",
      "     B-TRACK_CONDITION       0.00      0.00      0.00         3\n",
      "             B-WEATHER       0.00      0.00      0.00        13\n",
      "              I-ACTION       0.11      0.26      0.16       103\n",
      "            I-INCIDENT       0.00      0.00      0.00         7\n",
      "            I-PIT_CALL       0.00      0.00      0.00         3\n",
      "     I-POSITION_CHANGE       0.09      0.20      0.13        60\n",
      "           I-SITUATION       0.15      0.06      0.09       140\n",
      "I-STRATEGY_INSTRUCTION       0.21      0.03      0.04       120\n",
      "     I-TECHNICAL_ISSUE       0.02      0.03      0.02        38\n",
      "     I-TRACK_CONDITION       0.00      0.00      0.00        13\n",
      "             I-WEATHER       0.00      0.00      0.00        64\n",
      "                     O       0.60      0.01      0.02       339\n",
      "\n",
      "              accuracy                           0.06      1058\n",
      "             macro avg       0.07      0.06      0.03      1058\n",
      "          weighted avg       0.26      0.06      0.05      1058\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# The following evaluation cell is commented out to prevent accidental execution.\n",
    "# Uncomment the code when you are ready to perform the evaluation.\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # First, evaluate the model using our centralized evaluate function to get general metrics.\n",
    "# val_metrics = evaluate(val_loader)\n",
    "# print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "# print(f\"Validation metrics: accuracy={val_metrics['accuracy']:.4f}, precision={val_metrics['precision']:.4f}, \"\n",
    "#       f\"recall={val_metrics['recall']:.4f}, f1={val_metrics['f1']:.4f}\")\n",
    "\n",
    "# # Now, generate a detailed classification report for a finer analysis of each tag.\n",
    "# model.eval()  # Ensure the model is in evaluation mode to disable dropout, etc.\n",
    "# all_preds = []  # List to collect all prediction indices from the validation set.\n",
    "# all_labels = []  # List to collect all ground truth label indices.\n",
    "\n",
    "# # Disable gradient calculation for evaluation efficiency.\n",
    "# with torch.no_grad():\n",
    "#     # Iterate over each batch in the validation DataLoader.\n",
    "#     for batch in val_loader:\n",
    "#         # Move the input tensors and labels to the designated device (GPU or CPU).\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "        \n",
    "#         # Forward pass: obtain the model's output logits.\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         # Compute the predictions by taking the argmax over the logits for each token.\n",
    "#         preds = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "#         # Create a mask to filter out tokens with an ignore label (-100).\n",
    "#         active_mask = labels != -100\n",
    "#         # Extract the true labels and predictions from positions that are not ignored.\n",
    "#         true = labels[active_mask].cpu().numpy()\n",
    "#         pred = preds[active_mask].cpu().numpy()\n",
    "        \n",
    "#         # Append the results for the current batch to our overall lists.\n",
    "#         all_labels.extend(true)\n",
    "#         all_preds.extend(pred)\n",
    "\n",
    "# # Convert numerical indices into their corresponding tag names using the id2tag mapping.\n",
    "# true_tags = [id2tag[l] for l in all_labels]\n",
    "# pred_tags = [id2tag[p] for p in all_preds]\n",
    "\n",
    "# # Finally, print a detailed classification report including precision, recall, and F1 score for each tag.\n",
    "# print(classification_report(true_tags, pred_tags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.1 Test Set Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell evaluates the model on the test set and prints key performance metrics, including loss, accuracy, precision, recall, and F1-score. Additionally, to avoid losing results after execution, we format the classification report into a table.\n",
    "\n",
    "### Steps:\n",
    "#### General Test Set Evaluation:\n",
    "\n",
    "- Calls evaluate(test_loader) to obtain overall metrics.\n",
    "\n",
    "- Prints loss, accuracy, precision, recall, and F1-score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following test evaluation cell is commented out to prevent accidental execution.\n",
    "# Uncomment if you wish to evaluate the model on the test set.\n",
    "\n",
    "# print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "# # Compute general test set metrics using the evaluate function\n",
    "# test_metrics = evaluate(test_loader)\n",
    "# print(f\"Test loss: {test_metrics['loss']:.4f}\")\n",
    "# print(f\"Test metrics: accuracy={test_metrics['accuracy']:.4f}, precision={test_metrics['precision']:.4f}, \"\n",
    "#       f\"recall={test_metrics['recall']:.4f}, f1={test_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Next Steps \n",
    "\n",
    "Some metrics are quite challenging, with low values, and a global f1-score of 0.41. Therefore, I believe it is a good idea to **try other models** that can be more easy to adjust. As I mentioned early, I will train a **specific BERT model** made for NER, improve it, and comment the results before choosing the best model.\n",
    "\n",
    "| Entity                    | Precision | Recall | F1-Score | Support |\n",
    "|---------------------------|-----------|--------|----------|---------|\n",
    "| B-ACTION                 | 0.00      | 0.00   | 0.00     | 21      |\n",
    "| B-INCIDENT               | 0.00      | 0.50   | 0.01     | 2       |\n",
    "| B-PIT_CALL               | 0.00      | 0.00   | 0.00     | 1       |\n",
    "| B-POSITION_CHANGE        | 0.03      | 0.03   | 0.03     | 29      |\n",
    "| B-SITUATION              | 0.00      | 0.00   | 0.00     | 41      |\n",
    "| B-STRATEGY_INSTRUCTION   | 0.18      | 0.05   | 0.07     | 43      |\n",
    "| B-TECHNICAL_ISSUE        | 0.00      | 0.00   | 0.00     | 18      |\n",
    "| B-TRACK_CONDITION        | 0.00      | 0.00   | 0.00     | 3       |\n",
    "| B-WEATHER               | 0.00      | 0.00   | 0.00     | 13      |\n",
    "| I-ACTION                 | 0.11      | 0.26   | 0.16     | 103     |\n",
    "| I-INCIDENT               | 0.00      | 0.00   | 0.00     | 7       |\n",
    "| I-PIT_CALL               | 0.00      | 0.00   | 0.00     | 3       |\n",
    "| I-POSITION_CHANGE        | 0.09      | 0.20   | 0.13     | 60      |\n",
    "| I-SITUATION              | 0.15      | 0.06   | 0.09     | 140     |\n",
    "| I-STRATEGY_INSTRUCTION   | 0.21      | 0.03   | 0.04     | 120     |\n",
    "| I-TECHNICAL_ISSUE        | 0.02      | 0.03   | 0.02     | 38      |\n",
    "| I-TRACK_CONDITION        | 0.00      | 0.00   | 0.00     | 13      |\n",
    "| I-WEATHER               | 0.00      | 0.00   | 0.00     | 64      |\n",
    "| O                        | 0.60      | 0.01   | 0.02     | 339     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transitioning to Pre-trained BERT for NER\n",
    "\n",
    "## Futher explanations of why I am switching to BERT\n",
    "After experimenting with the DeBERTa v3 Large model, IÂ´m  now transitioning to a BERT-based approach using ``dbmdz/bert-large-cased-finetuned-conll03-english``. This strategic shift is motivated by several factors:\n",
    "\n",
    "### 1. Pre-trained NER Capabilites\n",
    "\n",
    "As I mentioned earlier, this BERT model has already been fine-tuned specifically for Named Entity Recognition on the CoNLL-03 dataset, providing a strong foundation for our F1 domain adaptation.\n",
    "\n",
    "### 2. Transfer Learning Advantage\n",
    "y leveraging a model already optimized for entity detection, we can potentially achieve faster convergence and better performance on our specialized F1 entities. Deberta is a model that can be used for another NLP task (as we saw in previous notebooks, for sentiment analysis or intent classification), so this model theoretically looks better.\n",
    "\n",
    "\n",
    "### 3. Model Comparison\n",
    "\n",
    "his allows us to benchmark performance between different transformer architectures (DeBERTa vs. BERT) to determine the most effective approach for our specific use case.\n",
    "\n",
    "### 4. Performance on challenging entities.\n",
    "\n",
    "My initial results with DeBERTa showed challenges in detecting certain entity types (STRATEGY_INSTRUCTION, TRACK_CONDITION). BERT's different attention mechanism and pre-training might help address these issues.\n",
    "\n",
    "## What IÂ´ll implement\n",
    "\n",
    "In the following sections, I will try to:\n",
    "\n",
    "1. **Initialize the BERT tokenizer and model** pre-trained on general NER tasks.\n",
    "\n",
    "2. **Develop a custom dataset class** optimized for BERT's tokenization approach. That is, our current class but slightly changed for BERTÂ´s architecture instead of DebertaÂ´s. \n",
    "\n",
    "3. **Implement specialized loss functions** (Focal Loss and Weighted Cross-Entropy) to address class imbalance.\n",
    "4. **Fine-tune the model with focus** on the most challenging entity categories.\n",
    "\n",
    "5. **Evaluate performance**with detailed per-entity metrics and classification reports.\n",
    "\n",
    "6. **Perform targeted analysis** of our most difficult entity classes.\n",
    "\n",
    "## Goal\n",
    "\n",
    "My goal is to create a more robust entity recognition system that can reliably extract strategic information from F1 radio communications, with particular emphasis on improving detection of critical race strategy elements that were challenging for our previous model.\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "The following code can result repetitive in some parts. However, it is necessary to redefine a great part of my old code, so I decided to implement it here to keep the old results and workflow. As some parts are almost the same, **shallower explanations will be made in those parts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: BertTokenizerFast\n",
      "Vocabulary size: 28996\n"
     ]
    }
   ],
   "source": [
    "# Again, we add a manual seed to always initialize the bert-large in the same seed. \n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the model name for the pretrained BERT tokenizer\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Check if it loaded correctly\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")  # Display tokenizer class name\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")  # Display vocabulary size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: dbmdz/bert-large-cased-finetuned-conll03-english\n",
      "Number of labels: 19\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Bert Large Model Initialization\n",
    "# ===========================\n",
    "\n",
    "# Determine the computation device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # Print the selected device\n",
    "\n",
    "# Get the number of labels based on the tag-to-ID mapping\n",
    "num_labels = len(tag2id)\n",
    "\n",
    "# Load the pretrained BERT model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_name,  # Pretrained BERT model name\n",
    "    num_labels=num_labels,  # Number of classification labels\n",
    "    id2label={i: l for l, i in tag2id.items()},  # Map IDs to labels (for interpretability)\n",
    "    label2id=tag2id,  # Map labels to IDs (for training and inference)\n",
    "    ignore_mismatched_sizes=True  # Allows model loading even if the classifier head size differs\n",
    ")\n",
    "\n",
    "# Move the model to the selected device (GPU/CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Print model information\n",
    "print(f\"Model loaded: {model_name}\")  # Confirm the model has been loaded\n",
    "print(f\"Number of labels: {num_labels}\")  # Display the number of classification labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Differences in `F1RadioNERDataset` (BERT) Compared to DeBERTa Version  \n",
    "\n",
    "This dataset class is designed for Named Entity Recognition (NER) in **Formula 1 Radio Messages**, but with adaptations for **BERT models** instead of DeBERTa. Below are the key differences:\n",
    "\n",
    "####  Tokenization Strategy  \n",
    "\n",
    "- In **DeBERTa**, we used `tokenizer.encode_plus()`, while here we use:  \n",
    "\n",
    "  ```python\n",
    "  tokenized_inputs = self.tokenizer(\n",
    "      words,\n",
    "      is_split_into_words=True,\n",
    "      max_length=self.max_len,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_tensors=\"pt\"\n",
    "  )\n",
    "- *Why?*\n",
    "    - ``is_split_into_words=True`` ensures that each token remains aligned with its original word, which is essential for NER.\n",
    "\n",
    "    - We specify ``return_tensors=\"pt\"`` to return PyTorch tensors directly (DeBERTa used NumPy arrays initially).\n",
    "\n",
    "#### Handling Labels (NER Tags)\n",
    "\n",
    "In DeBERTa, label handling was different.\n",
    "\n",
    "Here, we explicitly check if tags are in string format and map them to tag2id manually:\n",
    "\n",
    "```python\n",
    "tag_ids = []\n",
    "for tag in tags:\n",
    "    if isinstance(tag, str):\n",
    "        tag_ids.append(self.tag2id[tag])\n",
    "    else:\n",
    "        tag_ids.append(tag)\n",
    "```\n",
    "- *Why?*\n",
    "\n",
    "    - Some datasets store NER tags as strings, while others already have integer IDs.\n",
    "\n",
    "    - This ensures consistency across different data formats.\n",
    "\n",
    "#### 3ï¸. Word-to-Token Alignment  \n",
    "In both implementations, we align tokenized words with their corresponding labels.  \n",
    "\n",
    "However, in **BERT**, we explicitly use `word_ids(batch_index=0)` to retrieve word-level alignment:  \n",
    "\n",
    "```python\n",
    "word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "```\n",
    "- *Why?*\n",
    "\n",
    "    - ``word_ids`` maps tokens back to their original words, allowing us to correctly assign NER labels.\n",
    "\n",
    "    - This is crucial for handling subwords in BERTâs WordPiece tokenization.\n",
    "\n",
    "#### 4. Handling Subwords in Label Assignment\n",
    "\n",
    "In **DeBERTa, we only assigned labels** to the first subword, marking others as -100.\n",
    "\n",
    "In **BERT**, we allow two options:\n",
    "\n",
    "- Option 1: Assign -100 to subwords (ignore during training).\n",
    "\n",
    "- Option 2 (used): Assign the same label to all subwords (propagate labels).\n",
    "\n",
    "- *Why?*\n",
    "\n",
    "    - Some models perform better if all subwords share the same NER tag, as BERT does (or I found it does during training).\n",
    "\n",
    "    - Others prefer ignoring subwords (setting them to -100).\n",
    "\n",
    "--- \n",
    "\n",
    "#### Summary of Key Changes  \n",
    "\n",
    "| Feature           | DeBERTa Implementation         | BERT Implementation                        |\n",
    "|-------------------|--------------------------------|--------------------------------------------|\n",
    "| **Tokenization**  | `encode_plus()`               | `tokenizer(..., is_split_into_words=True)` |\n",
    "| **Label Handling** | Directly used dataset labels  | Explicit conversion from string to ID      |\n",
    "| **Word Alignment** | Implicit handling             | Uses `word_ids(batch_index=0)`             |\n",
    "| **Subword Labels** | Only first subword labeled    | Option to label all subwords               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1RadioNERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, tag2id, max_len=128):\n",
    "        # Initialize dataset with Hugging Face dataset, tokenizer, and tag2id mappings\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a sample from the dataset\n",
    "        item = self.dataset[idx]\n",
    "        words = item[\"tokens\"]  # Tokens from the dataset\n",
    "        tags = item[\"ner_tags\"]  # Corresponding NER tags\n",
    "        \n",
    "        # Convert tags from string to ID if necessary\n",
    "        tag_ids = []  # List to store numerical tag ids\n",
    "        for tag in tags:\n",
    "            if isinstance(tag, str):  # If the tag is a string, map it to its ID\n",
    "                tag_ids.append(self.tag2id[tag])\n",
    "            else:  # If the tag is already in ID form, use it directly\n",
    "                tag_ids.append(tag)\n",
    "        \n",
    "        # Tokenize the text and align the labels\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            words,  # Tokenized words\n",
    "            is_split_into_words=True,  # Ensure tokenization is done word by word\n",
    "            max_length=self.max_len,  # Maximum sequence length\n",
    "            padding=\"max_length\",  # Padding the sequences to the max length\n",
    "            truncation=True,  # Truncate sequences that exceed max_length\n",
    "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        # Initialize labels with -100 for padding tokens\n",
    "        labels = torch.ones(self.max_len, dtype=torch.long) * -100\n",
    "        \n",
    "        # Get word_ids to align the labels with words\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "        \n",
    "        # Assign labels to non-special tokens (word pieces)\n",
    "        previous_word_idx = None  # To keep track of the previous word index\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if word_idx is not None:  # If the token corresponds to a word\n",
    "                if word_idx < len(tag_ids):  # Check if the word index is valid\n",
    "                    # If it's the first subword, assign the label\n",
    "                    # If it's not (continuation of a word), assign -100 or the same label as you prefer\n",
    "                    if word_idx != previous_word_idx:  # New word\n",
    "                        labels[i] = tag_ids[word_idx]\n",
    "                    else:  # Continuation of the word (subword)\n",
    "                        # Option 1: Use -100 for continuations (ignore them)\n",
    "                        # labels[i] = -100\n",
    "                        # Option 2: Use the same label for all subwords of the word\n",
    "                        labels[i] = tag_ids[word_idx]\n",
    "            previous_word_idx = word_idx  # Update the previous word index\n",
    "        \n",
    "        # Return the tokenized inputs and labels\n",
    "        return {\n",
    "            \"input_ids\": tokenized_inputs[\"input_ids\"].flatten(),  # Flattened input ids\n",
    "            \"attention_mask\": tokenized_inputs[\"attention_mask\"].flatten(),  # Flattened attention mask\n",
    "            \"labels\": labels  # The aligned labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Use Focal Loss?\n",
    "Focal Loss helps to reduce the impact of easy-to-classify examples (which would otherwise dominate the loss function) and places more emphasis on harder-to-classify or misclassified examples. This is particularly useful in tasks with imbalanced class distributions (e.g., in named entity recognition, some classes might be underrepresented, like our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Focal Loss for token classification tasks\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, gamma=2.0):\n",
    "        \"\"\"\n",
    "        Focal Loss is designed to address the class imbalance problem by focusing more on hard-to-classify examples.\n",
    "        \n",
    "        Args:\n",
    "            weight (Tensor, optional): Class weights for addressing class imbalance.\n",
    "            gamma (float, optional): The focusing parameter, typically set to 2.0 to reduce the relative loss for well-classified examples.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weight = weight  # Weight for class imbalance, can be set to None or a tensor of shape [num_classes]\n",
    "        self.gamma = gamma  # Focusing parameter, controls the rate at which easy examples are down-weighted\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Forward pass for calculating Focal Loss.\n",
    "        \n",
    "        Args:\n",
    "            input (Tensor): Predicted logits from the model, shape [batch_size, seq_len, num_classes].\n",
    "            target (Tensor): True labels, shape [batch_size, seq_len].\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Computed focal loss value.\n",
    "        \"\"\"\n",
    "        # Adjust dimensions for token classification (batch_size, seq_len, num_classes)\n",
    "        if input.dim() > 2:\n",
    "            # Reshape input to (batch_size*seq_len, num_classes) to flatten sequence dimension\n",
    "            input = input.view(-1, input.size(-1))\n",
    "        \n",
    "        if target.dim() > 1:\n",
    "            # Flatten target to (batch_size*seq_len,) to match the input\n",
    "            target = target.view(-1)\n",
    "        \n",
    "        # Calculate Cross-Entropy Loss without reduction for each token in the sequence\n",
    "        ce_loss = F.cross_entropy(input, target, weight=self.weight, ignore_index=-100, reduction='none')\n",
    "        \n",
    "        # Calculate the probability (pt) for each class\n",
    "        pt = torch.exp(-ce_loss)  # pt is the probability that the model assigned to the correct class\n",
    "        \n",
    "        # Compute the focal loss\n",
    "        # The term (1 - pt) ** gamma reduces the contribution from easy examples\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        # Return the mean of the focal loss\n",
    "        return focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping the same Tarining Config, but now using the new loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "# Initialize an empty list to store labels from the training dataset\n",
    "train_labels = []\n",
    "for batch in train_loader:\n",
    "    labels = batch['labels']\n",
    "    \n",
    "    # Create a mask to filter out ignored tokens (-100) since they should not contribute to class weighting\n",
    "    mask = labels != -100  \n",
    "    train_labels.extend(labels[mask].numpy())  # Convert and store valid labels\n",
    "\n",
    "# Compute class weights to handle class imbalance in the dataset\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',  # This ensures that weights are inversely proportional to class frequency\n",
    "    classes=np.unique(train_labels),  # Extract unique class labels\n",
    "    y=train_labels  # Use all collected labels from the dataset\n",
    ")\n",
    "\n",
    "# Convert class weights to a PyTorch tensor and move them to the appropriate device (CPU/GPU)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# Define the loss function (CrossEntropyLoss) with class weights to give more importance to underrepresented classes\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)  \n",
    "# ignore_index=-100 ensures that padding tokens are not considered in loss computation\n",
    "\n",
    "# Set the learning rate; it was previously 2e-5 but has been slightly increased to 3e-5\n",
    "learning_rate = 3e-5\n",
    "\n",
    "# Define warmup steps: A small portion of the training steps is dedicated to gradually increasing the learning rate\n",
    "warmup_steps = int(0.05 * len(train_loader) * epochs)  # 5% of total training steps\n",
    "\n",
    "# Calculate the total number of training steps (number of batches per epoch * number of epochs)\n",
    "total_steps = len(train_loader) * epochs  \n",
    "\n",
    "# Define the AdamW optimizer, which helps prevent overfitting by applying weight decay\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.03)  \n",
    "# weight_decay=0.03 applies L2 regularization to reduce overfitting\n",
    "\n",
    "# Define a learning rate scheduler to linearly decrease the learning rate after warmup\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps,  # Gradual warmup phase\n",
    "    num_training_steps=total_steps  # Decay learning rate linearly after warmup\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Training loop. Commented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the best F1-score to track improvements across epochs\n",
    "# best_f1 = 0  \n",
    "\n",
    "# # Loop through the training process for the specified number of epochs\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}\")  # Display current epoch\n",
    "#     print(f\"{'='*50}\")\n",
    "    \n",
    "#     # Train the model for one epoch and obtain the training loss\n",
    "#     train_loss = train_epoch()  # The train_epoch() function is assumed to be already defined\n",
    "#     print(f\"Training loss: {train_loss:.4f}\")  # Print the training loss for monitoring\n",
    "    \n",
    "#     # Evaluate the model on the validation dataset\n",
    "#     val_metrics = evaluate(val_loader)  # The evaluate() function is assumed to be already defined\n",
    "#     print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "#     print(f\"Validation metrics: accuracy={val_metrics['accuracy']:.4f}, \"\n",
    "#           f\"precision={val_metrics['precision']:.4f}, recall={val_metrics['recall']:.4f}, \"\n",
    "#           f\"f1={val_metrics['f1']:.4f}\")  # Print validation performance metrics\n",
    "\n",
    "#     # Save the model if it achieves the best F1-score so far\n",
    "#     if val_metrics['f1'] > best_f1:\n",
    "#         best_f1 = val_metrics['f1']  # Update the best F1-score\n",
    "#         torch.save(model.state_dict(), 'best_bert_large_ner_model.pt')  # Save model weights\n",
    "#         print(f\"New best model saved with F1: {best_f1:.4f}\")  # Notify about model update\n",
    "\n",
    "# print(\"\\nTraining complete!\")  # Training process finished\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following evaluation cell is commented out to prevent accidental execution.\n",
    "# Uncomment the code when you are ready to perform the evaluation.\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # First, evaluate the model using our centralized evaluate function to get general metrics.\n",
    "# val_metrics = evaluate(val_loader)\n",
    "# print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "# print(f\"Validation metrics: accuracy={val_metrics['accuracy']:.4f}, precision={val_metrics['precision']:.4f}, \"\n",
    "#       f\"recall={val_metrics['recall']:.4f}, f1={val_metrics['f1']:.4f}\")\n",
    "\n",
    "# # Now, generate a detailed classification report for a finer analysis of each tag.\n",
    "# model.eval()  # Ensure the model is in evaluation mode to disable dropout, etc.\n",
    "# all_preds = []  # List to collect all prediction indices from the validation set.\n",
    "# all_labels = []  # List to collect all ground truth label indices.\n",
    "\n",
    "# # Disable gradient calculation for evaluation efficiency.\n",
    "# with torch.no_grad():\n",
    "#     # Iterate over each batch in the validation DataLoader.\n",
    "#     for batch in val_loader:\n",
    "#         # Move the input tensors and labels to the designated device (GPU or CPU).\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "        \n",
    "#         # Forward pass: obtain the model's output logits.\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         # Compute the predictions by taking the argmax over the logits for each token.\n",
    "#         preds = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "#         # Create a mask to filter out tokens with an ignore label (-100).\n",
    "#         active_mask = labels != -100\n",
    "#         # Extract the true labels and predictions from positions that are not ignored.\n",
    "#         true = labels[active_mask].cpu().numpy()\n",
    "#         pred = preds[active_mask].cpu().numpy()\n",
    "        \n",
    "#         # Append the results for the current batch to our overall lists.\n",
    "#         all_labels.extend(true)\n",
    "#         all_preds.extend(pred)\n",
    "\n",
    "# # Convert numerical indices into their corresponding tag names using the id2tag mapping.\n",
    "# true_tags = [id2tag[l] for l in all_labels]\n",
    "# pred_tags = [id2tag[p] for p in all_preds]\n",
    "\n",
    "# # Finally, print a detailed classification report including precision, recall, and F1 score for each tag.\n",
    "# print(classification_report(true_tags, pred_tags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focused Fine-Tuning for Challenging Entity Classes\n",
    "\n",
    "### Why We Need Additional Fine-Tuning\n",
    "\n",
    "After our initial training with the BERT model, we observed that certain entity classes remain particularly challenging to detect:\n",
    "\n",
    "1. **STRATEGY_INSTRUCTION**: Critical race strategy directives are being missed\n",
    "2. **TRACK_CONDITION**: Information about track surface status shows poor recall\n",
    "3. **TECHNICAL_ISSUE**: Mechanical problems are often misclassified\n",
    "4. **INCIDENT**: Racing incidents are inconsistently detected\n",
    "\n",
    "The standard training approach treats all entity classes equally, but our F1 radio domain has natural class imbalances. More importantly, some entity types (like strategy instructions) carry higher strategic value than others, making their accurate detection a priority.\n",
    "\n",
    "### Our Fine-Tuning Approach\n",
    "\n",
    "We'll implement a focused fine-tuning strategy with these key elements:\n",
    "\n",
    "1. **Class-Weighted Loss Function**: We're creating a custom `WeightedCrossEntropyLoss` that assigns higher importance (5x weight) to our target classes, particularly STRATEGY_INSTRUCTION and TRACK_CONDITION\n",
    "   \n",
    "2. **Lower Learning Rate**: Reducing from 3e-5 to 2e-6 to make smaller, more precise adjustments to the model\n",
    "\n",
    "3. **Short Training Cycle**: Using just 5 epochs to avoid overfitting while refining detection capabilities\n",
    "\n",
    "4. **Targeted Evaluation**: Specifically measuring improvements on our challenging entity classes\n",
    "\n",
    "This approach is similar to specialized medical image detection systems that prioritize detecting rare but critical conditions. By deliberately overweighting certain classes, we guide the model to be more sensitive to these important but challenging categories.\n",
    "\n",
    "### Expected Benefits\n",
    "\n",
    "This fine-tuning strategy should:\n",
    "\n",
    "1. Increase recall for strategic instructions and track conditions\n",
    "2. Maintain performance on well-detected entity classes\n",
    "3. Improve overall F1 score by addressing the weakest areas\n",
    "4. Create a more balanced model suitable for real-world F1 strategy applications\n",
    "\n",
    "The final evaluation will include detailed per-class metrics to verify if our targeted approach successfully improved detection of these critical racing information categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. First, load the saved model that we have already trained\n",
    "model_path = '../../outputs/week4/models/best_bert_large_ner_model.pt'  \n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "    num_labels=len(tag2id),\n",
    "    id2label={i: l for l, i in tag2id.items()},\n",
    "    label2id=tag2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_17800\\3129886198.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "print(\"Pre-trained model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation: WeightedCrossEntropyLoss\n",
    "\n",
    "The `WeightedCrossEntropyLoss` class implements a variant of cross-entropy loss with custom weights for specific classes in a sequence classification problem.\n",
    "\n",
    "### ð¹ Key Features:\n",
    "\n",
    "1. **Adjustable class weights:**\n",
    "   * Higher weights are assigned to specific classes (`STRATEGY_INSTRUCTION`, `TRACK_CONDITION`).\n",
    "   * Moderate weights are assigned to others (`TECHNICAL_ISSUE`, `INCIDENT`).\n",
    "   * All other classes keep the default weight of `1.0`.\n",
    "\n",
    "2. **Support for original and new target classes:**\n",
    "   * If `target_classes` is provided, it adjusts weights based on class relevance.\n",
    "\n",
    "3. **PyTorch Compatibility:**\n",
    "   * Uses `F.cross_entropy` with per-class weights (`self.class_weights`).\n",
    "   * Ignores `-100` indices, typically used for padding in NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, num_labels, target_classes=None, class_weight_factor=5.0):\n",
    "        \"\"\"\n",
    "        Implements Cross Entropy Loss with custom class weights.\n",
    "\n",
    "        Parameters:\n",
    "        - num_labels (int): Total number of labels in the classification task.\n",
    "        - target_classes (list, optional): Indices of target classes to assign custom weights.\n",
    "        - class_weight_factor (float, optional): Base weight for specific classes (default is 5.0).\n",
    "        \"\"\"\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "\n",
    "        # Initialize all class weights to 1.0\n",
    "        self.class_weights = torch.ones(num_labels, dtype=torch.float)\n",
    "        \n",
    "        # Identify original target classes\n",
    "        original_targets = []\n",
    "        for tag, idx in tag2id.items():\n",
    "            if \"STRATEGY_INSTRUCTION\" in tag or \"TRACK_CONDITION\" in tag:\n",
    "                original_targets.append(idx)\n",
    "        \n",
    "        # Assign weights to relevant classes\n",
    "        if target_classes:\n",
    "            for cls_idx in target_classes:\n",
    "                tag = id2tag[cls_idx]  # Convert index to label name\n",
    "                \n",
    "                if cls_idx in original_targets:\n",
    "                    # Keep a high weight for original target classes (default 5.0)\n",
    "                    self.class_weights[cls_idx] = class_weight_factor  \n",
    "                elif \"TECHNICAL_ISSUE\" in tag or \"INCIDENT\" in tag:\n",
    "                    # Assign a moderate weight (3.0) to new categories related to technical issues or incidents\n",
    "                    self.class_weights[cls_idx] = 3.0  \n",
    "\n",
    "        # Define the ignore index for padding tokens\n",
    "        self.ignore_index = -100\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Computes weighted cross-entropy loss.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (tensor): Model output with shape (batch_size, seq_len, num_labels).\n",
    "        - labels (tensor): Ground truth labels with shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "        - A scalar tensor representing the average loss.\n",
    "        \"\"\"\n",
    "        # Move class weights to the same device as logits\n",
    "        self.class_weights = self.class_weights.to(logits.device)\n",
    "        \n",
    "        # Apply cross-entropy loss with class weights\n",
    "        return F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),  # Reshape for correct loss computation\n",
    "            labels.view(-1),  # Ensure labels match expected format\n",
    "            weight=self.class_weights,  # Apply custom class weights\n",
    "            ignore_index=self.ignore_index  # Ignore padding tokens (-100)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Class Identification and Loss Function Configuration\n",
    "\n",
    "This code snippet:\n",
    "\n",
    "1. **Identifies specific target classes** in `tag2id` that contain `\"STRATEGY_INSTRUCTION\"` or `\"TRACK_CONDITION\"`.\n",
    "2. **Prints their corresponding indices** for debugging purposes.\n",
    "3. **Creates a `WeightedCrossEntropyLoss` instance** using these target classes and assigns them a higher weight (Ã5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target class: B-STRATEGY_INSTRUCTION (ID: 5)\n",
      "Target class: B-TRACK_CONDITION (ID: 7)\n",
      "Target class: I-STRATEGY_INSTRUCTION (ID: 14)\n",
      "Target class: I-TRACK_CONDITION (ID: 16)\n"
     ]
    }
   ],
   "source": [
    "# Identify indices of problematic classes\n",
    "target_class_indices = []\n",
    "for tag, idx in tag2id.items():\n",
    "    if \"STRATEGY_INSTRUCTION\" in tag or \"TRACK_CONDITION\" in tag:\n",
    "        target_class_indices.append(idx)\n",
    "        print(f\"Target class: {tag} (ID: {idx})\")  # Debugging output to verify selected classes\n",
    "\n",
    "# Create custom loss function with increased weight for target classes\n",
    "custom_loss = WeightedCrossEntropyLoss(\n",
    "    num_labels=len(tag2id),  # Total number of labels in the classification task\n",
    "    target_classes=target_class_indices,  # Indices of classes that need higher weighting\n",
    "    class_weight_factor=5.0  # Increase weight by 5x for the selected target classes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation: Modified Training Function with Custom Loss\n",
    "\n",
    "This function, `train_epoch_focused()`, modifies the standard training loop by incorporating the **custom weighted loss function** (`custom_loss`). The key changes include:\n",
    "\n",
    "1. **Using the** `WeightedCrossEntropyLoss` function to handle class imbalance.\n",
    "2. **Computing loss dynamically** based on `logits` and `labels`.\n",
    "3. **Gradient clipping** to prevent exploding gradients.\n",
    "4. **Updating both the optimizer and learning rate scheduler** after each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Modified training function to use custom loss\n",
    "def train_epoch_focused():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Normal forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Compute loss using custom function\n",
    "        loss = custom_loss(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Set a low learning rate for fine-tuning\n",
    "learning_rate = 2e-6  # Lower for fine-tuning\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Short cycle for fine-tuning\n",
    "fine_tuning_epochs = 5\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(0.1 * len(train_loader) * fine_tuning_epochs),\n",
    "    num_training_steps=len(train_loader) * fine_tuning_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data_loader):\n",
    "    \"\"\"Evaluates the model on the given data loader and computes relevant metrics.\"\"\"\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "    total_loss = 0  # Initialize the total loss accumulator\n",
    "    all_preds = []  # List to store all model predictions\n",
    "    all_labels = []  # List to store all ground-truth labels\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation to improve efficiency\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):  # Iterate over the data loader\n",
    "            # Move input tensors to the specified device (CPU or GPU)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass through the model to obtain logits (raw predictions)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits  # Extract logits from the model output\n",
    "            \n",
    "            # Compute loss using the custom loss function\n",
    "            loss = custom_loss(logits, labels)\n",
    "            total_loss += loss.item()  # Accumulate the total loss\n",
    "            \n",
    "            # Store predictions and true labels for later evaluation\n",
    "            all_preds.append(logits.detach().cpu().numpy())  # Convert logits to NumPy and store\n",
    "            all_labels.append(labels.detach().cpu().numpy())  # Convert labels to NumPy and store\n",
    "    \n",
    "    # Concatenate stored predictions and labels into single NumPy arrays\n",
    "    all_preds = np.concatenate([p for p in all_preds], axis=0)\n",
    "    all_labels = np.concatenate([l for l in all_labels], axis=0)\n",
    "    \n",
    "    # Compute evaluation metrics such as accuracy, precision, recall, and F1-score\n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)  # Include the average loss in the metrics\n",
    "    \n",
    "    return metrics  # Return computed evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Training Cycle Implementation\n",
    "\n",
    "This code implements the complete fine-tuning cycle with several important components:\n",
    "\n",
    "1. **Performance tracking:**\n",
    "   * Starts with previous best F1 score (0.4229) as baseline\n",
    "   * Continuously monitors for improvements in validation metrics\n",
    "\n",
    "2. **Epoch-based training:**\n",
    "   * Executes `train_epoch_focused()` for each epoch\n",
    "   * Displays comprehensive progress metrics including loss and F1 score\n",
    "\n",
    "3. **Model evaluation:**\n",
    "   * Uses custom `evaluate_model()` function optimized for our weighted classes\n",
    "   * Calculates precision, recall, and F1 score on validation data\n",
    "\n",
    "4. **Model persistence strategy:**\n",
    "   * Saves model only when F1 score improves over previous best\n",
    "   * Implements CPU offloading to prevent CUDA memory errors during saving\n",
    "   * Automatically restores model to GPU for continued training\n",
    "\n",
    "5. **GPU memory management:**\n",
    "   * Temporarily moves model to CPU during saving operations\n",
    "   * Returns model to GPU for continued training efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updated fine-tuning cycle\n",
    "# best_f1 = 0.4229  # Start with the previous best F1 score\n",
    "\n",
    "# print(\"\\nStarting fine-tuning focused on challenging classes...\")\n",
    "# for epoch in range(fine_tuning_epochs):\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Epoch {epoch+1}/{fine_tuning_epochs}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "    \n",
    "#     train_loss = train_epoch_focused()\n",
    "#     print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "#     # Use the new evaluation function\n",
    "#     val_metrics = evaluate_model(val_loader)\n",
    "#     print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "#     print(f\"Validation metrics: accuracy={val_metrics['accuracy']:.4f}, precision={val_metrics['precision']:.4f}, \"\n",
    "#           f\"recall={val_metrics['recall']:.4f}, f1={val_metrics['f1']:.4f}\")\n",
    "    \n",
    "#     # Save if F1 improves\n",
    "#     if val_metrics['f1'] > best_f1:\n",
    "#         best_f1 = val_metrics['f1']\n",
    "#         # Move to CPU to avoid CUDA errors\n",
    "#         model_cpu = model.cpu()\n",
    "#         torch.save(model_cpu.state_dict(), 'best_focused_bert_model.pt')\n",
    "#         # Restore to GPU\n",
    "#         model = model.to(device)\n",
    "#         print(f\"New best model saved with F1: {best_f1:.4f}\")\n",
    "\n",
    "# print(\"\\nFine-tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Model Evaluation with Target Class Analysis\n",
    "\n",
    "This evaluation procedure extends beyond standard metrics to provide detailed insights into model performance:\n",
    "\n",
    "1. **Overall model assessment:**\n",
    "   * Evaluates the fine-tuned model on the held-out test set\n",
    "   * Reports standard metrics (accuracy, precision, recall, F1)\n",
    "   * Uses our custom `evaluate_model()` function that handles weighted classes\n",
    "\n",
    "2. **Detailed classification analysis:**\n",
    "   * Generates comprehensive classification report across all entity types\n",
    "   * Shows per-class precision, recall, F1-score, and support\n",
    "   * Reveals both strengths and remaining challenges in the model\n",
    "\n",
    "3. **Target-focused evaluation:**\n",
    "   * Specifically analyzes performance on our four target entity classes:\n",
    "     - `B-STRATEGY_INSTRUCTION`: Beginning of strategy instructions\n",
    "     - `I-STRATEGY_INSTRUCTION`: Continuation of strategy instructions\n",
    "     - `B-TRACK_CONDITION`: Beginning of track condition descriptions\n",
    "     - `I-TRACK_CONDITION`: Continuation of track condition descriptions\n",
    "\n",
    "4. **Percentage-based success metrics:**\n",
    "   * Calculates the exact percentage of correctly predicted entities for each target class\n",
    "   * Provides clear visibility into whether our focused fine-tuning has succeeded\n",
    "   * Enables direct comparison with pre-fine-tuning performance\n",
    "\n",
    "This evaluation approach helps determine if our class-weighted fine-tuning strategy has successfully improved detection of the most challenging entity types while maintaining overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando en conjunto de test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bc7281fec34970900ffe2d58e930e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: accuracy=0.4411, precision=0.4543, recall=0.4411, f1=0.4298\n",
      "\n",
      "Classification report completo:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "              B-ACTION       0.60      0.43      0.50        14\n",
      "            B-INCIDENT       0.50      0.09      0.15        11\n",
      "            B-PIT_CALL       0.50      0.25      0.33         4\n",
      "     B-POSITION_CHANGE       0.67      0.55      0.60        11\n",
      "           B-SITUATION       0.33      0.20      0.25        40\n",
      "B-STRATEGY_INSTRUCTION       0.00      0.00      0.00         8\n",
      "     B-TECHNICAL_ISSUE       0.43      0.16      0.23        19\n",
      "     B-TRACK_CONDITION       0.00      0.00      0.00         2\n",
      "             B-WEATHER       0.33      0.26      0.29        23\n",
      "              I-ACTION       0.65      0.62      0.63        50\n",
      "            I-INCIDENT       0.38      0.23      0.29        26\n",
      "            I-PIT_CALL       0.50      0.12      0.19        25\n",
      "     I-POSITION_CHANGE       0.60      0.83      0.69        30\n",
      "           I-SITUATION       0.37      0.33      0.35       166\n",
      "I-STRATEGY_INSTRUCTION       0.00      0.00      0.00        37\n",
      "     I-TECHNICAL_ISSUE       0.36      0.17      0.23        75\n",
      "     I-TRACK_CONDITION       0.12      1.00      0.22         8\n",
      "             I-WEATHER       0.56      0.45      0.50       123\n",
      "                     O       0.54      0.70      0.61       271\n",
      "\n",
      "              accuracy                           0.44       943\n",
      "             macro avg       0.39      0.34      0.32       943\n",
      "          weighted avg       0.45      0.44      0.43       943\n",
      "\n",
      "\n",
      "AnÃ¡lisis de clases objetivo:\n",
      "\n",
      "Para B-STRATEGY_INSTRUCTION:\n",
      "Total ejemplos: 8\n",
      "Correctamente predichos: 0 (0.00%)\n",
      "\n",
      "Para I-STRATEGY_INSTRUCTION:\n",
      "Total ejemplos: 37\n",
      "Correctamente predichos: 0 (0.00%)\n",
      "\n",
      "Para B-TRACK_CONDITION:\n",
      "Total ejemplos: 2\n",
      "Correctamente predichos: 0 (0.00%)\n",
      "\n",
      "Para I-TRACK_CONDITION:\n",
      "Total ejemplos: 8\n",
      "Correctamente predichos: 8 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# # 6. Final evaluation focusing on difficult classes\n",
    "# print(\"\\nEvaluating on test set...\")\n",
    "# test_metrics = evaluate_model(test_loader)\n",
    "# print(f\"Test metrics: accuracy={test_metrics['accuracy']:.4f}, precision={test_metrics['precision']:.4f}, \"\n",
    "#       f\"recall={test_metrics['recall']:.4f}, f1={test_metrics['f1']:.4f}\")\n",
    "\n",
    "# # Detailed classification report\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# model.eval()\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "        \n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         preds = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "#         # Filter out padding tokens\n",
    "#         active_mask = labels != -100\n",
    "#         true = labels[active_mask].cpu().numpy()\n",
    "#         pred = preds[active_mask].cpu().numpy()\n",
    "        \n",
    "#         all_labels.extend(true)\n",
    "#         all_preds.extend(pred)\n",
    "\n",
    "# # Convert IDs to labels\n",
    "# true_tags = [id2tag[l] for l in all_labels]\n",
    "# pred_tags = [id2tag[p] for p in all_preds]\n",
    "\n",
    "# # Print full report\n",
    "# print(\"\\nFull classification report:\")\n",
    "# print(classification_report(true_tags, pred_tags))\n",
    "\n",
    "# # Specific analysis for target classes\n",
    "# print(\"\\nTarget class analysis:\")\n",
    "# target_tags = [\"B-STRATEGY_INSTRUCTION\", \"I-STRATEGY_INSTRUCTION\", \n",
    "#               \"B-TRACK_CONDITION\", \"I-TRACK_CONDITION\"]\n",
    "\n",
    "# for tag in target_tags:\n",
    "#     # Filter only instances of this label\n",
    "#     indices = [i for i, t in enumerate(true_tags) if t == tag]\n",
    "#     if indices:\n",
    "#         true_subset = [true_tags[i] for i in indices]\n",
    "#         pred_subset = [pred_tags[i] for i in indices]\n",
    "        \n",
    "#         print(f\"\\nFor {tag}:\")\n",
    "#         print(f\"Total examples: {len(indices)}\")\n",
    "#         correct = sum(1 for t, p in zip(true_subset, pred_subset) if t == p)\n",
    "#         print(f\"Correctly predicted: {correct} ({correct/len(indices)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `extract_entities_from_radio`\n",
    "\n",
    "This function provides the critical bridge between our trained NER model and practical applications by transforming raw F1 radio messages into structured entity data.\n",
    "\n",
    "### Key Processing Steps:\n",
    "\n",
    "1. **Text Preparation:**\n",
    "   * Splits the raw message into tokens\n",
    "   * Handles tokenization with proper word alignment for transformer input\n",
    "\n",
    "2. **Model Inference:**\n",
    "   * Sets model to evaluation mode\n",
    "   * Performs forward pass with gradient calculation disabled\n",
    "   * Extracts predicted entity tags from logits\n",
    "\n",
    "3. **Token Alignment:**\n",
    "   * Maps predictions back to original words using `word_ids`\n",
    "   * Handles subword tokenization by considering only the first subtoken of each word\n",
    "   * Maintains the integrity of the original message structure\n",
    "\n",
    "4. **Entity Reconstruction:**\n",
    "   * Applies BIO (Beginning-Inside-Outside) tag interpretation\n",
    "   * Reconstructs continuous multi-word entities\n",
    "   * Handles entity boundaries and transitions between entity types\n",
    "   * Groups tokens into complete entity phrases\n",
    "\n",
    "5. **Data Structuring:**\n",
    "   * Returns organized dictionary with entity types as keys\n",
    "   * Groups multiple instances of the same entity type\n",
    "   * Preserves the exact entity text as it appeared in the message\n",
    "\n",
    "This function enables practical applications like real-time race strategy assistance, automated highlight generation, and structured data extraction from F1 team communications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_radio(radio_message, model, tokenizer, id2tag):\n",
    "    \"\"\"\n",
    "    Extracts entities from an F1 radio message and returns them in a clean format.\n",
    "    \n",
    "    Args:\n",
    "        radio_message (str): The raw F1 team radio message text\n",
    "        model: The fine-tuned BERT model for entity recognition\n",
    "        tokenizer: The tokenizer corresponding to the model\n",
    "        id2tag (dict): Mapping from numeric IDs to entity tags\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with entity types as keys and lists of entity text as values\n",
    "    \"\"\"\n",
    "    # Split the message into individual word tokens\n",
    "    # This simple approach works for basic tokenization before passing to BERT tokenizer\n",
    "    tokens = radio_message.split()\n",
    "    \n",
    "    # Convert tokens to model inputs using the tokenizer\n",
    "    # Parameters:\n",
    "    #   - is_split_into_words=True: Indicates input is already tokenized\n",
    "    #   - return_tensors=\"pt\": Return PyTorch tensors\n",
    "    #   - padding=True: Add padding to reach maximum length\n",
    "    #   - truncation=True: Truncate if exceeds maximum length\n",
    "    inputs = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)  # Move tensors to the appropriate device (GPU/CPU)\n",
    "    \n",
    "    # Set model to evaluation mode to disable dropout, etc.\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient calculation for inference (saves memory and computation)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Get the predicted class for each token\n",
    "        # - outputs.logits: model output with shape [batch, seq_len, num_classes]\n",
    "        # - torch.argmax(..., dim=2): Get the class with highest probability for each token\n",
    "        # - [0]: Get the first (and only) example in the batch\n",
    "        # - .cpu().numpy(): Move to CPU and convert to numpy for processing\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)[0].cpu().numpy()\n",
    "    \n",
    "    # Initialize containers for processing results\n",
    "    entities = {}               # Will hold all extracted entities by type\n",
    "    current_entity = None       # The entity type we're currently tracking\n",
    "    current_text = []           # Tokens for the current entity we're building\n",
    "    \n",
    "    # BERT tokenizer splits words into subwords, so we need to map predictions back\n",
    "    # to original words. word_ids gives us this mapping.\n",
    "    word_ids = inputs.word_ids(batch_index=0)\n",
    "    previous_word_idx = None    # Track previous word to detect token boundaries\n",
    "    token_predictions = []      # Will hold one prediction per original token\n",
    "    \n",
    "    # First pass: map predictions from subwords back to original words\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        # Special tokens like [CLS] and [SEP] have word_idx=None\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "            \n",
    "        # We only want one prediction per original word, not per subword\n",
    "        # For words split into multiple subwords, we only take the prediction\n",
    "        # from the first subword (standard practice in BERT-based NER)\n",
    "        if word_idx != previous_word_idx:\n",
    "            tag_id = predictions[idx]          # Get the predicted class ID\n",
    "            tag = id2tag[tag_id]               # Convert ID to tag string (e.g., \"B-ACTION\")\n",
    "            token_predictions.append(tag)      # Store prediction for this original token\n",
    "            previous_word_idx = word_idx       # Update tracking variable\n",
    "    \n",
    "    # Second pass: process the predictions to extract continuous entities\n",
    "    # using the BIO (Beginning-Inside-Outside) tagging scheme\n",
    "    for i, (token, tag) in enumerate(zip(tokens, token_predictions)):\n",
    "        # Case 1: Beginning of a new entity (B-*)\n",
    "        if tag.startswith('B-'):\n",
    "            # If we were tracking a previous entity, finalize it before starting new one\n",
    "            if current_entity:\n",
    "                entity_text = ' '.join(current_text)\n",
    "                if current_entity not in entities:\n",
    "                    entities[current_entity] = []\n",
    "                entities[current_entity].append(entity_text)\n",
    "            \n",
    "            # Start tracking the new entity\n",
    "            current_entity = tag[2:]           # Remove the \"B-\" prefix to get entity type\n",
    "            current_text = [token]             # Start collecting tokens for this entity\n",
    "            \n",
    "        # Case 2: Inside/continuation of an entity (I-*)\n",
    "        elif tag.startswith('I-') and current_entity == tag[2:]:\n",
    "            # Only append if it's continuing the same entity type we're tracking\n",
    "            current_text.append(token)\n",
    "            \n",
    "        # Case 3: Outside any entity (O) or mismatch between I-tag and current entity\n",
    "        else:\n",
    "            # If we were tracking an entity, finalize it\n",
    "            if current_entity:\n",
    "                entity_text = ' '.join(current_text)\n",
    "                if current_entity not in entities:\n",
    "                    entities[current_entity] = []\n",
    "                entities[current_entity].append(entity_text)\n",
    "                # Reset tracking variables\n",
    "                current_entity = None\n",
    "                current_text = []\n",
    "    \n",
    "    # Handle edge case: if message ends while still tracking an entity\n",
    "    if current_entity:\n",
    "        entity_text = ' '.join(current_text)\n",
    "        if current_entity not in entities:\n",
    "            entities[current_entity] = []\n",
    "        entities[current_entity].append(entity_text)\n",
    "    \n",
    "    # Return the structured entity dictionary\n",
    "    # Format: {'ACTION': ['box this lap', 'push harder'], 'WEATHER': ['rain expected'], ...}\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Facing Entity Analysis Function\n",
    "\n",
    "The `analyze_f1_radio` function serves as the user-friendly interface to our NER system. It takes a raw F1 radio message as input, processes it through our entity extraction pipeline, and presents the results in a readable, hierarchical format.\n",
    "\n",
    "### Key Features:\n",
    "- **Simple Interface**: Accepts a single string parameter containing the radio message\n",
    "- **Integration Point**: Connects the underlying NER model with end-user applications\n",
    "- **Formatted Display**: Presents extracted entities in an organized, easy-to-read structure\n",
    "- **Entity Categorization**: Groups entities by type for clearer understanding of message content\n",
    "- **Null Handling**: Gracefully handles cases where no entities are detected\n",
    "- **Return Value**: Provides the structured entity dictionary for further programmatic use\n",
    "\n",
    "This function enables practical applications like real-time race strategy assistance, radio message categorization, and structured data visualization from F1 team communications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_f1_radio(message):\n",
    "    \"\"\"\n",
    "    Function for the end user: analyzes a message and displays the entities.\n",
    "    \n",
    "    This function provides a user-friendly interface to extract and display\n",
    "    named entities from F1 radio communications.\n",
    "    \n",
    "    Args:\n",
    "        message (str): The raw F1 team radio message to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with entity types as keys and lists of entity text as values\n",
    "    \"\"\"\n",
    "    # Print the original message to provide context for the analysis results\n",
    "    # The quotes help visually distinguish the message from other output\n",
    "    print(f\"\\nAnalyzing message: \\\"{message}\\\"\")\n",
    "    \n",
    "    # Process the message using our entity extraction function\n",
    "    # This passes the message through the NER model and structures the results\n",
    "    # The function handles all the complexity of tokenization and BIO tag processing\n",
    "    entities = extract_entities_from_radio(message, model, tokenizer, id2tag)\n",
    "    \n",
    "    # Begin displaying results with a header\n",
    "    print(\"\\nDetected entities:\")\n",
    "    \n",
    "    # Handle the case where no entities were detected\n",
    "    # This could happen with very short messages or messages without strategic content\n",
    "    if not entities:\n",
    "        print(\"  No relevant entities detected.\")\n",
    "    else:\n",
    "        # Sort entity types alphabetically for consistent output presentation\n",
    "        # For each entity type, display all instances found in the message\n",
    "        for entity_type, texts in sorted(entities.items()):\n",
    "            # Print the entity type with proper indentation\n",
    "            print(f\"  {entity_type}:\")\n",
    "            \n",
    "            # For each instance of this entity type, display with bullet points\n",
    "            # The quotes help visually distinguish the extracted text\n",
    "            for text in texts:\n",
    "                print(f\"    â¢ \\\"{text}\\\"\")\n",
    "    \n",
    "    # Return the structured entity dictionary for potential further processing\n",
    "    # This allows the function to be used both for display and as part of a pipeline\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, an exmaple usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing message: \"Box this lap, box this lap. We're switching to slicks.\"\n",
      "\n",
      "Detected entities:\n",
      "  PIT_CALL:\n",
      "    â¢ \"Box this lap,\"\n",
      "    â¢ \"box this lap.\"\n",
      "    â¢ \"We're switching to slicks.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Hamilton is 1.2 seconds behind you and closing fast. Defend position.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"Defend position.\"\n",
      "  POSITION_CHANGE:\n",
      "    â¢ \"Hamilton is 1.2 seconds behind you and closing fast.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Yellow flags in sector 2, incident at turn 7. Be careful.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"Be careful.\"\n",
      "  INCIDENT:\n",
      "    â¢ \"incident\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"Yellow flags in sector 2,\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Track is drying up now, lap times are improving.\"\n",
      "\n",
      "Detected entities:\n",
      "  SITUATION:\n",
      "    â¢ \"lap times are improving.\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"Track is drying up now,\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Box this lap and switch to intermediates â weâre facing a technical issue on the front wing and worsening track conditions.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"switch to intermediates â\"\n",
      "  PIT_CALL:\n",
      "    â¢ \"Box this lap\"\n",
      "  TECHNICAL_ISSUE:\n",
      "    â¢ \"front wing\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"worsening track conditions.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Incident at turn 6 with debris on the track; youâre 0.8 seconds behind â defend your position immediately.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"defend your position immediately.\"\n",
      "  INCIDENT:\n",
      "    â¢ \"Incident\"\n",
      "  POSITION_CHANGE:\n",
      "    â¢ \"youâre\"\n",
      "    â¢ \"0.8 seconds behind â\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"debris on the track;\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Box now, the track is drying rapidly while the weather forecast predicts rain incoming; adjust your strategy and check for any technical issues.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"adjust your strategy and check for any technical issues.\"\n",
      "  PIT_CALL:\n",
      "    â¢ \"Box\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"the\"\n",
      "    â¢ \"track is drying rapidly\"\n",
      "  WEATHER:\n",
      "    â¢ \"weather forecast predicts rain incoming;\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Maintain pace but be cautious: an incident at turn 3 is causing yellow flags and changing track conditions â reposition immediately.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"Maintain pace but\"\n",
      "    â¢ \"be cautious:\"\n",
      "  INCIDENT:\n",
      "    â¢ \"incident\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"yellow flags and\"\n",
      "    â¢ \"changing track conditions\"\n",
      "  WEATHER:\n",
      "    â¢ \"an\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Switch pit call: weâre experiencing a gearbox technical issue while the weather remains clear; focus on defending your position with updated strategy instructions.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"focus on defending your position with updated strategy instructions.\"\n",
      "  SITUATION:\n",
      "    â¢ \"weâre experiencing a\"\n",
      "  TECHNICAL_ISSUE:\n",
      "    â¢ \"gearbox\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Immediate action required â an incident occurred in sector 2 and track conditions are deteriorating; box next lap and follow strategy instructions.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"follow strategy instructions.\"\n",
      "  INCIDENT:\n",
      "    â¢ \"incident\"\n",
      "  PIT_CALL:\n",
      "    â¢ \"box next\"\n",
      "  STRATEGY_INSTRUCTION:\n",
      "    â¢ \"Immediate action required\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"track conditions are deteriorating;\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Overtake now, but be aware the weather might worsen and a technical issue with the engine is causing vibrations; adjust your positioning accordingly.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"Overtake now,\"\n",
      "    â¢ \"but\"\n",
      "    â¢ \"be aware\"\n",
      "    â¢ \"adjust your positioning accordingly.\"\n",
      "  INCIDENT:\n",
      "    â¢ \"a\"\n",
      "    â¢ \"technical issue with\"\n",
      "  WEATHER:\n",
      "    â¢ \"weather might worsen and\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Attention: the track is wet and slippery, and an incident at turn 5 has been reported; box this lap and modify your strategy as needed.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"box this lap and modify your strategy as needed.\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"the\"\n",
      "    â¢ \"track is wet and slippery,\"\n",
      "    â¢ \"and an incident at turn 5 has been reported;\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Driver reporting a technical issue with the rear brakes while track conditions are improving; defend your position and prepare for a pit call.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"defend your position and\"\n",
      "    â¢ \"prepare\"\n",
      "  INCIDENT:\n",
      "    â¢ \"technical issue with the\"\n",
      "  SITUATION:\n",
      "    â¢ \"Driver reporting\"\n",
      "  TECHNICAL_ISSUE:\n",
      "    â¢ \"rear brakes\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"track conditions\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Urgent: a multi-car incident in sector 3 has occurred, track conditions have deteriorated, and the weather is turning unpredictable; box immediately and follow strategy instructions.Okay Max, we're expecting rain in about 9 or 10 minutes. What are your thoughts? That you can get there or should we box? We'd need to box this lap to cover Leclerc. I can't see the weather, can I? I don't know.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"follow strategy instructions.Okay\"\n",
      "  INCIDENT:\n",
      "    â¢ \"a\"\n",
      "    â¢ \"multi-car\"\n",
      "  PIT_CALL:\n",
      "    â¢ \"box\"\n",
      "    â¢ \"We'd need to box this lap to cover Leclerc.\"\n",
      "  SITUATION:\n",
      "    â¢ \"I can't see the weather,\"\n",
      "    â¢ \"can I? I don't know.\"\n",
      "  STRATEGY_INSTRUCTION:\n",
      "    â¢ \"What are your thoughts? That you can get there or should we box?\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"track conditions have deteriorated,\"\n",
      "  WEATHER:\n",
      "    â¢ \"we're expecting rain in about 9 or 10 minutes.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing message: \"Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\"\n",
      "\n",
      "Detected entities:\n",
      "  ACTION:\n",
      "    â¢ \"keep to your protocol at the moment.\"\n",
      "  INCIDENT:\n",
      "    â¢ \"Ferrari in the wall,\"\n",
      "    â¢ \"that's Charles stopped.\"\n",
      "  SITUATION:\n",
      "    â¢ \"we've currently got yellows in turn 7.\"\n",
      "    â¢ \"We are expecting the potential of an aborted start,\"\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Prove the model with some real and synthetic messages\n",
    "example_messages = [\n",
    "    \"Box this lap, box this lap. We're switching to slicks.\",\n",
    "    \"Hamilton is 1.2 seconds behind you and closing fast. Defend position.\",\n",
    "    \"Yellow flags in sector 2, incident at turn 7. Be careful.\",\n",
    "    \"Track is drying up now, lap times are improving.\",\n",
    "    \"Box this lap and switch to intermediates â weâre facing a technical issue on the front wing and worsening track conditions.\",\n",
    "    \"Incident at turn 6 with debris on the track; youâre 0.8 seconds behind â defend your position immediately.\",\n",
    "    \"Box now, the track is drying rapidly while the weather forecast predicts rain incoming; adjust your strategy and check for any technical issues.\",\n",
    "    \"Maintain pace but be cautious: an incident at turn 3 is causing yellow flags and changing track conditions â reposition immediately.\",\n",
    "    \"Switch pit call: weâre experiencing a gearbox technical issue while the weather remains clear; focus on defending your position with updated strategy instructions.\",\n",
    "    \"Immediate action required â an incident occurred in sector 2 and track conditions are deteriorating; box next lap and follow strategy instructions.\",\n",
    "    \"Overtake now, but be aware the weather might worsen and a technical issue with the engine is causing vibrations; adjust your positioning accordingly.\",\n",
    "    \"Attention: the track is wet and slippery, and an incident at turn 5 has been reported; box this lap and modify your strategy as needed.\",\n",
    "    \"Driver reporting a technical issue with the rear brakes while track conditions are improving; defend your position and prepare for a pit call.\",\n",
    "    \"Urgent: a multi-car incident in sector 3 has occurred, track conditions have deteriorated, and the weather is turning unpredictable; box immediately and follow strategy instructions.\"\n",
    "    \"Okay Max, we're expecting rain in about 9 or 10 minutes. What are your thoughts? That you can get there or should we box? We'd need to box this lap to cover Leclerc. I can't see the weather, can I? I don't know.\",\n",
    "    \"Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\",\n",
    "]\n",
    "\n",
    "for message in example_messages:\n",
    "    analyze_f1_radio(message)\n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Model Analysis for F1 Radio Communications\n",
    "\n",
    "## Model Comparison Overview\n",
    "\n",
    "We evaluated three different models for extracting named entities from Formula 1 team radio communications:\n",
    "\n",
    "1. **DeBERTa v3 Large**: Advanced transformer architecture known for state-of-the-art performance on NLP tasks\n",
    "2. **BERT Large (pre-trained for NER)**: Model fine-tuned on CoNLL-03 dataset, adapted to our F1-specific entity classes\n",
    "3. **BERT Large with focused fine-tuning**: Final model with additional training focused on challenging entity classes\n",
    "\n",
    "## Performance Metrics Comparison\n",
    "\n",
    "| Model | Accuracy | Precision | Recall | F1-score |\n",
    "|-------|----------|-----------|--------|----------|\n",
    "| DeBERTa v3 Large | 0.4513 | 0.4283 | 0.4513 | 0.4115 |\n",
    "| BERT Large NER | 0.4199 | 0.4466 | 0.4199 | 0.4229 |\n",
    "| **BERT Large Fine-tuned** | **0.4411** | **0.4543** | **0.4411** | **0.4298** |\n",
    "\n",
    "## Entity-Level Performance Analysis (F1-scores)\n",
    "\n",
    "| Entity Type | DeBERTa v3 | BERT NER | BERT Fine-tuned |\n",
    "|-------------|------------|----------|-----------------|\n",
    "| ACTION | 0.42 | 0.54 | **0.57** |\n",
    "| POSITION_CHANGE | 0.26 | **0.66** | 0.65 |\n",
    "| INCIDENT | 0.00 | 0.22 | **0.22** |\n",
    "| TECHNICAL_ISSUE | 0.00 | 0.26 | **0.23** |\n",
    "| SITUATION | 0.16 | 0.30 | **0.30** |\n",
    "| TRACK_CONDITION | 0.06 | 0.11 | **0.11** |\n",
    "| WEATHER | **0.69** | 0.44 | 0.40 |\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "**We selected the fine-tuned BERT model for the following reasons:**\n",
    "\n",
    "1. **Best overall performance**: Achieved the highest F1-score (0.4298) and precision (0.4543) across all models\n",
    "2. **Balanced entity recognition**: More consistent performance across different entity types\n",
    "3. **Improved performance on critical entities**: Better recognition of ACTION, POSITION_CHANGE, and SITUATION entities, which are crucial for strategic decision-making\n",
    "4. **Better generalization**: Shows improved ability to identify both the beginning (B-) and continuation (I-) of entities\n",
    "\n",
    "While DeBERTa v3 performed well on WEATHER entities, it struggled significantly with several other important categories. The base BERT model showed promising results, but our focused fine-tuning approach improved performance further by emphasizing challenging entity classes through weighted loss functions.\n",
    "\n",
    "The fine-tuned model successfully recognizes 100% of I-TRACK_CONDITION instances and shows improved performance on technical issues and incidents compared to the initial models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "## Merging all the models\n",
    "\n",
    "In our next notebook, `N06_model_merging.ipynb`, we'll integrate the three specialized models we've developed throughout this project:\n",
    "\n",
    "1. **Sentiment Analysis Model:** Detects emotions and tone in radio communications\n",
    "2. **Intent Recognition Model:** Identifies the purpose and goals behind messages\n",
    "3. **Named Entity Recognition Model:** Extracts structured information about race elements\n",
    "\n",
    "### Integration Approach\n",
    "\n",
    "We'll create a unified pipeline that:\n",
    "\n",
    "1. Takes a raw F1 team radio message as input\n",
    "2. Processes it through each specialized model in parallel\n",
    "3. Combines the outputs into a comprehensive JSON structure\n",
    "4. Provides a single interface for analyzing radio communications\n",
    "\n",
    "### Benefits of Integration\n",
    "\n",
    "This merged approach offers several advantages:\n",
    "\n",
    "- **Comprehensive Analysis:** Captures semantic, pragmatic, and informational dimensions\n",
    "- **Standardized Output:** Provides a consistent JSON format for downstream applications\n",
    "- **Simplified Interface:** Requires just one function call to access all analyses\n",
    "- **Racing Context Awareness:** Combines different perspectives for better strategic insights\n",
    "\n",
    "\n",
    "## **Integration with logical agent**: \n",
    "\n",
    "Connect the NER system with the strategic recommendation engine for real-time race strategy optimization.\n",
    "\n",
    "\n",
    "The current model is production-ready and can reliably extract most entity types from F1 radio communications, providing valuable structured data for strategic decision-making systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
